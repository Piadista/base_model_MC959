{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 53973,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005558334722917014,
      "grad_norm": 3.1005589962005615,
      "learning_rate": 0.0004990736108795139,
      "loss": 3.9356,
      "step": 100
    },
    {
      "epoch": 0.011116669445834029,
      "grad_norm": 2.5297152996063232,
      "learning_rate": 0.0004981472217590277,
      "loss": 3.5618,
      "step": 200
    },
    {
      "epoch": 0.01667500416875104,
      "grad_norm": 2.478330135345459,
      "learning_rate": 0.0004972208326385415,
      "loss": 3.3918,
      "step": 300
    },
    {
      "epoch": 0.022233338891668057,
      "grad_norm": 2.4246256351470947,
      "learning_rate": 0.0004962944435180554,
      "loss": 3.2581,
      "step": 400
    },
    {
      "epoch": 0.02779167361458507,
      "grad_norm": 2.9803333282470703,
      "learning_rate": 0.0004953680543975692,
      "loss": 3.307,
      "step": 500
    },
    {
      "epoch": 0.03335000833750208,
      "grad_norm": 2.744901180267334,
      "learning_rate": 0.000494441665277083,
      "loss": 3.1957,
      "step": 600
    },
    {
      "epoch": 0.0389083430604191,
      "grad_norm": 3.2832608222961426,
      "learning_rate": 0.0004935152761565968,
      "loss": 3.1661,
      "step": 700
    },
    {
      "epoch": 0.044466677783336114,
      "grad_norm": 3.2287769317626953,
      "learning_rate": 0.0004925888870361107,
      "loss": 3.1009,
      "step": 800
    },
    {
      "epoch": 0.05002501250625312,
      "grad_norm": 2.925997257232666,
      "learning_rate": 0.0004916624979156246,
      "loss": 3.0741,
      "step": 900
    },
    {
      "epoch": 0.05558334722917014,
      "grad_norm": 2.6128687858581543,
      "learning_rate": 0.0004907361087951383,
      "loss": 3.0133,
      "step": 1000
    },
    {
      "epoch": 0.061141681952087155,
      "grad_norm": 2.257406234741211,
      "learning_rate": 0.0004898097196746521,
      "loss": 3.0109,
      "step": 1100
    },
    {
      "epoch": 0.06670001667500416,
      "grad_norm": 2.8663244247436523,
      "learning_rate": 0.000488883330554166,
      "loss": 3.0125,
      "step": 1200
    },
    {
      "epoch": 0.07225835139792118,
      "grad_norm": 2.631938934326172,
      "learning_rate": 0.0004879569414336798,
      "loss": 3.0027,
      "step": 1300
    },
    {
      "epoch": 0.0778166861208382,
      "grad_norm": 3.0647921562194824,
      "learning_rate": 0.00048703055231319367,
      "loss": 2.9091,
      "step": 1400
    },
    {
      "epoch": 0.08337502084375521,
      "grad_norm": 2.829359769821167,
      "learning_rate": 0.00048610416319270746,
      "loss": 2.8951,
      "step": 1500
    },
    {
      "epoch": 0.08893335556667223,
      "grad_norm": 4.678634166717529,
      "learning_rate": 0.0004851777740722213,
      "loss": 2.8911,
      "step": 1600
    },
    {
      "epoch": 0.09449169028958924,
      "grad_norm": 2.828413248062134,
      "learning_rate": 0.00048425138495173514,
      "loss": 2.903,
      "step": 1700
    },
    {
      "epoch": 0.10005002501250625,
      "grad_norm": 3.436892032623291,
      "learning_rate": 0.00048332499583124893,
      "loss": 2.8519,
      "step": 1800
    },
    {
      "epoch": 0.10560835973542326,
      "grad_norm": 3.317812442779541,
      "learning_rate": 0.0004823986067107628,
      "loss": 2.8324,
      "step": 1900
    },
    {
      "epoch": 0.11116669445834028,
      "grad_norm": 3.449934959411621,
      "learning_rate": 0.0004814722175902767,
      "loss": 2.8211,
      "step": 2000
    },
    {
      "epoch": 0.1167250291812573,
      "grad_norm": 3.173543930053711,
      "learning_rate": 0.00048055509236099534,
      "loss": 2.8624,
      "step": 2100
    },
    {
      "epoch": 0.12228336390417431,
      "grad_norm": 2.647733211517334,
      "learning_rate": 0.0004796287032405092,
      "loss": 2.8473,
      "step": 2200
    },
    {
      "epoch": 0.12784169862709133,
      "grad_norm": 2.9023091793060303,
      "learning_rate": 0.000478702314120023,
      "loss": 2.779,
      "step": 2300
    },
    {
      "epoch": 0.13340003335000833,
      "grad_norm": 3.1474077701568604,
      "learning_rate": 0.0004777759249995368,
      "loss": 2.7263,
      "step": 2400
    },
    {
      "epoch": 0.13895836807292536,
      "grad_norm": 2.7441868782043457,
      "learning_rate": 0.00047684953587905066,
      "loss": 2.7727,
      "step": 2500
    },
    {
      "epoch": 0.14451670279584236,
      "grad_norm": 3.246187210083008,
      "learning_rate": 0.00047592314675856445,
      "loss": 2.8415,
      "step": 2600
    },
    {
      "epoch": 0.1500750375187594,
      "grad_norm": 2.4023735523223877,
      "learning_rate": 0.0004749967576380783,
      "loss": 2.7427,
      "step": 2700
    },
    {
      "epoch": 0.1556333722416764,
      "grad_norm": 3.59515380859375,
      "learning_rate": 0.00047407036851759214,
      "loss": 2.7298,
      "step": 2800
    },
    {
      "epoch": 0.1611917069645934,
      "grad_norm": 2.480437994003296,
      "learning_rate": 0.000473143979397106,
      "loss": 2.7176,
      "step": 2900
    },
    {
      "epoch": 0.16675004168751043,
      "grad_norm": 3.302696466445923,
      "learning_rate": 0.00047221759027661983,
      "loss": 2.7062,
      "step": 3000
    },
    {
      "epoch": 0.17230837641042743,
      "grad_norm": 3.255425214767456,
      "learning_rate": 0.0004712912011561336,
      "loss": 2.6722,
      "step": 3100
    },
    {
      "epoch": 0.17786671113334446,
      "grad_norm": 3.4520394802093506,
      "learning_rate": 0.00047036481203564746,
      "loss": 2.6474,
      "step": 3200
    },
    {
      "epoch": 0.18342504585626146,
      "grad_norm": 3.3510308265686035,
      "learning_rate": 0.0004694384229151613,
      "loss": 2.6903,
      "step": 3300
    },
    {
      "epoch": 0.1889833805791785,
      "grad_norm": 2.9480462074279785,
      "learning_rate": 0.0004685120337946751,
      "loss": 2.6268,
      "step": 3400
    },
    {
      "epoch": 0.1945417153020955,
      "grad_norm": 2.9323506355285645,
      "learning_rate": 0.00046758564467418894,
      "loss": 2.7025,
      "step": 3500
    },
    {
      "epoch": 0.2001000500250125,
      "grad_norm": 2.8298897743225098,
      "learning_rate": 0.0004666592555537028,
      "loss": 2.6579,
      "step": 3600
    },
    {
      "epoch": 0.20565838474792952,
      "grad_norm": 3.4068987369537354,
      "learning_rate": 0.00046573286643321663,
      "loss": 2.5458,
      "step": 3700
    },
    {
      "epoch": 0.21121671947084653,
      "grad_norm": 3.2034687995910645,
      "learning_rate": 0.00046480647731273047,
      "loss": 2.6124,
      "step": 3800
    },
    {
      "epoch": 0.21677505419376356,
      "grad_norm": 3.193591356277466,
      "learning_rate": 0.00046388008819224426,
      "loss": 2.6027,
      "step": 3900
    },
    {
      "epoch": 0.22233338891668056,
      "grad_norm": 3.6627237796783447,
      "learning_rate": 0.000462962962962963,
      "loss": 2.6536,
      "step": 4000
    },
    {
      "epoch": 0.2278917236395976,
      "grad_norm": 3.551919937133789,
      "learning_rate": 0.0004620365738424768,
      "loss": 2.6327,
      "step": 4100
    },
    {
      "epoch": 0.2334500583625146,
      "grad_norm": 2.7870683670043945,
      "learning_rate": 0.0004611101847219906,
      "loss": 2.6502,
      "step": 4200
    },
    {
      "epoch": 0.2390083930854316,
      "grad_norm": 3.4406650066375732,
      "learning_rate": 0.00046018379560150446,
      "loss": 2.5855,
      "step": 4300
    },
    {
      "epoch": 0.24456672780834862,
      "grad_norm": 3.4533815383911133,
      "learning_rate": 0.00045925740648101825,
      "loss": 2.5817,
      "step": 4400
    },
    {
      "epoch": 0.25012506253126565,
      "grad_norm": 3.427034616470337,
      "learning_rate": 0.00045833101736053215,
      "loss": 2.646,
      "step": 4500
    },
    {
      "epoch": 0.25568339725418265,
      "grad_norm": 3.3074281215667725,
      "learning_rate": 0.000457404628240046,
      "loss": 2.523,
      "step": 4600
    },
    {
      "epoch": 0.26124173197709966,
      "grad_norm": 3.612468719482422,
      "learning_rate": 0.0004564782391195598,
      "loss": 2.6189,
      "step": 4700
    },
    {
      "epoch": 0.26680006670001666,
      "grad_norm": 3.4657931327819824,
      "learning_rate": 0.0004555518499990736,
      "loss": 2.638,
      "step": 4800
    },
    {
      "epoch": 0.2723584014229337,
      "grad_norm": 4.106339454650879,
      "learning_rate": 0.00045462546087858747,
      "loss": 2.6157,
      "step": 4900
    },
    {
      "epoch": 0.2779167361458507,
      "grad_norm": 2.6842823028564453,
      "learning_rate": 0.00045369907175810126,
      "loss": 2.5394,
      "step": 5000
    },
    {
      "epoch": 0.2834750708687677,
      "grad_norm": 2.9384686946868896,
      "learning_rate": 0.0004527726826376151,
      "loss": 2.584,
      "step": 5100
    },
    {
      "epoch": 0.2890334055916847,
      "grad_norm": 4.326462268829346,
      "learning_rate": 0.00045184629351712895,
      "loss": 2.5793,
      "step": 5200
    },
    {
      "epoch": 0.2945917403146017,
      "grad_norm": 3.434221029281616,
      "learning_rate": 0.0004509199043966428,
      "loss": 2.6148,
      "step": 5300
    },
    {
      "epoch": 0.3001500750375188,
      "grad_norm": 2.803065538406372,
      "learning_rate": 0.00044999351527615664,
      "loss": 2.5589,
      "step": 5400
    },
    {
      "epoch": 0.3057084097604358,
      "grad_norm": 3.0405941009521484,
      "learning_rate": 0.0004490671261556704,
      "loss": 2.6017,
      "step": 5500
    },
    {
      "epoch": 0.3112667444833528,
      "grad_norm": 3.316812515258789,
      "learning_rate": 0.00044814073703518427,
      "loss": 2.5875,
      "step": 5600
    },
    {
      "epoch": 0.3168250792062698,
      "grad_norm": 2.3511013984680176,
      "learning_rate": 0.0004472143479146981,
      "loss": 2.5373,
      "step": 5700
    },
    {
      "epoch": 0.3223834139291868,
      "grad_norm": 3.414001226425171,
      "learning_rate": 0.0004462879587942119,
      "loss": 2.5851,
      "step": 5800
    },
    {
      "epoch": 0.32794174865210385,
      "grad_norm": 3.6073198318481445,
      "learning_rate": 0.00044536156967372575,
      "loss": 2.5605,
      "step": 5900
    },
    {
      "epoch": 0.33350008337502085,
      "grad_norm": 3.2356317043304443,
      "learning_rate": 0.0004444351805532396,
      "loss": 2.5646,
      "step": 6000
    },
    {
      "epoch": 0.33905841809793785,
      "grad_norm": 2.9078073501586914,
      "learning_rate": 0.00044350879143275344,
      "loss": 2.5939,
      "step": 6100
    },
    {
      "epoch": 0.34461675282085485,
      "grad_norm": 2.83648419380188,
      "learning_rate": 0.0004425824023122673,
      "loss": 2.5541,
      "step": 6200
    },
    {
      "epoch": 0.3501750875437719,
      "grad_norm": 2.9212052822113037,
      "learning_rate": 0.00044165601319178107,
      "loss": 2.4985,
      "step": 6300
    },
    {
      "epoch": 0.3557334222666889,
      "grad_norm": 3.551506519317627,
      "learning_rate": 0.0004407296240712949,
      "loss": 2.5747,
      "step": 6400
    },
    {
      "epoch": 0.3612917569896059,
      "grad_norm": 2.705430746078491,
      "learning_rate": 0.00043980323495080876,
      "loss": 2.5739,
      "step": 6500
    },
    {
      "epoch": 0.3668500917125229,
      "grad_norm": 3.156269073486328,
      "learning_rate": 0.00043887684583032255,
      "loss": 2.4797,
      "step": 6600
    },
    {
      "epoch": 0.3724084264354399,
      "grad_norm": 2.779757022857666,
      "learning_rate": 0.0004379504567098364,
      "loss": 2.6034,
      "step": 6700
    },
    {
      "epoch": 0.377966761158357,
      "grad_norm": 3.4444780349731445,
      "learning_rate": 0.00043702406758935024,
      "loss": 2.5032,
      "step": 6800
    },
    {
      "epoch": 0.383525095881274,
      "grad_norm": 7.763994216918945,
      "learning_rate": 0.0004360976784688641,
      "loss": 2.4812,
      "step": 6900
    },
    {
      "epoch": 0.389083430604191,
      "grad_norm": 2.8708767890930176,
      "learning_rate": 0.0004351712893483779,
      "loss": 2.5102,
      "step": 7000
    },
    {
      "epoch": 0.394641765327108,
      "grad_norm": 3.5508852005004883,
      "learning_rate": 0.00043424490022789177,
      "loss": 2.5121,
      "step": 7100
    },
    {
      "epoch": 0.400200100050025,
      "grad_norm": 2.802677869796753,
      "learning_rate": 0.00043331851110740556,
      "loss": 2.5031,
      "step": 7200
    },
    {
      "epoch": 0.40575843477294204,
      "grad_norm": 2.489384174346924,
      "learning_rate": 0.0004324013858781243,
      "loss": 2.4782,
      "step": 7300
    },
    {
      "epoch": 0.41131676949585905,
      "grad_norm": 3.6103925704956055,
      "learning_rate": 0.00043147499675763807,
      "loss": 2.4749,
      "step": 7400
    },
    {
      "epoch": 0.41687510421877605,
      "grad_norm": 3.760629177093506,
      "learning_rate": 0.0004305486076371519,
      "loss": 2.4731,
      "step": 7500
    },
    {
      "epoch": 0.42243343894169305,
      "grad_norm": 2.902778387069702,
      "learning_rate": 0.00042962221851666575,
      "loss": 2.5142,
      "step": 7600
    },
    {
      "epoch": 0.4279917736646101,
      "grad_norm": 3.0991697311401367,
      "learning_rate": 0.00042869582939617954,
      "loss": 2.5194,
      "step": 7700
    },
    {
      "epoch": 0.4335501083875271,
      "grad_norm": 3.8925273418426514,
      "learning_rate": 0.00042776944027569344,
      "loss": 2.5279,
      "step": 7800
    },
    {
      "epoch": 0.4391084431104441,
      "grad_norm": 2.8482322692871094,
      "learning_rate": 0.00042684305115520723,
      "loss": 2.5193,
      "step": 7900
    },
    {
      "epoch": 0.4446667778333611,
      "grad_norm": 2.9159343242645264,
      "learning_rate": 0.0004259166620347211,
      "loss": 2.5615,
      "step": 8000
    },
    {
      "epoch": 0.4502251125562781,
      "grad_norm": 3.2747457027435303,
      "learning_rate": 0.0004249902729142349,
      "loss": 2.4444,
      "step": 8100
    },
    {
      "epoch": 0.4557834472791952,
      "grad_norm": 3.6051652431488037,
      "learning_rate": 0.0004240638837937487,
      "loss": 2.4784,
      "step": 8200
    },
    {
      "epoch": 0.4613417820021122,
      "grad_norm": 3.391796112060547,
      "learning_rate": 0.00042313749467326255,
      "loss": 2.5622,
      "step": 8300
    },
    {
      "epoch": 0.4669001167250292,
      "grad_norm": 3.043330192565918,
      "learning_rate": 0.0004222111055527764,
      "loss": 2.4429,
      "step": 8400
    },
    {
      "epoch": 0.4724584514479462,
      "grad_norm": 3.226562738418579,
      "learning_rate": 0.0004212847164322902,
      "loss": 2.472,
      "step": 8500
    },
    {
      "epoch": 0.4780167861708632,
      "grad_norm": 2.811988592147827,
      "learning_rate": 0.0004203583273118041,
      "loss": 2.4099,
      "step": 8600
    },
    {
      "epoch": 0.48357512089378024,
      "grad_norm": 3.106863260269165,
      "learning_rate": 0.0004194319381913179,
      "loss": 2.4798,
      "step": 8700
    },
    {
      "epoch": 0.48913345561669724,
      "grad_norm": 2.708744525909424,
      "learning_rate": 0.0004185055490708317,
      "loss": 2.4843,
      "step": 8800
    },
    {
      "epoch": 0.49469179033961425,
      "grad_norm": 3.4587535858154297,
      "learning_rate": 0.00041757915995034557,
      "loss": 2.4659,
      "step": 8900
    },
    {
      "epoch": 0.5002501250625313,
      "grad_norm": 3.5197527408599854,
      "learning_rate": 0.00041665277082985936,
      "loss": 2.4168,
      "step": 9000
    },
    {
      "epoch": 0.5058084597854483,
      "grad_norm": 3.0076329708099365,
      "learning_rate": 0.0004157263817093732,
      "loss": 2.474,
      "step": 9100
    },
    {
      "epoch": 0.5113667945083653,
      "grad_norm": 4.145804405212402,
      "learning_rate": 0.00041479999258888704,
      "loss": 2.4592,
      "step": 9200
    },
    {
      "epoch": 0.5169251292312823,
      "grad_norm": 2.9662728309631348,
      "learning_rate": 0.0004138736034684009,
      "loss": 2.4904,
      "step": 9300
    },
    {
      "epoch": 0.5224834639541993,
      "grad_norm": 2.684993267059326,
      "learning_rate": 0.0004129564782391196,
      "loss": 2.4375,
      "step": 9400
    },
    {
      "epoch": 0.5280417986771163,
      "grad_norm": 3.1242823600769043,
      "learning_rate": 0.00041203935300983827,
      "loss": 2.5207,
      "step": 9500
    },
    {
      "epoch": 0.5336001334000333,
      "grad_norm": 2.13852596282959,
      "learning_rate": 0.0004111129638893521,
      "loss": 2.4226,
      "step": 9600
    },
    {
      "epoch": 0.5391584681229503,
      "grad_norm": 2.6460394859313965,
      "learning_rate": 0.00041018657476886596,
      "loss": 2.5041,
      "step": 9700
    },
    {
      "epoch": 0.5447168028458674,
      "grad_norm": 2.791234016418457,
      "learning_rate": 0.00040926018564837975,
      "loss": 2.4916,
      "step": 9800
    },
    {
      "epoch": 0.5502751375687844,
      "grad_norm": 3.0926947593688965,
      "learning_rate": 0.0004083337965278936,
      "loss": 2.5131,
      "step": 9900
    },
    {
      "epoch": 0.5558334722917014,
      "grad_norm": 3.0212244987487793,
      "learning_rate": 0.0004074074074074074,
      "loss": 2.4275,
      "step": 10000
    },
    {
      "epoch": 0.5613918070146184,
      "grad_norm": 5.038130283355713,
      "learning_rate": 0.0004064810182869212,
      "loss": 2.4517,
      "step": 10100
    },
    {
      "epoch": 0.5669501417375354,
      "grad_norm": 4.308995723724365,
      "learning_rate": 0.00040555462916643507,
      "loss": 2.4692,
      "step": 10200
    },
    {
      "epoch": 0.5725084764604524,
      "grad_norm": 2.651266098022461,
      "learning_rate": 0.0004046282400459489,
      "loss": 2.3945,
      "step": 10300
    },
    {
      "epoch": 0.5780668111833694,
      "grad_norm": 3.9038193225860596,
      "learning_rate": 0.00040370185092546276,
      "loss": 2.4598,
      "step": 10400
    },
    {
      "epoch": 0.5836251459062864,
      "grad_norm": 3.95068097114563,
      "learning_rate": 0.0004027754618049766,
      "loss": 2.4561,
      "step": 10500
    },
    {
      "epoch": 0.5891834806292034,
      "grad_norm": 3.5056097507476807,
      "learning_rate": 0.0004018490726844904,
      "loss": 2.4353,
      "step": 10600
    },
    {
      "epoch": 0.5947418153521205,
      "grad_norm": 2.5815112590789795,
      "learning_rate": 0.00040092268356400424,
      "loss": 2.484,
      "step": 10700
    },
    {
      "epoch": 0.6003001500750376,
      "grad_norm": 2.7148478031158447,
      "learning_rate": 0.000399996294443518,
      "loss": 2.5292,
      "step": 10800
    },
    {
      "epoch": 0.6058584847979546,
      "grad_norm": 4.6479034423828125,
      "learning_rate": 0.00039906990532303187,
      "loss": 2.3867,
      "step": 10900
    },
    {
      "epoch": 0.6114168195208716,
      "grad_norm": 4.99106502532959,
      "learning_rate": 0.0003981435162025457,
      "loss": 2.3918,
      "step": 11000
    },
    {
      "epoch": 0.6169751542437886,
      "grad_norm": 3.450252056121826,
      "learning_rate": 0.00039721712708205956,
      "loss": 2.4326,
      "step": 11100
    },
    {
      "epoch": 0.6225334889667056,
      "grad_norm": 3.552492618560791,
      "learning_rate": 0.0003962907379615734,
      "loss": 2.4546,
      "step": 11200
    },
    {
      "epoch": 0.6280918236896226,
      "grad_norm": 3.122478485107422,
      "learning_rate": 0.00039536434884108725,
      "loss": 2.3611,
      "step": 11300
    },
    {
      "epoch": 0.6336501584125396,
      "grad_norm": 3.222546339035034,
      "learning_rate": 0.00039443795972060104,
      "loss": 2.4659,
      "step": 11400
    },
    {
      "epoch": 0.6392084931354566,
      "grad_norm": 3.1596381664276123,
      "learning_rate": 0.0003935115706001149,
      "loss": 2.4918,
      "step": 11500
    },
    {
      "epoch": 0.6447668278583736,
      "grad_norm": 4.333948612213135,
      "learning_rate": 0.0003925851814796287,
      "loss": 2.4803,
      "step": 11600
    },
    {
      "epoch": 0.6503251625812907,
      "grad_norm": 2.67012882232666,
      "learning_rate": 0.0003916587923591425,
      "loss": 2.4835,
      "step": 11700
    },
    {
      "epoch": 0.6558834973042077,
      "grad_norm": 3.108262538909912,
      "learning_rate": 0.0003907324032386564,
      "loss": 2.4185,
      "step": 11800
    },
    {
      "epoch": 0.6614418320271247,
      "grad_norm": 3.180651903152466,
      "learning_rate": 0.0003898060141181702,
      "loss": 2.4895,
      "step": 11900
    },
    {
      "epoch": 0.6670001667500417,
      "grad_norm": 3.336681842803955,
      "learning_rate": 0.00038887962499768405,
      "loss": 2.5195,
      "step": 12000
    },
    {
      "epoch": 0.6725585014729587,
      "grad_norm": 3.262268304824829,
      "learning_rate": 0.0003879532358771979,
      "loss": 2.4452,
      "step": 12100
    },
    {
      "epoch": 0.6781168361958757,
      "grad_norm": 3.6913394927978516,
      "learning_rate": 0.0003870268467567117,
      "loss": 2.3857,
      "step": 12200
    },
    {
      "epoch": 0.6836751709187927,
      "grad_norm": 2.8540191650390625,
      "learning_rate": 0.0003861004576362255,
      "loss": 2.4136,
      "step": 12300
    },
    {
      "epoch": 0.6892335056417097,
      "grad_norm": 3.5771830081939697,
      "learning_rate": 0.00038517406851573937,
      "loss": 2.4877,
      "step": 12400
    },
    {
      "epoch": 0.6947918403646267,
      "grad_norm": 2.280330181121826,
      "learning_rate": 0.00038424767939525316,
      "loss": 2.4224,
      "step": 12500
    },
    {
      "epoch": 0.7003501750875438,
      "grad_norm": 2.744743824005127,
      "learning_rate": 0.00038332129027476706,
      "loss": 2.3249,
      "step": 12600
    },
    {
      "epoch": 0.7059085098104608,
      "grad_norm": 2.9570460319519043,
      "learning_rate": 0.00038239490115428085,
      "loss": 2.3626,
      "step": 12700
    },
    {
      "epoch": 0.7114668445333778,
      "grad_norm": 4.146138668060303,
      "learning_rate": 0.0003814685120337947,
      "loss": 2.4251,
      "step": 12800
    },
    {
      "epoch": 0.7170251792562948,
      "grad_norm": 2.6880810260772705,
      "learning_rate": 0.00038054212291330854,
      "loss": 2.4297,
      "step": 12900
    },
    {
      "epoch": 0.7225835139792118,
      "grad_norm": 3.1954147815704346,
      "learning_rate": 0.0003796157337928223,
      "loss": 2.4042,
      "step": 13000
    },
    {
      "epoch": 0.7281418487021288,
      "grad_norm": 3.723010778427124,
      "learning_rate": 0.00037868934467233617,
      "loss": 2.4535,
      "step": 13100
    },
    {
      "epoch": 0.7337001834250458,
      "grad_norm": 3.2458841800689697,
      "learning_rate": 0.00037776295555185,
      "loss": 2.386,
      "step": 13200
    },
    {
      "epoch": 0.7392585181479628,
      "grad_norm": 3.0155797004699707,
      "learning_rate": 0.0003768365664313638,
      "loss": 2.3703,
      "step": 13300
    },
    {
      "epoch": 0.7448168528708798,
      "grad_norm": 3.9787681102752686,
      "learning_rate": 0.0003759101773108777,
      "loss": 2.4254,
      "step": 13400
    },
    {
      "epoch": 0.7503751875937968,
      "grad_norm": 3.145472288131714,
      "learning_rate": 0.0003749837881903915,
      "loss": 2.425,
      "step": 13500
    },
    {
      "epoch": 0.755933522316714,
      "grad_norm": 3.169259548187256,
      "learning_rate": 0.00037405739906990534,
      "loss": 2.3775,
      "step": 13600
    },
    {
      "epoch": 0.761491857039631,
      "grad_norm": 3.574570894241333,
      "learning_rate": 0.00037314027384062405,
      "loss": 2.4033,
      "step": 13700
    },
    {
      "epoch": 0.767050191762548,
      "grad_norm": 3.204981803894043,
      "learning_rate": 0.00037221388472013784,
      "loss": 2.3569,
      "step": 13800
    },
    {
      "epoch": 0.772608526485465,
      "grad_norm": 2.660659074783325,
      "learning_rate": 0.0003712874955996517,
      "loss": 2.4647,
      "step": 13900
    },
    {
      "epoch": 0.778166861208382,
      "grad_norm": 5.066927909851074,
      "learning_rate": 0.00037036110647916553,
      "loss": 2.4154,
      "step": 14000
    },
    {
      "epoch": 0.783725195931299,
      "grad_norm": 2.4242777824401855,
      "learning_rate": 0.0003694347173586793,
      "loss": 2.3707,
      "step": 14100
    },
    {
      "epoch": 0.789283530654216,
      "grad_norm": 3.1037349700927734,
      "learning_rate": 0.00036850832823819317,
      "loss": 2.4174,
      "step": 14200
    },
    {
      "epoch": 0.794841865377133,
      "grad_norm": 2.4381425380706787,
      "learning_rate": 0.00036759120300891183,
      "loss": 2.4411,
      "step": 14300
    },
    {
      "epoch": 0.80040020010005,
      "grad_norm": 2.909268379211426,
      "learning_rate": 0.00036666481388842573,
      "loss": 2.4577,
      "step": 14400
    },
    {
      "epoch": 0.8059585348229671,
      "grad_norm": 2.619173526763916,
      "learning_rate": 0.00036573842476793957,
      "loss": 2.4067,
      "step": 14500
    },
    {
      "epoch": 0.8115168695458841,
      "grad_norm": 3.06593656539917,
      "learning_rate": 0.00036481203564745336,
      "loss": 2.4107,
      "step": 14600
    },
    {
      "epoch": 0.8170752042688011,
      "grad_norm": 2.5698933601379395,
      "learning_rate": 0.0003638856465269672,
      "loss": 2.3606,
      "step": 14700
    },
    {
      "epoch": 0.8226335389917181,
      "grad_norm": 2.751298189163208,
      "learning_rate": 0.000362959257406481,
      "loss": 2.3876,
      "step": 14800
    },
    {
      "epoch": 0.8281918737146351,
      "grad_norm": 3.3786704540252686,
      "learning_rate": 0.00036203286828599484,
      "loss": 2.3996,
      "step": 14900
    },
    {
      "epoch": 0.8337502084375521,
      "grad_norm": 3.521937608718872,
      "learning_rate": 0.0003611064791655087,
      "loss": 2.3935,
      "step": 15000
    },
    {
      "epoch": 0.8393085431604691,
      "grad_norm": 3.2864813804626465,
      "learning_rate": 0.0003601800900450225,
      "loss": 2.4362,
      "step": 15100
    },
    {
      "epoch": 0.8448668778833861,
      "grad_norm": 3.0221447944641113,
      "learning_rate": 0.00035925370092453637,
      "loss": 2.399,
      "step": 15200
    },
    {
      "epoch": 0.8504252126063031,
      "grad_norm": 3.232283115386963,
      "learning_rate": 0.0003583273118040502,
      "loss": 2.3787,
      "step": 15300
    },
    {
      "epoch": 0.8559835473292202,
      "grad_norm": 4.003358840942383,
      "learning_rate": 0.000357400922683564,
      "loss": 2.3759,
      "step": 15400
    },
    {
      "epoch": 0.8615418820521372,
      "grad_norm": 2.1832756996154785,
      "learning_rate": 0.00035647453356307785,
      "loss": 2.4178,
      "step": 15500
    },
    {
      "epoch": 0.8671002167750542,
      "grad_norm": 3.198848009109497,
      "learning_rate": 0.00035554814444259164,
      "loss": 2.3371,
      "step": 15600
    },
    {
      "epoch": 0.8726585514979712,
      "grad_norm": 3.0089519023895264,
      "learning_rate": 0.0003546217553221055,
      "loss": 2.3637,
      "step": 15700
    },
    {
      "epoch": 0.8782168862208882,
      "grad_norm": 3.8461999893188477,
      "learning_rate": 0.00035369536620161933,
      "loss": 2.3606,
      "step": 15800
    },
    {
      "epoch": 0.8837752209438052,
      "grad_norm": 2.886531114578247,
      "learning_rate": 0.0003527689770811332,
      "loss": 2.3661,
      "step": 15900
    },
    {
      "epoch": 0.8893335556667222,
      "grad_norm": 2.607515811920166,
      "learning_rate": 0.000351842587960647,
      "loss": 2.3009,
      "step": 16000
    },
    {
      "epoch": 0.8948918903896392,
      "grad_norm": 3.640130043029785,
      "learning_rate": 0.00035091619884016086,
      "loss": 2.3711,
      "step": 16100
    },
    {
      "epoch": 0.9004502251125562,
      "grad_norm": 2.6037778854370117,
      "learning_rate": 0.00034998980971967465,
      "loss": 2.3567,
      "step": 16200
    },
    {
      "epoch": 0.9060085598354732,
      "grad_norm": 3.265484094619751,
      "learning_rate": 0.0003490634205991885,
      "loss": 2.3214,
      "step": 16300
    },
    {
      "epoch": 0.9115668945583904,
      "grad_norm": 2.546191453933716,
      "learning_rate": 0.00034813703147870234,
      "loss": 2.4078,
      "step": 16400
    },
    {
      "epoch": 0.9171252292813074,
      "grad_norm": 3.407686471939087,
      "learning_rate": 0.00034721064235821613,
      "loss": 2.4181,
      "step": 16500
    },
    {
      "epoch": 0.9226835640042244,
      "grad_norm": 3.4878294467926025,
      "learning_rate": 0.00034628425323773,
      "loss": 2.3951,
      "step": 16600
    },
    {
      "epoch": 0.9282418987271414,
      "grad_norm": 2.9789741039276123,
      "learning_rate": 0.0003453578641172438,
      "loss": 2.3951,
      "step": 16700
    },
    {
      "epoch": 0.9338002334500584,
      "grad_norm": 3.0798850059509277,
      "learning_rate": 0.00034443147499675766,
      "loss": 2.3528,
      "step": 16800
    },
    {
      "epoch": 0.9393585681729754,
      "grad_norm": 2.8609044551849365,
      "learning_rate": 0.0003435050858762715,
      "loss": 2.3791,
      "step": 16900
    },
    {
      "epoch": 0.9449169028958924,
      "grad_norm": 2.8023500442504883,
      "learning_rate": 0.0003425786967557853,
      "loss": 2.3963,
      "step": 17000
    },
    {
      "epoch": 0.9504752376188094,
      "grad_norm": 3.664949655532837,
      "learning_rate": 0.00034165230763529914,
      "loss": 2.3924,
      "step": 17100
    },
    {
      "epoch": 0.9560335723417264,
      "grad_norm": 3.657867431640625,
      "learning_rate": 0.000340725918514813,
      "loss": 2.4154,
      "step": 17200
    },
    {
      "epoch": 0.9615919070646435,
      "grad_norm": 3.8413515090942383,
      "learning_rate": 0.0003397995293943268,
      "loss": 2.3276,
      "step": 17300
    },
    {
      "epoch": 0.9671502417875605,
      "grad_norm": 4.313300132751465,
      "learning_rate": 0.0003388731402738406,
      "loss": 2.3533,
      "step": 17400
    },
    {
      "epoch": 0.9727085765104775,
      "grad_norm": 4.205825328826904,
      "learning_rate": 0.00033794675115335446,
      "loss": 2.4336,
      "step": 17500
    },
    {
      "epoch": 0.9782669112333945,
      "grad_norm": 2.838458299636841,
      "learning_rate": 0.0003370203620328683,
      "loss": 2.3956,
      "step": 17600
    },
    {
      "epoch": 0.9838252459563115,
      "grad_norm": 3.2681920528411865,
      "learning_rate": 0.00033609397291238215,
      "loss": 2.3871,
      "step": 17700
    },
    {
      "epoch": 0.9893835806792285,
      "grad_norm": 3.5006978511810303,
      "learning_rate": 0.00033516758379189594,
      "loss": 2.3966,
      "step": 17800
    },
    {
      "epoch": 0.9949419154021455,
      "grad_norm": 2.4809844493865967,
      "learning_rate": 0.0003342411946714098,
      "loss": 2.437,
      "step": 17900
    },
    {
      "epoch": 1.0,
      "eval_runtime": 213.1124,
      "eval_samples_per_second": 337.667,
      "eval_steps_per_second": 21.106,
      "step": 17991
    },
    {
      "epoch": 1.0005002501250626,
      "grad_norm": 2.7497189044952393,
      "learning_rate": 0.00033331480555092363,
      "loss": 2.3959,
      "step": 18000
    },
    {
      "epoch": 1.0060585848479795,
      "grad_norm": 2.9808082580566406,
      "learning_rate": 0.0003323884164304374,
      "loss": 2.3791,
      "step": 18100
    },
    {
      "epoch": 1.0116169195708966,
      "grad_norm": 3.824009418487549,
      "learning_rate": 0.00033146202730995126,
      "loss": 2.4463,
      "step": 18200
    },
    {
      "epoch": 1.0171752542938135,
      "grad_norm": 3.584442138671875,
      "learning_rate": 0.0003305356381894651,
      "loss": 2.3193,
      "step": 18300
    },
    {
      "epoch": 1.0227335890167306,
      "grad_norm": 3.0202438831329346,
      "learning_rate": 0.0003296185129601838,
      "loss": 2.3866,
      "step": 18400
    },
    {
      "epoch": 1.0282919237396475,
      "grad_norm": 3.265673875808716,
      "learning_rate": 0.00032869212383969767,
      "loss": 2.3837,
      "step": 18500
    },
    {
      "epoch": 1.0338502584625646,
      "grad_norm": 2.798877477645874,
      "learning_rate": 0.00032776573471921146,
      "loss": 2.3705,
      "step": 18600
    },
    {
      "epoch": 1.0394085931854817,
      "grad_norm": 4.006992340087891,
      "learning_rate": 0.0003268393455987253,
      "loss": 2.3624,
      "step": 18700
    },
    {
      "epoch": 1.0449669279083986,
      "grad_norm": 3.565595865249634,
      "learning_rate": 0.00032591295647823915,
      "loss": 2.3758,
      "step": 18800
    },
    {
      "epoch": 1.0505252626313157,
      "grad_norm": 2.875436305999756,
      "learning_rate": 0.00032498656735775294,
      "loss": 2.428,
      "step": 18900
    },
    {
      "epoch": 1.0560835973542326,
      "grad_norm": 3.7394723892211914,
      "learning_rate": 0.0003240601782372668,
      "loss": 2.3307,
      "step": 19000
    },
    {
      "epoch": 1.0616419320771497,
      "grad_norm": 4.995050430297852,
      "learning_rate": 0.00032313378911678057,
      "loss": 2.4099,
      "step": 19100
    },
    {
      "epoch": 1.0672002668000666,
      "grad_norm": 3.9855353832244873,
      "learning_rate": 0.00032220739999629447,
      "loss": 2.3624,
      "step": 19200
    },
    {
      "epoch": 1.0727586015229837,
      "grad_norm": 3.1125073432922363,
      "learning_rate": 0.0003212810108758083,
      "loss": 2.3608,
      "step": 19300
    },
    {
      "epoch": 1.0783169362459006,
      "grad_norm": 3.2628049850463867,
      "learning_rate": 0.0003203546217553221,
      "loss": 2.3761,
      "step": 19400
    },
    {
      "epoch": 1.0838752709688178,
      "grad_norm": 2.7292287349700928,
      "learning_rate": 0.00031942823263483595,
      "loss": 2.3849,
      "step": 19500
    },
    {
      "epoch": 1.0894336056917346,
      "grad_norm": 3.808123826980591,
      "learning_rate": 0.0003185018435143498,
      "loss": 2.3578,
      "step": 19600
    },
    {
      "epoch": 1.0949919404146518,
      "grad_norm": 5.290189266204834,
      "learning_rate": 0.0003175754543938636,
      "loss": 2.3309,
      "step": 19700
    },
    {
      "epoch": 1.1005502751375689,
      "grad_norm": 2.527315378189087,
      "learning_rate": 0.0003166490652733774,
      "loss": 2.3071,
      "step": 19800
    },
    {
      "epoch": 1.1061086098604858,
      "grad_norm": 2.9514379501342773,
      "learning_rate": 0.0003157226761528912,
      "loss": 2.3902,
      "step": 19900
    },
    {
      "epoch": 1.1116669445834029,
      "grad_norm": 3.885544538497925,
      "learning_rate": 0.0003147962870324051,
      "loss": 2.3719,
      "step": 20000
    },
    {
      "epoch": 1.1172252793063198,
      "grad_norm": 2.9444141387939453,
      "learning_rate": 0.00031386989791191896,
      "loss": 2.3578,
      "step": 20100
    },
    {
      "epoch": 1.1227836140292369,
      "grad_norm": 2.7276694774627686,
      "learning_rate": 0.00031294350879143275,
      "loss": 2.2538,
      "step": 20200
    },
    {
      "epoch": 1.1283419487521538,
      "grad_norm": 3.7027792930603027,
      "learning_rate": 0.0003120171196709466,
      "loss": 2.3054,
      "step": 20300
    },
    {
      "epoch": 1.1339002834750709,
      "grad_norm": 2.991628646850586,
      "learning_rate": 0.00031109073055046044,
      "loss": 2.3356,
      "step": 20400
    },
    {
      "epoch": 1.1394586181979878,
      "grad_norm": 2.6593995094299316,
      "learning_rate": 0.0003101643414299742,
      "loss": 2.2693,
      "step": 20500
    },
    {
      "epoch": 1.1450169529209049,
      "grad_norm": 2.9199156761169434,
      "learning_rate": 0.00030923795230948807,
      "loss": 2.3761,
      "step": 20600
    },
    {
      "epoch": 1.150575287643822,
      "grad_norm": 3.1493935585021973,
      "learning_rate": 0.00030831156318900197,
      "loss": 2.3143,
      "step": 20700
    },
    {
      "epoch": 1.1561336223667389,
      "grad_norm": 2.943277359008789,
      "learning_rate": 0.00030738517406851576,
      "loss": 2.3624,
      "step": 20800
    },
    {
      "epoch": 1.161691957089656,
      "grad_norm": 2.9830243587493896,
      "learning_rate": 0.0003064587849480296,
      "loss": 2.3953,
      "step": 20900
    },
    {
      "epoch": 1.167250291812573,
      "grad_norm": 2.6042590141296387,
      "learning_rate": 0.0003055323958275434,
      "loss": 2.3938,
      "step": 21000
    },
    {
      "epoch": 1.17280862653549,
      "grad_norm": 3.073004722595215,
      "learning_rate": 0.0003046152705982621,
      "loss": 2.3689,
      "step": 21100
    },
    {
      "epoch": 1.178366961258407,
      "grad_norm": 2.6618101596832275,
      "learning_rate": 0.00030368888147777595,
      "loss": 2.3128,
      "step": 21200
    },
    {
      "epoch": 1.183925295981324,
      "grad_norm": 3.2558670043945312,
      "learning_rate": 0.00030276249235728974,
      "loss": 2.3392,
      "step": 21300
    },
    {
      "epoch": 1.189483630704241,
      "grad_norm": 3.528033971786499,
      "learning_rate": 0.0003018361032368036,
      "loss": 2.3383,
      "step": 21400
    },
    {
      "epoch": 1.195041965427158,
      "grad_norm": 3.6430611610412598,
      "learning_rate": 0.0003009097141163174,
      "loss": 2.3636,
      "step": 21500
    },
    {
      "epoch": 1.2006003001500751,
      "grad_norm": 3.8214762210845947,
      "learning_rate": 0.0002999833249958313,
      "loss": 2.3346,
      "step": 21600
    },
    {
      "epoch": 1.206158634872992,
      "grad_norm": 4.389652729034424,
      "learning_rate": 0.0002990569358753451,
      "loss": 2.3795,
      "step": 21700
    },
    {
      "epoch": 1.2117169695959091,
      "grad_norm": 3.1983234882354736,
      "learning_rate": 0.0002981305467548589,
      "loss": 2.2719,
      "step": 21800
    },
    {
      "epoch": 1.217275304318826,
      "grad_norm": 2.6790573596954346,
      "learning_rate": 0.00029720415763437275,
      "loss": 2.4204,
      "step": 21900
    },
    {
      "epoch": 1.2228336390417431,
      "grad_norm": 4.059282302856445,
      "learning_rate": 0.0002962777685138866,
      "loss": 2.2938,
      "step": 22000
    },
    {
      "epoch": 1.22839197376466,
      "grad_norm": 3.048311710357666,
      "learning_rate": 0.0002953513793934004,
      "loss": 2.3792,
      "step": 22100
    },
    {
      "epoch": 1.2339503084875771,
      "grad_norm": 2.4973878860473633,
      "learning_rate": 0.00029442499027291423,
      "loss": 2.2946,
      "step": 22200
    },
    {
      "epoch": 1.239508643210494,
      "grad_norm": 2.937943696975708,
      "learning_rate": 0.000293498601152428,
      "loss": 2.3546,
      "step": 22300
    },
    {
      "epoch": 1.2450669779334111,
      "grad_norm": 3.562570571899414,
      "learning_rate": 0.0002925722120319419,
      "loss": 2.3834,
      "step": 22400
    },
    {
      "epoch": 1.2506253126563283,
      "grad_norm": 3.2022440433502197,
      "learning_rate": 0.00029164582291145576,
      "loss": 2.3449,
      "step": 22500
    },
    {
      "epoch": 1.2561836473792451,
      "grad_norm": 2.541745185852051,
      "learning_rate": 0.00029071943379096955,
      "loss": 2.3164,
      "step": 22600
    },
    {
      "epoch": 1.2617419821021623,
      "grad_norm": 2.8519411087036133,
      "learning_rate": 0.0002897930446704834,
      "loss": 2.2838,
      "step": 22700
    },
    {
      "epoch": 1.2673003168250792,
      "grad_norm": 2.995774507522583,
      "learning_rate": 0.00028886665554999724,
      "loss": 2.3093,
      "step": 22800
    },
    {
      "epoch": 1.2728586515479963,
      "grad_norm": 4.796917915344238,
      "learning_rate": 0.00028794026642951103,
      "loss": 2.3439,
      "step": 22900
    },
    {
      "epoch": 1.2784169862709132,
      "grad_norm": 4.09352970123291,
      "learning_rate": 0.0002870138773090249,
      "loss": 2.3525,
      "step": 23000
    },
    {
      "epoch": 1.2839753209938303,
      "grad_norm": 2.9241018295288086,
      "learning_rate": 0.0002860874881885387,
      "loss": 2.3546,
      "step": 23100
    },
    {
      "epoch": 1.2895336557167472,
      "grad_norm": 2.860395669937134,
      "learning_rate": 0.00028517036295925744,
      "loss": 2.346,
      "step": 23200
    },
    {
      "epoch": 1.2950919904396643,
      "grad_norm": 2.7758216857910156,
      "learning_rate": 0.0002842439738387713,
      "loss": 2.3083,
      "step": 23300
    },
    {
      "epoch": 1.3006503251625814,
      "grad_norm": 2.8922359943389893,
      "learning_rate": 0.0002833175847182851,
      "loss": 2.3244,
      "step": 23400
    },
    {
      "epoch": 1.3062086598854983,
      "grad_norm": 3.648071527481079,
      "learning_rate": 0.0002823911955977989,
      "loss": 2.3289,
      "step": 23500
    },
    {
      "epoch": 1.3117669946084154,
      "grad_norm": 3.3252148628234863,
      "learning_rate": 0.00028146480647731276,
      "loss": 2.4181,
      "step": 23600
    },
    {
      "epoch": 1.3173253293313323,
      "grad_norm": 4.02315616607666,
      "learning_rate": 0.00028053841735682655,
      "loss": 2.2995,
      "step": 23700
    },
    {
      "epoch": 1.3228836640542494,
      "grad_norm": 3.6594350337982178,
      "learning_rate": 0.0002796120282363404,
      "loss": 2.3285,
      "step": 23800
    },
    {
      "epoch": 1.3284419987771663,
      "grad_norm": 4.348306655883789,
      "learning_rate": 0.0002786856391158542,
      "loss": 2.3369,
      "step": 23900
    },
    {
      "epoch": 1.3340003335000834,
      "grad_norm": 3.8326609134674072,
      "learning_rate": 0.0002777592499953681,
      "loss": 2.2813,
      "step": 24000
    },
    {
      "epoch": 1.3395586682230003,
      "grad_norm": 4.38558292388916,
      "learning_rate": 0.00027683286087488193,
      "loss": 2.3368,
      "step": 24100
    },
    {
      "epoch": 1.3451170029459174,
      "grad_norm": 3.501079559326172,
      "learning_rate": 0.0002759064717543957,
      "loss": 2.3546,
      "step": 24200
    },
    {
      "epoch": 1.3506753376688345,
      "grad_norm": 3.668149471282959,
      "learning_rate": 0.00027498008263390956,
      "loss": 2.3062,
      "step": 24300
    },
    {
      "epoch": 1.3562336723917514,
      "grad_norm": 4.483002185821533,
      "learning_rate": 0.0002740536935134234,
      "loss": 2.3825,
      "step": 24400
    },
    {
      "epoch": 1.3617920071146685,
      "grad_norm": 3.7019903659820557,
      "learning_rate": 0.0002731273043929372,
      "loss": 2.2693,
      "step": 24500
    },
    {
      "epoch": 1.3673503418375854,
      "grad_norm": 2.9095749855041504,
      "learning_rate": 0.00027220091527245104,
      "loss": 2.3252,
      "step": 24600
    },
    {
      "epoch": 1.3729086765605025,
      "grad_norm": 2.6015470027923584,
      "learning_rate": 0.00027127452615196483,
      "loss": 2.2684,
      "step": 24700
    },
    {
      "epoch": 1.3784670112834194,
      "grad_norm": 3.739347457885742,
      "learning_rate": 0.00027034813703147873,
      "loss": 2.3699,
      "step": 24800
    },
    {
      "epoch": 1.3840253460063365,
      "grad_norm": 2.873169422149658,
      "learning_rate": 0.00026942174791099257,
      "loss": 2.3543,
      "step": 24900
    },
    {
      "epoch": 1.3895836807292534,
      "grad_norm": 3.9348623752593994,
      "learning_rate": 0.00026849535879050636,
      "loss": 2.2923,
      "step": 25000
    },
    {
      "epoch": 1.3951420154521705,
      "grad_norm": 4.024529457092285,
      "learning_rate": 0.0002675689696700202,
      "loss": 2.3439,
      "step": 25100
    },
    {
      "epoch": 1.4007003501750876,
      "grad_norm": 3.6768198013305664,
      "learning_rate": 0.00026664258054953405,
      "loss": 2.3876,
      "step": 25200
    },
    {
      "epoch": 1.4062586848980045,
      "grad_norm": 3.43015718460083,
      "learning_rate": 0.00026571619142904784,
      "loss": 2.3466,
      "step": 25300
    },
    {
      "epoch": 1.4118170196209217,
      "grad_norm": 3.3382632732391357,
      "learning_rate": 0.0002647898023085617,
      "loss": 2.3229,
      "step": 25400
    },
    {
      "epoch": 1.4173753543438385,
      "grad_norm": 3.4400577545166016,
      "learning_rate": 0.00026386341318807553,
      "loss": 2.3731,
      "step": 25500
    },
    {
      "epoch": 1.4229336890667557,
      "grad_norm": 3.0692341327667236,
      "learning_rate": 0.00026293702406758937,
      "loss": 2.3182,
      "step": 25600
    },
    {
      "epoch": 1.4284920237896725,
      "grad_norm": 2.981257915496826,
      "learning_rate": 0.0002620106349471032,
      "loss": 2.2667,
      "step": 25700
    },
    {
      "epoch": 1.4340503585125897,
      "grad_norm": 3.1256802082061768,
      "learning_rate": 0.0002610935097178219,
      "loss": 2.2799,
      "step": 25800
    },
    {
      "epoch": 1.4396086932355066,
      "grad_norm": 3.5983285903930664,
      "learning_rate": 0.0002601671205973357,
      "loss": 2.3252,
      "step": 25900
    },
    {
      "epoch": 1.4451670279584237,
      "grad_norm": 3.356708526611328,
      "learning_rate": 0.00025924073147684957,
      "loss": 2.2614,
      "step": 26000
    },
    {
      "epoch": 1.4507253626813408,
      "grad_norm": 2.93110728263855,
      "learning_rate": 0.00025831434235636336,
      "loss": 2.3052,
      "step": 26100
    },
    {
      "epoch": 1.4562836974042577,
      "grad_norm": 2.702329158782959,
      "learning_rate": 0.0002573879532358772,
      "loss": 2.2824,
      "step": 26200
    },
    {
      "epoch": 1.4618420321271746,
      "grad_norm": 2.373631000518799,
      "learning_rate": 0.000256461564115391,
      "loss": 2.3504,
      "step": 26300
    },
    {
      "epoch": 1.4674003668500917,
      "grad_norm": 3.6872928142547607,
      "learning_rate": 0.00025553517499490484,
      "loss": 2.286,
      "step": 26400
    },
    {
      "epoch": 1.4729587015730088,
      "grad_norm": 3.5130703449249268,
      "learning_rate": 0.00025460878587441874,
      "loss": 2.3502,
      "step": 26500
    },
    {
      "epoch": 1.4785170362959257,
      "grad_norm": 2.9474122524261475,
      "learning_rate": 0.0002536823967539325,
      "loss": 2.3062,
      "step": 26600
    },
    {
      "epoch": 1.4840753710188428,
      "grad_norm": 3.448390483856201,
      "learning_rate": 0.00025275600763344637,
      "loss": 2.3096,
      "step": 26700
    },
    {
      "epoch": 1.4896337057417597,
      "grad_norm": 3.6597182750701904,
      "learning_rate": 0.0002518296185129602,
      "loss": 2.2274,
      "step": 26800
    },
    {
      "epoch": 1.4951920404646768,
      "grad_norm": 3.2535789012908936,
      "learning_rate": 0.000250903229392474,
      "loss": 2.3204,
      "step": 26900
    },
    {
      "epoch": 1.500750375187594,
      "grad_norm": 3.567807674407959,
      "learning_rate": 0.00024997684027198785,
      "loss": 2.3703,
      "step": 27000
    },
    {
      "epoch": 1.5063087099105108,
      "grad_norm": 2.887366771697998,
      "learning_rate": 0.0002490504511515017,
      "loss": 2.2884,
      "step": 27100
    },
    {
      "epoch": 1.5118670446334277,
      "grad_norm": 3.3445069789886475,
      "learning_rate": 0.0002481240620310155,
      "loss": 2.4123,
      "step": 27200
    },
    {
      "epoch": 1.5174253793563448,
      "grad_norm": 2.571953773498535,
      "learning_rate": 0.0002471976729105293,
      "loss": 2.2779,
      "step": 27300
    },
    {
      "epoch": 1.522983714079262,
      "grad_norm": 3.7828378677368164,
      "learning_rate": 0.00024627128379004317,
      "loss": 2.3107,
      "step": 27400
    },
    {
      "epoch": 1.528542048802179,
      "grad_norm": 3.8167665004730225,
      "learning_rate": 0.000245344894669557,
      "loss": 2.3669,
      "step": 27500
    },
    {
      "epoch": 1.534100383525096,
      "grad_norm": 3.8866124153137207,
      "learning_rate": 0.0002444185055490708,
      "loss": 2.3133,
      "step": 27600
    },
    {
      "epoch": 1.5396587182480128,
      "grad_norm": 4.89750337600708,
      "learning_rate": 0.00024349211642858467,
      "loss": 2.3552,
      "step": 27700
    },
    {
      "epoch": 1.54521705297093,
      "grad_norm": 3.3285140991210938,
      "learning_rate": 0.0002425657273080985,
      "loss": 2.3254,
      "step": 27800
    },
    {
      "epoch": 1.550775387693847,
      "grad_norm": 4.8960280418396,
      "learning_rate": 0.0002416486020788172,
      "loss": 2.3618,
      "step": 27900
    },
    {
      "epoch": 1.556333722416764,
      "grad_norm": 3.048023223876953,
      "learning_rate": 0.00024072221295833103,
      "loss": 2.3223,
      "step": 28000
    },
    {
      "epoch": 1.5618920571396808,
      "grad_norm": 2.4635558128356934,
      "learning_rate": 0.00023979582383784484,
      "loss": 2.2241,
      "step": 28100
    },
    {
      "epoch": 1.567450391862598,
      "grad_norm": 3.9391119480133057,
      "learning_rate": 0.0002388694347173587,
      "loss": 2.308,
      "step": 28200
    },
    {
      "epoch": 1.573008726585515,
      "grad_norm": 3.195760488510132,
      "learning_rate": 0.00023794304559687253,
      "loss": 2.393,
      "step": 28300
    },
    {
      "epoch": 1.578567061308432,
      "grad_norm": 3.0661709308624268,
      "learning_rate": 0.00023701665647638635,
      "loss": 2.2477,
      "step": 28400
    },
    {
      "epoch": 1.584125396031349,
      "grad_norm": 2.915454149246216,
      "learning_rate": 0.00023609026735590017,
      "loss": 2.3071,
      "step": 28500
    },
    {
      "epoch": 1.589683730754266,
      "grad_norm": 2.86879301071167,
      "learning_rate": 0.000235163878235414,
      "loss": 2.3645,
      "step": 28600
    },
    {
      "epoch": 1.595242065477183,
      "grad_norm": 3.1491122245788574,
      "learning_rate": 0.00023423748911492785,
      "loss": 2.332,
      "step": 28700
    },
    {
      "epoch": 1.6008004002001002,
      "grad_norm": 2.5516483783721924,
      "learning_rate": 0.00023331109999444167,
      "loss": 2.3015,
      "step": 28800
    },
    {
      "epoch": 1.606358734923017,
      "grad_norm": 3.2343969345092773,
      "learning_rate": 0.0002323847108739555,
      "loss": 2.321,
      "step": 28900
    },
    {
      "epoch": 1.611917069645934,
      "grad_norm": 3.61881685256958,
      "learning_rate": 0.00023145832175346933,
      "loss": 2.3846,
      "step": 29000
    },
    {
      "epoch": 1.617475404368851,
      "grad_norm": 3.505850315093994,
      "learning_rate": 0.00023053193263298318,
      "loss": 2.3264,
      "step": 29100
    },
    {
      "epoch": 1.6230337390917682,
      "grad_norm": 4.052062034606934,
      "learning_rate": 0.000229605543512497,
      "loss": 2.3719,
      "step": 29200
    },
    {
      "epoch": 1.628592073814685,
      "grad_norm": 3.8094708919525146,
      "learning_rate": 0.0002286791543920108,
      "loss": 2.3401,
      "step": 29300
    },
    {
      "epoch": 1.6341504085376022,
      "grad_norm": 4.045341968536377,
      "learning_rate": 0.00022775276527152465,
      "loss": 2.2124,
      "step": 29400
    },
    {
      "epoch": 1.639708743260519,
      "grad_norm": 3.1173222064971924,
      "learning_rate": 0.0002268263761510385,
      "loss": 2.2904,
      "step": 29500
    },
    {
      "epoch": 1.6452670779834362,
      "grad_norm": 3.883795976638794,
      "learning_rate": 0.00022589998703055232,
      "loss": 2.3232,
      "step": 29600
    },
    {
      "epoch": 1.6508254127063533,
      "grad_norm": 2.9878969192504883,
      "learning_rate": 0.00022497359791006613,
      "loss": 2.3531,
      "step": 29700
    },
    {
      "epoch": 1.6563837474292702,
      "grad_norm": 3.9987053871154785,
      "learning_rate": 0.00022404720878957998,
      "loss": 2.3516,
      "step": 29800
    },
    {
      "epoch": 1.661942082152187,
      "grad_norm": 2.907966375350952,
      "learning_rate": 0.00022312081966909382,
      "loss": 2.3332,
      "step": 29900
    },
    {
      "epoch": 1.6675004168751042,
      "grad_norm": 3.3758914470672607,
      "learning_rate": 0.00022219443054860764,
      "loss": 2.3203,
      "step": 30000
    },
    {
      "epoch": 1.6730587515980213,
      "grad_norm": 4.3941850662231445,
      "learning_rate": 0.00022127730531932633,
      "loss": 2.2293,
      "step": 30100
    },
    {
      "epoch": 1.6786170863209382,
      "grad_norm": 3.7942733764648438,
      "learning_rate": 0.00022035091619884017,
      "loss": 2.3267,
      "step": 30200
    },
    {
      "epoch": 1.6841754210438553,
      "grad_norm": 3.5306010246276855,
      "learning_rate": 0.000219424527078354,
      "loss": 2.3051,
      "step": 30300
    },
    {
      "epoch": 1.6897337557667722,
      "grad_norm": 2.7985012531280518,
      "learning_rate": 0.00021849813795786783,
      "loss": 2.3032,
      "step": 30400
    },
    {
      "epoch": 1.6952920904896893,
      "grad_norm": 3.503164291381836,
      "learning_rate": 0.00021757174883738165,
      "loss": 2.2876,
      "step": 30500
    },
    {
      "epoch": 1.7008504252126064,
      "grad_norm": 4.073112964630127,
      "learning_rate": 0.0002166453597168955,
      "loss": 2.3279,
      "step": 30600
    },
    {
      "epoch": 1.7064087599355233,
      "grad_norm": 3.7558648586273193,
      "learning_rate": 0.0002157189705964093,
      "loss": 2.3804,
      "step": 30700
    },
    {
      "epoch": 1.7119670946584402,
      "grad_norm": 3.392747640609741,
      "learning_rate": 0.00021479258147592316,
      "loss": 2.3011,
      "step": 30800
    },
    {
      "epoch": 1.7175254293813573,
      "grad_norm": 4.566091060638428,
      "learning_rate": 0.00021386619235543697,
      "loss": 2.2459,
      "step": 30900
    },
    {
      "epoch": 1.7230837641042744,
      "grad_norm": 4.578968048095703,
      "learning_rate": 0.00021293980323495082,
      "loss": 2.2784,
      "step": 31000
    },
    {
      "epoch": 1.7286420988271913,
      "grad_norm": 3.153482675552368,
      "learning_rate": 0.00021201341411446463,
      "loss": 2.3205,
      "step": 31100
    },
    {
      "epoch": 1.7342004335501082,
      "grad_norm": 4.0504279136657715,
      "learning_rate": 0.00021108702499397848,
      "loss": 2.3692,
      "step": 31200
    },
    {
      "epoch": 1.7397587682730253,
      "grad_norm": 3.3217933177948,
      "learning_rate": 0.0002101606358734923,
      "loss": 2.2907,
      "step": 31300
    },
    {
      "epoch": 1.7453171029959424,
      "grad_norm": 4.340034484863281,
      "learning_rate": 0.00020923424675300614,
      "loss": 2.2945,
      "step": 31400
    },
    {
      "epoch": 1.7508754377188596,
      "grad_norm": 3.5785348415374756,
      "learning_rate": 0.00020830785763251998,
      "loss": 2.4133,
      "step": 31500
    },
    {
      "epoch": 1.7564337724417765,
      "grad_norm": 2.483463764190674,
      "learning_rate": 0.0002073814685120338,
      "loss": 2.2888,
      "step": 31600
    },
    {
      "epoch": 1.7619921071646933,
      "grad_norm": 3.182701587677002,
      "learning_rate": 0.00020645507939154762,
      "loss": 2.3263,
      "step": 31700
    },
    {
      "epoch": 1.7675504418876105,
      "grad_norm": 2.6162755489349365,
      "learning_rate": 0.00020552869027106146,
      "loss": 2.2634,
      "step": 31800
    },
    {
      "epoch": 1.7731087766105276,
      "grad_norm": 2.9349124431610107,
      "learning_rate": 0.0002046023011505753,
      "loss": 2.2907,
      "step": 31900
    },
    {
      "epoch": 1.7786671113334445,
      "grad_norm": 4.236143112182617,
      "learning_rate": 0.00020367591203008912,
      "loss": 2.2982,
      "step": 32000
    },
    {
      "epoch": 1.7842254460563614,
      "grad_norm": 3.2520699501037598,
      "learning_rate": 0.00020275878680080781,
      "loss": 2.2304,
      "step": 32100
    },
    {
      "epoch": 1.7897837807792785,
      "grad_norm": 2.9135539531707764,
      "learning_rate": 0.00020183239768032163,
      "loss": 2.3036,
      "step": 32200
    },
    {
      "epoch": 1.7953421155021956,
      "grad_norm": 3.301486015319824,
      "learning_rate": 0.00020090600855983548,
      "loss": 2.2931,
      "step": 32300
    },
    {
      "epoch": 1.8009004502251127,
      "grad_norm": 2.5180916786193848,
      "learning_rate": 0.00019997961943934932,
      "loss": 2.2777,
      "step": 32400
    },
    {
      "epoch": 1.8064587849480296,
      "grad_norm": 4.226002216339111,
      "learning_rate": 0.00019905323031886314,
      "loss": 2.2597,
      "step": 32500
    },
    {
      "epoch": 1.8120171196709465,
      "grad_norm": 2.9869978427886963,
      "learning_rate": 0.00019812684119837698,
      "loss": 2.3066,
      "step": 32600
    },
    {
      "epoch": 1.8175754543938636,
      "grad_norm": 3.177776575088501,
      "learning_rate": 0.0001972004520778908,
      "loss": 2.2822,
      "step": 32700
    },
    {
      "epoch": 1.8231337891167807,
      "grad_norm": 2.802577257156372,
      "learning_rate": 0.00019627406295740464,
      "loss": 2.3457,
      "step": 32800
    },
    {
      "epoch": 1.8286921238396976,
      "grad_norm": 3.5471129417419434,
      "learning_rate": 0.00019534767383691846,
      "loss": 2.2616,
      "step": 32900
    },
    {
      "epoch": 1.8342504585626145,
      "grad_norm": 3.263761043548584,
      "learning_rate": 0.0001944212847164323,
      "loss": 2.3014,
      "step": 33000
    },
    {
      "epoch": 1.8398087932855316,
      "grad_norm": 3.171278715133667,
      "learning_rate": 0.00019349489559594612,
      "loss": 2.2546,
      "step": 33100
    },
    {
      "epoch": 1.8453671280084487,
      "grad_norm": 3.2812232971191406,
      "learning_rate": 0.00019256850647545996,
      "loss": 2.2807,
      "step": 33200
    },
    {
      "epoch": 1.8509254627313658,
      "grad_norm": 4.58549690246582,
      "learning_rate": 0.00019164211735497378,
      "loss": 2.2471,
      "step": 33300
    },
    {
      "epoch": 1.8564837974542827,
      "grad_norm": 3.7740559577941895,
      "learning_rate": 0.00019071572823448762,
      "loss": 2.2252,
      "step": 33400
    },
    {
      "epoch": 1.8620421321771996,
      "grad_norm": 3.6603565216064453,
      "learning_rate": 0.00018978933911400144,
      "loss": 2.2969,
      "step": 33500
    },
    {
      "epoch": 1.8676004669001167,
      "grad_norm": 3.6545252799987793,
      "learning_rate": 0.00018886294999351529,
      "loss": 2.2402,
      "step": 33600
    },
    {
      "epoch": 1.8731588016230338,
      "grad_norm": 3.8471667766571045,
      "learning_rate": 0.0001879365608730291,
      "loss": 2.2827,
      "step": 33700
    },
    {
      "epoch": 1.8787171363459507,
      "grad_norm": 3.4681169986724854,
      "learning_rate": 0.00018701017175254295,
      "loss": 2.2989,
      "step": 33800
    },
    {
      "epoch": 1.8842754710688676,
      "grad_norm": 4.295457363128662,
      "learning_rate": 0.00018608378263205676,
      "loss": 2.3415,
      "step": 33900
    },
    {
      "epoch": 1.8898338057917847,
      "grad_norm": 3.5387911796569824,
      "learning_rate": 0.0001851573935115706,
      "loss": 2.3339,
      "step": 34000
    },
    {
      "epoch": 1.8953921405147018,
      "grad_norm": 3.811830997467041,
      "learning_rate": 0.00018423100439108443,
      "loss": 2.2496,
      "step": 34100
    },
    {
      "epoch": 1.900950475237619,
      "grad_norm": 3.5194764137268066,
      "learning_rate": 0.00018331387916180312,
      "loss": 2.3575,
      "step": 34200
    },
    {
      "epoch": 1.9065088099605358,
      "grad_norm": 4.548266887664795,
      "learning_rate": 0.00018238749004131696,
      "loss": 2.2264,
      "step": 34300
    },
    {
      "epoch": 1.9120671446834527,
      "grad_norm": 3.5495316982269287,
      "learning_rate": 0.00018146110092083078,
      "loss": 2.3405,
      "step": 34400
    },
    {
      "epoch": 1.9176254794063698,
      "grad_norm": 2.368492841720581,
      "learning_rate": 0.00018053471180034462,
      "loss": 2.2355,
      "step": 34500
    },
    {
      "epoch": 1.923183814129287,
      "grad_norm": 4.599992752075195,
      "learning_rate": 0.00017960832267985844,
      "loss": 2.2616,
      "step": 34600
    },
    {
      "epoch": 1.9287421488522039,
      "grad_norm": 3.4184799194335938,
      "learning_rate": 0.00017868193355937228,
      "loss": 2.2964,
      "step": 34700
    },
    {
      "epoch": 1.9343004835751207,
      "grad_norm": 3.2053334712982178,
      "learning_rate": 0.00017775554443888613,
      "loss": 2.3071,
      "step": 34800
    },
    {
      "epoch": 1.9398588182980379,
      "grad_norm": 3.064089298248291,
      "learning_rate": 0.00017682915531839994,
      "loss": 2.2958,
      "step": 34900
    },
    {
      "epoch": 1.945417153020955,
      "grad_norm": 3.115879535675049,
      "learning_rate": 0.0001759027661979138,
      "loss": 2.4005,
      "step": 35000
    },
    {
      "epoch": 1.950975487743872,
      "grad_norm": 4.241665363311768,
      "learning_rate": 0.0001749763770774276,
      "loss": 2.3421,
      "step": 35100
    },
    {
      "epoch": 1.956533822466789,
      "grad_norm": 3.6941609382629395,
      "learning_rate": 0.00017404998795694145,
      "loss": 2.2452,
      "step": 35200
    },
    {
      "epoch": 1.9620921571897059,
      "grad_norm": 2.754927635192871,
      "learning_rate": 0.00017312359883645527,
      "loss": 2.2791,
      "step": 35300
    },
    {
      "epoch": 1.967650491912623,
      "grad_norm": 2.670074224472046,
      "learning_rate": 0.0001721972097159691,
      "loss": 2.3272,
      "step": 35400
    },
    {
      "epoch": 1.97320882663554,
      "grad_norm": 4.863351821899414,
      "learning_rate": 0.00017127082059548293,
      "loss": 2.3074,
      "step": 35500
    },
    {
      "epoch": 1.978767161358457,
      "grad_norm": 3.0901753902435303,
      "learning_rate": 0.00017034443147499677,
      "loss": 2.2608,
      "step": 35600
    },
    {
      "epoch": 1.9843254960813739,
      "grad_norm": 3.194368839263916,
      "learning_rate": 0.0001694180423545106,
      "loss": 2.2907,
      "step": 35700
    },
    {
      "epoch": 1.989883830804291,
      "grad_norm": 3.5181117057800293,
      "learning_rate": 0.00016849165323402443,
      "loss": 2.3304,
      "step": 35800
    },
    {
      "epoch": 1.995442165527208,
      "grad_norm": 3.735055685043335,
      "learning_rate": 0.00016756526411353825,
      "loss": 2.229,
      "step": 35900
    },
    {
      "epoch": 2.0,
      "eval_runtime": 210.7598,
      "eval_samples_per_second": 341.436,
      "eval_steps_per_second": 21.342,
      "step": 35982
    },
    {
      "epoch": 2.001000500250125,
      "grad_norm": 2.6020333766937256,
      "learning_rate": 0.0001666388749930521,
      "loss": 2.3484,
      "step": 36000
    },
    {
      "epoch": 2.006558834973042,
      "grad_norm": 4.232458591461182,
      "learning_rate": 0.0001657124858725659,
      "loss": 2.2497,
      "step": 36100
    },
    {
      "epoch": 2.012117169695959,
      "grad_norm": 2.7018516063690186,
      "learning_rate": 0.00016478609675207975,
      "loss": 2.3642,
      "step": 36200
    },
    {
      "epoch": 2.017675504418876,
      "grad_norm": 3.540071487426758,
      "learning_rate": 0.00016385970763159357,
      "loss": 2.272,
      "step": 36300
    },
    {
      "epoch": 2.0232338391417932,
      "grad_norm": 2.3098342418670654,
      "learning_rate": 0.00016293331851110742,
      "loss": 2.3344,
      "step": 36400
    },
    {
      "epoch": 2.0287921738647103,
      "grad_norm": 2.4638922214508057,
      "learning_rate": 0.0001620161932818261,
      "loss": 2.2748,
      "step": 36500
    },
    {
      "epoch": 2.034350508587627,
      "grad_norm": 2.8493752479553223,
      "learning_rate": 0.00016108980416133992,
      "loss": 2.2669,
      "step": 36600
    },
    {
      "epoch": 2.039908843310544,
      "grad_norm": 6.446722030639648,
      "learning_rate": 0.00016016341504085377,
      "loss": 2.2167,
      "step": 36700
    },
    {
      "epoch": 2.0454671780334612,
      "grad_norm": 2.634093999862671,
      "learning_rate": 0.00015923702592036758,
      "loss": 2.2527,
      "step": 36800
    },
    {
      "epoch": 2.0510255127563783,
      "grad_norm": 2.8151023387908936,
      "learning_rate": 0.00015831063679988143,
      "loss": 2.2303,
      "step": 36900
    },
    {
      "epoch": 2.056583847479295,
      "grad_norm": 3.153388738632202,
      "learning_rate": 0.00015738424767939525,
      "loss": 2.2536,
      "step": 37000
    },
    {
      "epoch": 2.062142182202212,
      "grad_norm": 3.2480857372283936,
      "learning_rate": 0.0001564578585589091,
      "loss": 2.3289,
      "step": 37100
    },
    {
      "epoch": 2.0677005169251292,
      "grad_norm": 2.851106643676758,
      "learning_rate": 0.0001555314694384229,
      "loss": 2.2778,
      "step": 37200
    },
    {
      "epoch": 2.0732588516480464,
      "grad_norm": 3.1981418132781982,
      "learning_rate": 0.00015460508031793675,
      "loss": 2.2267,
      "step": 37300
    },
    {
      "epoch": 2.0788171863709635,
      "grad_norm": 2.8993964195251465,
      "learning_rate": 0.0001536786911974506,
      "loss": 2.2651,
      "step": 37400
    },
    {
      "epoch": 2.08437552109388,
      "grad_norm": 2.7189218997955322,
      "learning_rate": 0.0001527523020769644,
      "loss": 2.2582,
      "step": 37500
    },
    {
      "epoch": 2.0899338558167972,
      "grad_norm": 2.5766210556030273,
      "learning_rate": 0.00015182591295647823,
      "loss": 2.2842,
      "step": 37600
    },
    {
      "epoch": 2.0954921905397144,
      "grad_norm": 2.6479380130767822,
      "learning_rate": 0.00015089952383599207,
      "loss": 2.2834,
      "step": 37700
    },
    {
      "epoch": 2.1010505252626315,
      "grad_norm": 3.4104013442993164,
      "learning_rate": 0.00014997313471550592,
      "loss": 2.2932,
      "step": 37800
    },
    {
      "epoch": 2.106608859985548,
      "grad_norm": 3.869614362716675,
      "learning_rate": 0.00014904674559501973,
      "loss": 2.258,
      "step": 37900
    },
    {
      "epoch": 2.1121671947084653,
      "grad_norm": 2.9946606159210205,
      "learning_rate": 0.00014812035647453355,
      "loss": 2.2452,
      "step": 38000
    },
    {
      "epoch": 2.1177255294313824,
      "grad_norm": 5.300754070281982,
      "learning_rate": 0.0001471939673540474,
      "loss": 2.2555,
      "step": 38100
    },
    {
      "epoch": 2.1232838641542995,
      "grad_norm": 3.019921064376831,
      "learning_rate": 0.00014626757823356124,
      "loss": 2.224,
      "step": 38200
    },
    {
      "epoch": 2.1288421988772166,
      "grad_norm": 2.818495988845825,
      "learning_rate": 0.00014534118911307506,
      "loss": 2.1955,
      "step": 38300
    },
    {
      "epoch": 2.1344005336001333,
      "grad_norm": 4.025593280792236,
      "learning_rate": 0.00014442406388379375,
      "loss": 2.3372,
      "step": 38400
    },
    {
      "epoch": 2.1399588683230504,
      "grad_norm": 3.6025421619415283,
      "learning_rate": 0.0001434976747633076,
      "loss": 2.3338,
      "step": 38500
    },
    {
      "epoch": 2.1455172030459675,
      "grad_norm": 3.0643744468688965,
      "learning_rate": 0.0001425712856428214,
      "loss": 2.2788,
      "step": 38600
    },
    {
      "epoch": 2.1510755377688846,
      "grad_norm": 3.3702805042266846,
      "learning_rate": 0.00014164489652233525,
      "loss": 2.2479,
      "step": 38700
    },
    {
      "epoch": 2.1566338724918013,
      "grad_norm": 3.538275718688965,
      "learning_rate": 0.00014071850740184907,
      "loss": 2.2192,
      "step": 38800
    },
    {
      "epoch": 2.1621922072147184,
      "grad_norm": 3.5178534984588623,
      "learning_rate": 0.00013979211828136291,
      "loss": 2.3274,
      "step": 38900
    },
    {
      "epoch": 2.1677505419376355,
      "grad_norm": 3.7348084449768066,
      "learning_rate": 0.00013886572916087673,
      "loss": 2.2546,
      "step": 39000
    },
    {
      "epoch": 2.1733088766605526,
      "grad_norm": 2.8668947219848633,
      "learning_rate": 0.00013793934004039057,
      "loss": 2.2757,
      "step": 39100
    },
    {
      "epoch": 2.1788672113834693,
      "grad_norm": 3.0794918537139893,
      "learning_rate": 0.0001370129509199044,
      "loss": 2.245,
      "step": 39200
    },
    {
      "epoch": 2.1844255461063864,
      "grad_norm": 3.0928821563720703,
      "learning_rate": 0.00013608656179941824,
      "loss": 2.2362,
      "step": 39300
    },
    {
      "epoch": 2.1899838808293035,
      "grad_norm": 4.438195705413818,
      "learning_rate": 0.00013516017267893208,
      "loss": 2.2573,
      "step": 39400
    },
    {
      "epoch": 2.1955422155522206,
      "grad_norm": 2.9614434242248535,
      "learning_rate": 0.0001342337835584459,
      "loss": 2.2229,
      "step": 39500
    },
    {
      "epoch": 2.2011005502751377,
      "grad_norm": 3.337982416152954,
      "learning_rate": 0.00013330739443795971,
      "loss": 2.2413,
      "step": 39600
    },
    {
      "epoch": 2.2066588849980544,
      "grad_norm": 2.602626323699951,
      "learning_rate": 0.00013238100531747356,
      "loss": 2.2636,
      "step": 39700
    },
    {
      "epoch": 2.2122172197209715,
      "grad_norm": 2.596564769744873,
      "learning_rate": 0.0001314546161969874,
      "loss": 2.2948,
      "step": 39800
    },
    {
      "epoch": 2.2177755544438886,
      "grad_norm": 3.674177885055542,
      "learning_rate": 0.00013052822707650122,
      "loss": 2.3086,
      "step": 39900
    },
    {
      "epoch": 2.2233338891668057,
      "grad_norm": 3.7974653244018555,
      "learning_rate": 0.00012960183795601504,
      "loss": 2.217,
      "step": 40000
    },
    {
      "epoch": 2.228892223889723,
      "grad_norm": 3.4789257049560547,
      "learning_rate": 0.00012867544883552888,
      "loss": 2.2206,
      "step": 40100
    },
    {
      "epoch": 2.2344505586126395,
      "grad_norm": 3.475925922393799,
      "learning_rate": 0.00012774905971504272,
      "loss": 2.2816,
      "step": 40200
    },
    {
      "epoch": 2.2400088933355566,
      "grad_norm": 3.6527225971221924,
      "learning_rate": 0.00012682267059455654,
      "loss": 2.3093,
      "step": 40300
    },
    {
      "epoch": 2.2455672280584738,
      "grad_norm": 3.1405489444732666,
      "learning_rate": 0.00012589628147407036,
      "loss": 2.377,
      "step": 40400
    },
    {
      "epoch": 2.251125562781391,
      "grad_norm": 2.947239398956299,
      "learning_rate": 0.0001249698923535842,
      "loss": 2.2872,
      "step": 40500
    },
    {
      "epoch": 2.2566838975043075,
      "grad_norm": 2.977829694747925,
      "learning_rate": 0.00012404350323309802,
      "loss": 2.2365,
      "step": 40600
    },
    {
      "epoch": 2.2622422322272246,
      "grad_norm": 2.77634596824646,
      "learning_rate": 0.00012311711411261186,
      "loss": 2.2607,
      "step": 40700
    },
    {
      "epoch": 2.2678005669501418,
      "grad_norm": 2.71993088722229,
      "learning_rate": 0.00012219072499212568,
      "loss": 2.2413,
      "step": 40800
    },
    {
      "epoch": 2.273358901673059,
      "grad_norm": 4.789777755737305,
      "learning_rate": 0.00012126433587163952,
      "loss": 2.261,
      "step": 40900
    },
    {
      "epoch": 2.2789172363959755,
      "grad_norm": 2.9119386672973633,
      "learning_rate": 0.00012033794675115336,
      "loss": 2.2325,
      "step": 41000
    },
    {
      "epoch": 2.2844755711188927,
      "grad_norm": 3.7562432289123535,
      "learning_rate": 0.00011941155763066719,
      "loss": 2.3233,
      "step": 41100
    },
    {
      "epoch": 2.2900339058418098,
      "grad_norm": 3.2551090717315674,
      "learning_rate": 0.00011848516851018102,
      "loss": 2.2274,
      "step": 41200
    },
    {
      "epoch": 2.295592240564727,
      "grad_norm": 3.2555835247039795,
      "learning_rate": 0.00011755877938969485,
      "loss": 2.2464,
      "step": 41300
    },
    {
      "epoch": 2.301150575287644,
      "grad_norm": 3.360274314880371,
      "learning_rate": 0.00011663239026920868,
      "loss": 2.3114,
      "step": 41400
    },
    {
      "epoch": 2.3067089100105607,
      "grad_norm": 3.611954927444458,
      "learning_rate": 0.00011570600114872251,
      "loss": 2.2545,
      "step": 41500
    },
    {
      "epoch": 2.3122672447334778,
      "grad_norm": 2.94146466255188,
      "learning_rate": 0.00011477961202823635,
      "loss": 2.2818,
      "step": 41600
    },
    {
      "epoch": 2.317825579456395,
      "grad_norm": 3.8388757705688477,
      "learning_rate": 0.00011385322290775017,
      "loss": 2.2367,
      "step": 41700
    },
    {
      "epoch": 2.323383914179312,
      "grad_norm": 3.279381513595581,
      "learning_rate": 0.00011292683378726401,
      "loss": 2.2385,
      "step": 41800
    },
    {
      "epoch": 2.328942248902229,
      "grad_norm": 3.9599499702453613,
      "learning_rate": 0.00011200044466677783,
      "loss": 2.2698,
      "step": 41900
    },
    {
      "epoch": 2.334500583625146,
      "grad_norm": 2.709542751312256,
      "learning_rate": 0.00011107405554629167,
      "loss": 2.1919,
      "step": 42000
    },
    {
      "epoch": 2.340058918348063,
      "grad_norm": 3.80522084236145,
      "learning_rate": 0.00011014766642580549,
      "loss": 2.339,
      "step": 42100
    },
    {
      "epoch": 2.34561725307098,
      "grad_norm": 3.140627861022949,
      "learning_rate": 0.00010922127730531934,
      "loss": 2.1763,
      "step": 42200
    },
    {
      "epoch": 2.351175587793897,
      "grad_norm": 3.2914273738861084,
      "learning_rate": 0.00010829488818483315,
      "loss": 2.2041,
      "step": 42300
    },
    {
      "epoch": 2.356733922516814,
      "grad_norm": 2.8517279624938965,
      "learning_rate": 0.000107368499064347,
      "loss": 2.2465,
      "step": 42400
    },
    {
      "epoch": 2.362292257239731,
      "grad_norm": 3.681190013885498,
      "learning_rate": 0.00010644210994386081,
      "loss": 2.265,
      "step": 42500
    },
    {
      "epoch": 2.367850591962648,
      "grad_norm": 4.011406898498535,
      "learning_rate": 0.0001055249847145795,
      "loss": 2.3307,
      "step": 42600
    },
    {
      "epoch": 2.373408926685565,
      "grad_norm": 3.151193857192993,
      "learning_rate": 0.00010459859559409335,
      "loss": 2.2432,
      "step": 42700
    },
    {
      "epoch": 2.378967261408482,
      "grad_norm": 2.839426279067993,
      "learning_rate": 0.00010367220647360717,
      "loss": 2.3199,
      "step": 42800
    },
    {
      "epoch": 2.384525596131399,
      "grad_norm": 3.2619686126708984,
      "learning_rate": 0.00010274581735312101,
      "loss": 2.3147,
      "step": 42900
    },
    {
      "epoch": 2.390083930854316,
      "grad_norm": 3.0076980590820312,
      "learning_rate": 0.00010181942823263483,
      "loss": 2.2439,
      "step": 43000
    },
    {
      "epoch": 2.395642265577233,
      "grad_norm": 4.189390182495117,
      "learning_rate": 0.00010089303911214867,
      "loss": 2.2492,
      "step": 43100
    },
    {
      "epoch": 2.4012006003001503,
      "grad_norm": 3.575686454772949,
      "learning_rate": 9.99666499916625e-05,
      "loss": 2.298,
      "step": 43200
    },
    {
      "epoch": 2.406758935023067,
      "grad_norm": 3.214797258377075,
      "learning_rate": 9.904026087117633e-05,
      "loss": 2.2614,
      "step": 43300
    },
    {
      "epoch": 2.412317269745984,
      "grad_norm": 4.197504043579102,
      "learning_rate": 9.811387175069016e-05,
      "loss": 2.2403,
      "step": 43400
    },
    {
      "epoch": 2.417875604468901,
      "grad_norm": 4.347015380859375,
      "learning_rate": 9.7187482630204e-05,
      "loss": 2.1949,
      "step": 43500
    },
    {
      "epoch": 2.4234339391918183,
      "grad_norm": 2.4733378887176514,
      "learning_rate": 9.626109350971782e-05,
      "loss": 2.2191,
      "step": 43600
    },
    {
      "epoch": 2.4289922739147354,
      "grad_norm": 3.272690773010254,
      "learning_rate": 9.533470438923165e-05,
      "loss": 2.2274,
      "step": 43700
    },
    {
      "epoch": 2.434550608637652,
      "grad_norm": 3.448394298553467,
      "learning_rate": 9.44083152687455e-05,
      "loss": 2.2725,
      "step": 43800
    },
    {
      "epoch": 2.440108943360569,
      "grad_norm": 2.868072271347046,
      "learning_rate": 9.348192614825932e-05,
      "loss": 2.1554,
      "step": 43900
    },
    {
      "epoch": 2.4456672780834863,
      "grad_norm": 4.181543827056885,
      "learning_rate": 9.255553702777316e-05,
      "loss": 2.3626,
      "step": 44000
    },
    {
      "epoch": 2.4512256128064034,
      "grad_norm": 2.848834753036499,
      "learning_rate": 9.162914790728698e-05,
      "loss": 2.277,
      "step": 44100
    },
    {
      "epoch": 2.45678394752932,
      "grad_norm": 2.983574390411377,
      "learning_rate": 9.070275878680082e-05,
      "loss": 2.2694,
      "step": 44200
    },
    {
      "epoch": 2.462342282252237,
      "grad_norm": 2.564924955368042,
      "learning_rate": 8.977636966631464e-05,
      "loss": 2.2683,
      "step": 44300
    },
    {
      "epoch": 2.4679006169751543,
      "grad_norm": 2.97113299369812,
      "learning_rate": 8.884998054582848e-05,
      "loss": 2.1879,
      "step": 44400
    },
    {
      "epoch": 2.4734589516980714,
      "grad_norm": 3.0412580966949463,
      "learning_rate": 8.79235914253423e-05,
      "loss": 2.266,
      "step": 44500
    },
    {
      "epoch": 2.479017286420988,
      "grad_norm": 4.054337501525879,
      "learning_rate": 8.699720230485614e-05,
      "loss": 2.2075,
      "step": 44600
    },
    {
      "epoch": 2.484575621143905,
      "grad_norm": 2.8735225200653076,
      "learning_rate": 8.607081318436996e-05,
      "loss": 2.2338,
      "step": 44700
    },
    {
      "epoch": 2.4901339558668223,
      "grad_norm": 3.407614231109619,
      "learning_rate": 8.51444240638838e-05,
      "loss": 2.2932,
      "step": 44800
    },
    {
      "epoch": 2.4956922905897394,
      "grad_norm": 3.3312530517578125,
      "learning_rate": 8.421803494339762e-05,
      "loss": 2.2773,
      "step": 44900
    },
    {
      "epoch": 2.5012506253126565,
      "grad_norm": 2.7641549110412598,
      "learning_rate": 8.329164582291147e-05,
      "loss": 2.2722,
      "step": 45000
    },
    {
      "epoch": 2.506808960035573,
      "grad_norm": 3.1989803314208984,
      "learning_rate": 8.237452059363016e-05,
      "loss": 2.2362,
      "step": 45100
    },
    {
      "epoch": 2.5123672947584903,
      "grad_norm": 3.235041618347168,
      "learning_rate": 8.144813147314397e-05,
      "loss": 2.2754,
      "step": 45200
    },
    {
      "epoch": 2.5179256294814074,
      "grad_norm": 3.4287538528442383,
      "learning_rate": 8.052174235265782e-05,
      "loss": 2.304,
      "step": 45300
    },
    {
      "epoch": 2.5234839642043245,
      "grad_norm": 2.8686439990997314,
      "learning_rate": 7.959535323217163e-05,
      "loss": 2.2242,
      "step": 45400
    },
    {
      "epoch": 2.5290422989272416,
      "grad_norm": 3.1530442237854004,
      "learning_rate": 7.866896411168548e-05,
      "loss": 2.2588,
      "step": 45500
    },
    {
      "epoch": 2.5346006336501583,
      "grad_norm": 2.7161126136779785,
      "learning_rate": 7.77425749911993e-05,
      "loss": 2.2372,
      "step": 45600
    },
    {
      "epoch": 2.5401589683730754,
      "grad_norm": 2.9079039096832275,
      "learning_rate": 7.681618587071314e-05,
      "loss": 2.1634,
      "step": 45700
    },
    {
      "epoch": 2.5457173030959925,
      "grad_norm": 3.856595039367676,
      "learning_rate": 7.588979675022696e-05,
      "loss": 2.2452,
      "step": 45800
    },
    {
      "epoch": 2.551275637818909,
      "grad_norm": 2.4552536010742188,
      "learning_rate": 7.49634076297408e-05,
      "loss": 2.291,
      "step": 45900
    },
    {
      "epoch": 2.5568339725418263,
      "grad_norm": 3.4273030757904053,
      "learning_rate": 7.403701850925462e-05,
      "loss": 2.2075,
      "step": 46000
    },
    {
      "epoch": 2.5623923072647434,
      "grad_norm": 3.806185722351074,
      "learning_rate": 7.311062938876846e-05,
      "loss": 2.2987,
      "step": 46100
    },
    {
      "epoch": 2.5679506419876605,
      "grad_norm": 4.3185954093933105,
      "learning_rate": 7.218424026828229e-05,
      "loss": 2.1803,
      "step": 46200
    },
    {
      "epoch": 2.5735089767105777,
      "grad_norm": 2.7306087017059326,
      "learning_rate": 7.125785114779612e-05,
      "loss": 2.3375,
      "step": 46300
    },
    {
      "epoch": 2.5790673114334943,
      "grad_norm": 3.4967215061187744,
      "learning_rate": 7.033146202730995e-05,
      "loss": 2.2908,
      "step": 46400
    },
    {
      "epoch": 2.5846256461564114,
      "grad_norm": 3.374149799346924,
      "learning_rate": 6.940507290682378e-05,
      "loss": 2.179,
      "step": 46500
    },
    {
      "epoch": 2.5901839808793286,
      "grad_norm": 2.839182138442993,
      "learning_rate": 6.847868378633761e-05,
      "loss": 2.2709,
      "step": 46600
    },
    {
      "epoch": 2.5957423156022457,
      "grad_norm": 3.0053772926330566,
      "learning_rate": 6.755229466585145e-05,
      "loss": 2.3064,
      "step": 46700
    },
    {
      "epoch": 2.6013006503251628,
      "grad_norm": 2.9468021392822266,
      "learning_rate": 6.662590554536528e-05,
      "loss": 2.1898,
      "step": 46800
    },
    {
      "epoch": 2.6068589850480794,
      "grad_norm": 4.154954433441162,
      "learning_rate": 6.56995164248791e-05,
      "loss": 2.2686,
      "step": 46900
    },
    {
      "epoch": 2.6124173197709966,
      "grad_norm": 2.9686362743377686,
      "learning_rate": 6.477312730439294e-05,
      "loss": 2.2398,
      "step": 47000
    },
    {
      "epoch": 2.6179756544939137,
      "grad_norm": 3.312373638153076,
      "learning_rate": 6.384673818390677e-05,
      "loss": 2.2963,
      "step": 47100
    },
    {
      "epoch": 2.623533989216831,
      "grad_norm": 2.60799503326416,
      "learning_rate": 6.29203490634206e-05,
      "loss": 2.2053,
      "step": 47200
    },
    {
      "epoch": 2.629092323939748,
      "grad_norm": 3.428436756134033,
      "learning_rate": 6.200322383413929e-05,
      "loss": 2.2731,
      "step": 47300
    },
    {
      "epoch": 2.6346506586626646,
      "grad_norm": 3.3928933143615723,
      "learning_rate": 6.107683471365313e-05,
      "loss": 2.2167,
      "step": 47400
    },
    {
      "epoch": 2.6402089933855817,
      "grad_norm": 2.8829140663146973,
      "learning_rate": 6.015044559316696e-05,
      "loss": 2.2049,
      "step": 47500
    },
    {
      "epoch": 2.645767328108499,
      "grad_norm": 2.768062114715576,
      "learning_rate": 5.922405647268079e-05,
      "loss": 2.231,
      "step": 47600
    },
    {
      "epoch": 2.6513256628314155,
      "grad_norm": 2.1637461185455322,
      "learning_rate": 5.829766735219462e-05,
      "loss": 2.2499,
      "step": 47700
    },
    {
      "epoch": 2.6568839975543326,
      "grad_norm": 2.9101107120513916,
      "learning_rate": 5.737127823170845e-05,
      "loss": 2.3346,
      "step": 47800
    },
    {
      "epoch": 2.6624423322772497,
      "grad_norm": 2.700615644454956,
      "learning_rate": 5.644488911122228e-05,
      "loss": 2.238,
      "step": 47900
    },
    {
      "epoch": 2.668000667000167,
      "grad_norm": 3.0933403968811035,
      "learning_rate": 5.551849999073611e-05,
      "loss": 2.2706,
      "step": 48000
    },
    {
      "epoch": 2.673559001723084,
      "grad_norm": 2.7927987575531006,
      "learning_rate": 5.459211087024994e-05,
      "loss": 2.1815,
      "step": 48100
    },
    {
      "epoch": 2.6791173364460006,
      "grad_norm": 3.473292112350464,
      "learning_rate": 5.366572174976377e-05,
      "loss": 2.1755,
      "step": 48200
    },
    {
      "epoch": 2.6846756711689177,
      "grad_norm": 3.7031805515289307,
      "learning_rate": 5.27393326292776e-05,
      "loss": 2.1801,
      "step": 48300
    },
    {
      "epoch": 2.690234005891835,
      "grad_norm": 3.607415199279785,
      "learning_rate": 5.181294350879143e-05,
      "loss": 2.2365,
      "step": 48400
    },
    {
      "epoch": 2.695792340614752,
      "grad_norm": 3.168142557144165,
      "learning_rate": 5.088655438830526e-05,
      "loss": 2.1977,
      "step": 48500
    },
    {
      "epoch": 2.701350675337669,
      "grad_norm": 3.6069507598876953,
      "learning_rate": 4.996016526781909e-05,
      "loss": 2.1628,
      "step": 48600
    },
    {
      "epoch": 2.7069090100605857,
      "grad_norm": 3.3101108074188232,
      "learning_rate": 4.9033776147332923e-05,
      "loss": 2.2705,
      "step": 48700
    },
    {
      "epoch": 2.712467344783503,
      "grad_norm": 4.290210247039795,
      "learning_rate": 4.8107387026846754e-05,
      "loss": 2.2388,
      "step": 48800
    },
    {
      "epoch": 2.71802567950642,
      "grad_norm": 3.123013973236084,
      "learning_rate": 4.7180997906360585e-05,
      "loss": 2.2497,
      "step": 48900
    },
    {
      "epoch": 2.723584014229337,
      "grad_norm": 2.6944730281829834,
      "learning_rate": 4.6254608785874415e-05,
      "loss": 2.2546,
      "step": 49000
    },
    {
      "epoch": 2.729142348952254,
      "grad_norm": 4.302056312561035,
      "learning_rate": 4.5328219665388246e-05,
      "loss": 2.2595,
      "step": 49100
    },
    {
      "epoch": 2.734700683675171,
      "grad_norm": 2.812668561935425,
      "learning_rate": 4.4401830544902076e-05,
      "loss": 2.1644,
      "step": 49200
    },
    {
      "epoch": 2.740259018398088,
      "grad_norm": 2.490410089492798,
      "learning_rate": 4.347544142441591e-05,
      "loss": 2.2131,
      "step": 49300
    },
    {
      "epoch": 2.745817353121005,
      "grad_norm": 3.7763335704803467,
      "learning_rate": 4.2558316195134605e-05,
      "loss": 2.2195,
      "step": 49400
    },
    {
      "epoch": 2.7513756878439217,
      "grad_norm": 3.7402563095092773,
      "learning_rate": 4.1631927074648435e-05,
      "loss": 2.1375,
      "step": 49500
    },
    {
      "epoch": 2.756934022566839,
      "grad_norm": 2.770596981048584,
      "learning_rate": 4.0705537954162266e-05,
      "loss": 2.2973,
      "step": 49600
    },
    {
      "epoch": 2.762492357289756,
      "grad_norm": 3.698094129562378,
      "learning_rate": 3.97791488336761e-05,
      "loss": 2.2331,
      "step": 49700
    },
    {
      "epoch": 2.768050692012673,
      "grad_norm": 4.080539226531982,
      "learning_rate": 3.8852759713189934e-05,
      "loss": 2.2855,
      "step": 49800
    },
    {
      "epoch": 2.77360902673559,
      "grad_norm": 3.2416398525238037,
      "learning_rate": 3.7926370592703764e-05,
      "loss": 2.1886,
      "step": 49900
    },
    {
      "epoch": 2.779167361458507,
      "grad_norm": 2.492769479751587,
      "learning_rate": 3.6999981472217595e-05,
      "loss": 2.2372,
      "step": 50000
    },
    {
      "epoch": 2.784725696181424,
      "grad_norm": 2.995269536972046,
      "learning_rate": 3.6073592351731425e-05,
      "loss": 2.2271,
      "step": 50100
    },
    {
      "epoch": 2.790284030904341,
      "grad_norm": 3.1993980407714844,
      "learning_rate": 3.5147203231245256e-05,
      "loss": 2.2123,
      "step": 50200
    },
    {
      "epoch": 2.795842365627258,
      "grad_norm": 3.3172316551208496,
      "learning_rate": 3.4220814110759086e-05,
      "loss": 2.2045,
      "step": 50300
    },
    {
      "epoch": 2.8014007003501753,
      "grad_norm": 2.5258326530456543,
      "learning_rate": 3.329442499027292e-05,
      "loss": 2.1813,
      "step": 50400
    },
    {
      "epoch": 2.806959035073092,
      "grad_norm": 3.014559745788574,
      "learning_rate": 3.236803586978675e-05,
      "loss": 2.2339,
      "step": 50500
    },
    {
      "epoch": 2.812517369796009,
      "grad_norm": 3.8663034439086914,
      "learning_rate": 3.144164674930058e-05,
      "loss": 2.2392,
      "step": 50600
    },
    {
      "epoch": 2.818075704518926,
      "grad_norm": 4.14202880859375,
      "learning_rate": 3.051525762881441e-05,
      "loss": 2.2271,
      "step": 50700
    },
    {
      "epoch": 2.8236340392418433,
      "grad_norm": 3.1088931560516357,
      "learning_rate": 2.958886850832824e-05,
      "loss": 2.2318,
      "step": 50800
    },
    {
      "epoch": 2.8291923739647604,
      "grad_norm": 3.5895814895629883,
      "learning_rate": 2.866247938784207e-05,
      "loss": 2.2365,
      "step": 50900
    },
    {
      "epoch": 2.834750708687677,
      "grad_norm": 2.816875696182251,
      "learning_rate": 2.77360902673559e-05,
      "loss": 2.1602,
      "step": 51000
    },
    {
      "epoch": 2.840309043410594,
      "grad_norm": 2.868539810180664,
      "learning_rate": 2.680970114686973e-05,
      "loss": 2.2368,
      "step": 51100
    },
    {
      "epoch": 2.8458673781335113,
      "grad_norm": 3.182009696960449,
      "learning_rate": 2.588331202638356e-05,
      "loss": 2.232,
      "step": 51200
    },
    {
      "epoch": 2.851425712856428,
      "grad_norm": 3.3765976428985596,
      "learning_rate": 2.4956922905897395e-05,
      "loss": 2.2077,
      "step": 51300
    },
    {
      "epoch": 2.856984047579345,
      "grad_norm": 3.769620418548584,
      "learning_rate": 2.4030533785411226e-05,
      "loss": 2.3149,
      "step": 51400
    },
    {
      "epoch": 2.862542382302262,
      "grad_norm": 2.6067328453063965,
      "learning_rate": 2.3104144664925056e-05,
      "loss": 2.2541,
      "step": 51500
    },
    {
      "epoch": 2.8681007170251793,
      "grad_norm": 4.284789085388184,
      "learning_rate": 2.2177755544438887e-05,
      "loss": 2.1798,
      "step": 51600
    },
    {
      "epoch": 2.8736590517480964,
      "grad_norm": 3.104787588119507,
      "learning_rate": 2.1251366423952717e-05,
      "loss": 2.2718,
      "step": 51700
    },
    {
      "epoch": 2.879217386471013,
      "grad_norm": 6.105128765106201,
      "learning_rate": 2.0334241194671412e-05,
      "loss": 2.2665,
      "step": 51800
    },
    {
      "epoch": 2.88477572119393,
      "grad_norm": 3.7280924320220947,
      "learning_rate": 1.9407852074185242e-05,
      "loss": 2.2125,
      "step": 51900
    },
    {
      "epoch": 2.8903340559168473,
      "grad_norm": 2.91523814201355,
      "learning_rate": 1.8481462953699073e-05,
      "loss": 2.2582,
      "step": 52000
    },
    {
      "epoch": 2.8958923906397644,
      "grad_norm": 2.363229990005493,
      "learning_rate": 1.7555073833212904e-05,
      "loss": 2.2631,
      "step": 52100
    },
    {
      "epoch": 2.9014507253626816,
      "grad_norm": 3.621305465698242,
      "learning_rate": 1.6628684712726734e-05,
      "loss": 2.2097,
      "step": 52200
    },
    {
      "epoch": 2.9070090600855982,
      "grad_norm": 2.2399120330810547,
      "learning_rate": 1.5702295592240565e-05,
      "loss": 2.2056,
      "step": 52300
    },
    {
      "epoch": 2.9125673948085153,
      "grad_norm": 3.902909755706787,
      "learning_rate": 1.4775906471754397e-05,
      "loss": 2.1598,
      "step": 52400
    },
    {
      "epoch": 2.9181257295314325,
      "grad_norm": 4.621135234832764,
      "learning_rate": 1.3849517351268227e-05,
      "loss": 2.184,
      "step": 52500
    },
    {
      "epoch": 2.923684064254349,
      "grad_norm": 3.3461356163024902,
      "learning_rate": 1.2923128230782058e-05,
      "loss": 2.1998,
      "step": 52600
    },
    {
      "epoch": 2.9292423989772667,
      "grad_norm": 2.9665536880493164,
      "learning_rate": 1.1996739110295889e-05,
      "loss": 2.2899,
      "step": 52700
    },
    {
      "epoch": 2.9348007337001834,
      "grad_norm": 3.5830295085906982,
      "learning_rate": 1.107034998980972e-05,
      "loss": 2.2464,
      "step": 52800
    },
    {
      "epoch": 2.9403590684231005,
      "grad_norm": 2.851125955581665,
      "learning_rate": 1.0143960869323551e-05,
      "loss": 2.2493,
      "step": 52900
    },
    {
      "epoch": 2.9459174031460176,
      "grad_norm": 3.524927854537964,
      "learning_rate": 9.217571748837382e-06,
      "loss": 2.1712,
      "step": 53000
    },
    {
      "epoch": 2.9514757378689342,
      "grad_norm": 3.267448902130127,
      "learning_rate": 8.291182628351212e-06,
      "loss": 2.1884,
      "step": 53100
    },
    {
      "epoch": 2.9570340725918514,
      "grad_norm": 3.4638803005218506,
      "learning_rate": 7.364793507865044e-06,
      "loss": 2.2529,
      "step": 53200
    },
    {
      "epoch": 2.9625924073147685,
      "grad_norm": 4.101757049560547,
      "learning_rate": 6.4384043873788744e-06,
      "loss": 2.1648,
      "step": 53300
    },
    {
      "epoch": 2.9681507420376856,
      "grad_norm": 3.416546106338501,
      "learning_rate": 5.512015266892706e-06,
      "loss": 2.2142,
      "step": 53400
    },
    {
      "epoch": 2.9737090767606027,
      "grad_norm": 2.8716702461242676,
      "learning_rate": 4.585626146406537e-06,
      "loss": 2.2338,
      "step": 53500
    },
    {
      "epoch": 2.9792674114835194,
      "grad_norm": 2.9655280113220215,
      "learning_rate": 3.6592370259203674e-06,
      "loss": 2.1493,
      "step": 53600
    },
    {
      "epoch": 2.9848257462064365,
      "grad_norm": 3.5789599418640137,
      "learning_rate": 2.7328479054341988e-06,
      "loss": 2.2783,
      "step": 53700
    },
    {
      "epoch": 2.9903840809293536,
      "grad_norm": 3.083874225616455,
      "learning_rate": 1.8064587849480295e-06,
      "loss": 2.2376,
      "step": 53800
    },
    {
      "epoch": 2.9959424156522707,
      "grad_norm": 3.1653454303741455,
      "learning_rate": 8.800696644618606e-07,
      "loss": 2.2301,
      "step": 53900
    }
  ],
  "logging_steps": 100,
  "max_steps": 53973,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.701663188450202e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
