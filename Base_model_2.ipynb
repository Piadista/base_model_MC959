{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ce98eb-b1af-4dd1-a3b1-f8a00cb7d762",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "A feedforward multilayer ANN for classification task of CVEs - in their vectorization format - into their corresponding CWE.\n",
    "\n",
    "To do:\n",
    "- Import the data\n",
    "- Train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92624d-7d68-414e-8539-1edfcbfdffd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98eff6-40c5-4e53-a257-cc52500fe944",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65d391-292a-4918-913c-29498b350bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pinecone\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c984-f846-4bc7-8ec3-c9cbfa831691",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1beb95a4-7d6a-4a19-9811-69bec3e994d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382622c-81fa-4b22-9167-71c779e1c15d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import data for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef01fd8-1593-4137-b7a6-8f62bb32ade8",
   "metadata": {},
   "source": [
    "Import data in the database, with the CVE in vector format and it's corresponding CWE number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c1b611e-7a36-40cf-9b1a-e240d3dee286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database with the CVEs in their vector format\n",
    "pc = Pinecone(api_key=\"pcsk_3fsLLS_KMLjRJ9jybj773qMy13aawRBFZveGrn2LrZhVP3GtMWet7PdZgNubyB8xRM6JQ7\")\n",
    "index = pc.Index(\"mc959\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99598cab-19c7-4aca-af2a-a7bf765c419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete\n",
    "def fetch_data_from_pinecone(index, num_samples):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    namespace = \"ns1\"\n",
    "    # Loop until we have collected the desired number of samples\n",
    "    while len(X_data) < num_samples:\n",
    "        # Generate a random query vector (or use a specific vector if needed)\n",
    "        query_vector = np.random.rand(input_size).tolist()\n",
    "        response = index.query(\n",
    "            namespace=namespace,\n",
    "            vector=query_vector,\n",
    "            top_k=5,  # Adjust as needed to fetch more matches per query\n",
    "            include_values=True,\n",
    "            include_metadata=True,\n",
    "            # Add any filters if necessary\n",
    "            # filter={\"genre\": {\"$eq\": \"action\"}}\n",
    "        )\n",
    "        if response and 'matches' in response:\n",
    "            for match in response['matches']:\n",
    "                vector = match.get('values', [])\n",
    "                metadata = match.get('metadata', {})\n",
    "                if vector and metadata:\n",
    "                    X_data.append(vector)\n",
    "                    y_data.append(metadata.get('label', 0))  # Assuming label is in 'label'\n",
    "                    # Stop if we've collected enough samples\n",
    "                    if len(X_data) >= num_samples:\n",
    "                        break\n",
    "    return np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0170f2e-2089-4948-a8b8-2775f9edd18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_vector = np.random.rand(768).tolist()\n",
    "\n",
    "# Query the index for the top 5 most similar vectors\n",
    "response = index.query(\n",
    "    vector=query_vector,   # Your query vector with 768 dimensions\n",
    "    top_k=5,               # Retrieve top 5 closest vectors\n",
    "    include_values=True,   # Include the vector values in the response\n",
    "    include_metadata=True, # Include any metadata associated with the vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7913bd4-df71-40e7-a3c7-2a54ef55ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector size encoder CVE\n",
    "V_CVE_size = 768\n",
    "\n",
    "# Number of possible CWEs to return\n",
    "N_CWE = 1365"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654be14-e73d-441a-adfb-a763a207eabe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ANN Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc965533-ac9b-48d8-a70c-f464585367f8",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937b7706-e6c5-474f-a625-51b6fe396ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = V_CVE_size          # Input vector size (V_CVE_size)\n",
    "num_classes = N_CWE              # Number of output classes (N_CWE)\n",
    "hidden_sizes = [256, 128, 64]    # Sizes of hidden layers\n",
    "activation_function = nn.ReLU    # Activation function to be used\n",
    "batch_size = 32                  # Batch size\n",
    "learning_rate = 1e-3             # Learning rate\n",
    "num_epochs = 100                  # Number of training epochs\n",
    "dropout_prob = 0.3              # Dropout probability for regularization (not used!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3989930-7d9e-464c-a0e0-74c7af5772e4",
   "metadata": {},
   "source": [
    "Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ed7f64-81f9-4975-beb6-719f574ee6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model dynamically\n",
    "layers = []\n",
    "\n",
    "# Input layer\n",
    "layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "layers.append(activation_function())\n",
    "\n",
    "# Hidden layers\n",
    "for i in range(len(hidden_sizes) - 1):\n",
    "    layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "    layers.append(activation_function())\n",
    "    layers.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "# Output layer\n",
    "layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "# Create the sequential model\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fce6e88-f95a-4c49-8497-8473e2df537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Dropout(p=0.3, inplace=False)\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.3, inplace=False)\n",
      "  (8): Linear(in_features=64, out_features=1365, bias=True)\n",
      "  (9): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Network Model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f79d2-2167-4572-9351-9e31b620b610",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5708d07c-087e-43ac-aec8-73134717028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function definiton\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106df99f-efad-4ae3-a773-f2fb7264a415",
   "metadata": {},
   "source": [
    "Optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f062d9f-3cc8-4e7c-bcee-9afe288971c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimization method\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97411a31-2ed8-4a40-b3df-70cbc6284324",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ANN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb56c8-eb09-4241-9d78-07b36b1925ba",
   "metadata": {},
   "source": [
    "Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454118c-975b-4988-87f6-0f2963ed281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "# Get the data\n",
    "X_array, y_array = fetch_data_from_pinecone(index, num_samples)\n",
    "\n",
    "# Ensure no missing values\n",
    "if np.isnan(X_array).any():\n",
    "    print(\"Data contains NaN values. Removing rows with NaN values...\")\n",
    "    # Create a mask for rows without NaNs\n",
    "    mask = ~np.isnan(X_array).any(axis=1)\n",
    "    \n",
    "    # Filter X_array and y_array using the mask\n",
    "    X_array = X_array[mask]\n",
    "    y_array = y_array[mask]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_array, dtype=torch.long)  # CrossEntropyLoss expects labels of type Long\n",
    "\n",
    "# Define split proportions\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "test_size = int(0.3 * len(full_dataset))\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df45a0-14ea-4397-a670-1d7d56579857",
   "metadata": {},
   "source": [
    "Store data for training and the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea262872-b158-443e-bf97-7c3833b8eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store loss and accuracy for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076cb18-19b8-44e9-b94c-ae64b2aa6ce0",
   "metadata": {},
   "source": [
    "Choose the GPU, if avaliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1d966b-771d-4379-9068-b2012b6d8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and use the GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61462bc0-b6dd-4234-8d31-ffabf419fb90",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274658b-1ee3-4473-ab6b-0ba3335bd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # # Move the batch to the GPU\n",
    "        # batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # # Validation phase (removed because of time issues)\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "    # val_correct = 0\n",
    "    # val_total = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for val_X, val_y in val_loader:\n",
    "    #         outputs = model(val_X)\n",
    "    #         loss = loss_fn(outputs, val_y)\n",
    "    #         val_loss += loss.item()\n",
    "    #         _, predicted = torch.max(outputs.data, 1)\n",
    "    #         val_total += val_y.size(0)\n",
    "    #         val_correct += (predicted == val_y).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.state_dict(), \"base_model.pth\")\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13cfb2-87bb-45a2-a9fb-88de8a2ab01f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ANN analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82957d2f-5eae-4c96-9782-18b215c06e99",
   "metadata": {},
   "source": [
    "Data from training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc24f80-e394-47e6-904e-e4879c5ed5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Loss and Accuracy\n",
    "\n",
    "# Plot Loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy over epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fbd64-baa8-4164-bc30-e8740ab4f8a9",
   "metadata": {},
   "source": [
    "Model final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06147990-64ec-4904-97e6-2004db367e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Data and Compute Performance Metrics\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for test_X, test_y in test_loader:\n",
    "        outputs = model(test_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(test_y.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260911a6-22b3-4cef-8f45-1c48702a9c7c",
   "metadata": {},
   "source": [
    "Save the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf8beeed-2768-40cf-9001-0baa530c2888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'base_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"base_model.pth\")\n",
    "print(\"Model saved as 'base_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
