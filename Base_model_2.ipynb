{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ce98eb-b1af-4dd1-a3b1-f8a00cb7d762",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "A feedforward multilayer ANN for classification task of CVEs - in their vectorization format - into their corresponding CWE.\n",
    "\n",
    "To do:\n",
    "- Transform CVE data into vector format\n",
    "- Train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92624d-7d68-414e-8539-1edfcbfdffd8",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98eff6-40c5-4e53-a257-cc52500fe944",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d65d391-292a-4918-913c-29498b350bfb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/guilherme/.local/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 KB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/guilherme/.local/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: requests in /home/guilherme/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/guilherme/.local/lib/python3.10/site-packages (from transformers) (1.25.1)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/guilherme/.local/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/guilherme/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/guilherme/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guilherme/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.2 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install Pinecone\n",
    "# !pip install torch\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c984-f846-4bc7-8ec3-c9cbfa831691",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1beb95a4-7d6a-4a19-9811-69bec3e994d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382622c-81fa-4b22-9167-71c779e1c15d",
   "metadata": {},
   "source": [
    "### Import data for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef01fd8-1593-4137-b7a6-8f62bb32ade8",
   "metadata": {},
   "source": [
    "Import data in the csv folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d62c69-cec0-4f51-99e7-99f6138a0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cwes = 1427\n",
    "\n",
    "### Train data\n",
    "# Step 1: Load the CSV file\n",
    "file_path_train = \"train.csv\"\n",
    "data_train = pd.read_csv(file_path_train)\n",
    "\n",
    "# Step 2: Separate X and Y\n",
    "X_train = [s[:-1] if s.endswith('.') else s for s in data_train.iloc[:, 1].values]  # Keep only the CVE description\n",
    "Y_train = data_train.iloc[:, 2:].values  # Latter two columns for targets\n",
    "\n",
    "# Extract numeric part from 'CWE-' using list comprehension and regex\n",
    "numeric_cwe_train = np.array([int(re.search(r'CWE-(\\d{1,4})', row[0]).group(1)) for row in Y_train])\n",
    "\n",
    "# Create one-hot encoded matrix\n",
    "one_hot_encoded_fixed_train = np.zeros((len(numeric_cwe_train), number_of_cwes))  # Initialize with zeros\n",
    "\n",
    "# Set the position corresponding to the CWE ID to 1\n",
    "for i, cwe_id in enumerate(numeric_cwe_train):\n",
    "    one_hot_encoded_fixed_train[i, cwe_id-1] = 1  # Set the position `cwe_id - 1` to 1\n",
    "\n",
    "### Test data\n",
    "file_path_test = \"test.csv\"\n",
    "data_test = pd.read_csv(file_path_test)\n",
    "\n",
    "# Step 2: Separate X and Y\n",
    "X_test = [s[:-1] if s.endswith('.') else s for s in data_test.iloc[:, 1].values] # Keep only the CVE description\n",
    "Y_test = data_test.iloc[:, 2:].values  # Latter two columns for targets\n",
    "\n",
    "# Extract numeric part from 'CWE-' using list comprehension and regex\n",
    "numeric_cwe_test = np.array([int(re.search(r'CWE-(\\d{1,4})', row[0]).group(1)) for row in Y_test])\n",
    "\n",
    "# Create one-hot encoded matrix\n",
    "one_hot_encoded_fixed_test = np.zeros((len(numeric_cwe_test), number_of_cwes))  # Initialize with zeros\n",
    "\n",
    "# Set the position corresponding to the CWE ID to 1\n",
    "for i, cwe_id in enumerate(numeric_cwe_test):\n",
    "    one_hot_encoded_fixed_test[i, cwe_id-1] = 1  # Set the position `cwe_id - 1` to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13855229-6c49-4bb3-bb1d-56ad28d9df02",
   "metadata": {},
   "source": [
    "Loading model for embedding the CVEs description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50f229d-cfe1-4e68-909a-841276b5a59c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in /home/guilherme/Documents/MC959/Projeto/models/model_1/fine_tuned_lora_mlm. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/guilherme/Documents/MC959/Projeto/models/model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fine_tuned_lora_mlm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m----> 7\u001b[0m model_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1049\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m   1047\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1053\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in /home/guilherme/Documents/MC959/Projeto/models/model_1/fine_tuned_lora_mlm. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth"
     ]
    }
   ],
   "source": [
    "# Set a version number for the model to manage multiple versions effectively\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = f\"/home/guilherme/Documents/MC959/Projeto/models/model_{MODEL_VERSION}/fine_tuned_lora_mlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_embedding = AutoModelForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934e08e-235b-4176-b90a-bae7cbf43f1a",
   "metadata": {},
   "source": [
    "Transform train and test data into their embedding format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7913bd4-df71-40e7-a3c7-2a54ef55ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "    embeddings = []\n",
    "    for input in inputs:\n",
    "        tokens =  tokenizer(input, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_embedding.base_model(**tokens)  # Camada base\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy() \n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for a list of CVE descriptions\n",
    "embeddings_train = encoder(X_train)\n",
    "embeddings_test = encoder(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654be14-e73d-441a-adfb-a763a207eabe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ANN Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc965533-ac9b-48d8-a70c-f464585367f8",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937b7706-e6c5-474f-a625-51b6fe396ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 768          # Input vector size (V_CVE_size)\n",
    "num_classes = number_of_cwes              # Number of output classes (N_CWE)\n",
    "hidden_sizes = [256, 128, 64]    # Sizes of hidden layers\n",
    "activation_function = nn.ReLU    # Activation function to be used\n",
    "batch_size = 32                  # Batch size\n",
    "learning_rate = 1e-3             # Learning rate\n",
    "num_epochs = 100                  # Number of training epochs\n",
    "dropout_prob = 0.5             # Dropout probability for regularization (not used!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3989930-7d9e-464c-a0e0-74c7af5772e4",
   "metadata": {},
   "source": [
    "Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ed7f64-81f9-4975-beb6-719f574ee6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model dynamically\n",
    "layers = []\n",
    "\n",
    "# Input layer\n",
    "layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "layers.append(activation_function())\n",
    "\n",
    "# Hidden layers\n",
    "for i in range(len(hidden_sizes) - 1):\n",
    "    layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "    layers.append(activation_function())\n",
    "    layers.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "# Output layer\n",
    "layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "# Create the sequential model\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fce6e88-f95a-4c49-8497-8473e2df537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Dropout(p=0.3, inplace=False)\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.3, inplace=False)\n",
      "  (8): Linear(in_features=64, out_features=1365, bias=True)\n",
      "  (9): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Network Model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f79d2-2167-4572-9351-9e31b620b610",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5708d07c-087e-43ac-aec8-73134717028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function definiton\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106df99f-efad-4ae3-a773-f2fb7264a415",
   "metadata": {},
   "source": [
    "Optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f062d9f-3cc8-4e7c-bcee-9afe288971c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimization method\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97411a31-2ed8-4a40-b3df-70cbc6284324",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ANN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb56c8-eb09-4241-9d78-07b36b1925ba",
   "metadata": {},
   "source": [
    "Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454118c-975b-4988-87f6-0f2963ed281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "X_train = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(one_hot_encoded_fixed_train, dtype=torch.long)  # CrossEntropyLoss expects labels of type Long\n",
    "\n",
    "# Create a TensorDataset from X_train and y_train\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "X_test = torch.tensor(embeddings_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(one_hot_encoded_fixed_test, dtype=torch.long)  # CrossEntropyLoss expects labels of type Long\n",
    "\n",
    "# Create a TensorDataset from X_train and y_train\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# # Ensure no missing values\n",
    "# if np.isnan(X_array).any():\n",
    "#     print(\"Data contains NaN values. Removing rows with NaN values...\")\n",
    "#     # Create a mask for rows without NaNs\n",
    "#     mask = ~np.isnan(X_array).any(axis=1)\n",
    "    \n",
    "#     # Filter X_array and y_array using the mask\n",
    "#     X_array = X_array[mask]\n",
    "#     y_array = y_array[mask]\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_array, dtype=torch.long)  # CrossEntropyLoss expects labels of type Long\n",
    "\n",
    "# # Define split proportions\n",
    "# train_size = int(0.7 * len(full_dataset))\n",
    "# test_size = int(0.3 * len(full_dataset))\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df45a0-14ea-4397-a670-1d7d56579857",
   "metadata": {},
   "source": [
    "Store data for training and the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea262872-b158-443e-bf97-7c3833b8eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store loss and accuracy for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076cb18-19b8-44e9-b94c-ae64b2aa6ce0",
   "metadata": {},
   "source": [
    "Choose the GPU, if avaliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1d966b-771d-4379-9068-b2012b6d8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and use the GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61462bc0-b6dd-4234-8d31-ffabf419fb90",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274658b-1ee3-4473-ab6b-0ba3335bd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # # Move the batch to the GPU\n",
    "        # batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # # Validation phase (removed because of time issues)\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "    # val_correct = 0\n",
    "    # val_total = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for val_X, val_y in val_loader:\n",
    "    #         outputs = model(val_X)\n",
    "    #         loss = loss_fn(outputs, val_y)\n",
    "    #         val_loss += loss.item()\n",
    "    #         _, predicted = torch.max(outputs.data, 1)\n",
    "    #         val_total += val_y.size(0)\n",
    "    #         val_correct += (predicted == val_y).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.state_dict(), \"base_model.pth\")\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13cfb2-87bb-45a2-a9fb-88de8a2ab01f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ANN analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82957d2f-5eae-4c96-9782-18b215c06e99",
   "metadata": {},
   "source": [
    "Data from training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc24f80-e394-47e6-904e-e4879c5ed5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Loss and Accuracy\n",
    "\n",
    "# Plot Loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy over epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fbd64-baa8-4164-bc30-e8740ab4f8a9",
   "metadata": {},
   "source": [
    "Model final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06147990-64ec-4904-97e6-2004db367e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Data and Compute Performance Metrics\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for test_X, test_y in test_loader:\n",
    "        outputs = model(test_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(test_y.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260911a6-22b3-4cef-8f45-1c48702a9c7c",
   "metadata": {},
   "source": [
    "Save the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf8beeed-2768-40cf-9001-0baa530c2888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'base_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"base_model.pth\")\n",
    "print(\"Model saved as 'base_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
