{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\leo72\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_base_dir = \"./raw_data\"\n",
    "\n",
    "# Load dataset for self-supervised learning \n",
    "cve_dataset = pd.read_csv(f\"{raw_data_base_dir}/cve_dataset.csv\")\n",
    "cwe_dataset = pd.read_csv(f\"{raw_data_base_dir}/cwe_dataset.csv\")\n",
    "\n",
    "# Exlude NA lines and drop cwe column from cve dataset\n",
    "cve_dataset.dropna(subset=['description'], inplace=True)\n",
    "cve_dataset.drop(columns=['cwe'], inplace=True)\n",
    "\n",
    "# Exlude NA lines from cwe dataset\n",
    "cwe_dataset.dropna(subset=['title','description'], inplace=True)\n",
    "cwe_dataset['metadescription'] = cwe_dataset['cwe_id'] + \" - \" + cwe_dataset['title'] + \" : \" + cwe_dataset['description']\n",
    "\n",
    "# Load dataset for supervised learning\n",
    "cwe2cve_dataset = pd.read_csv(f\"{raw_data_base_dir}/cwe2cve_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"./datasets\"\n",
    "\n",
    "# Split CVE dataset into train and test\n",
    "cve_train_data, cve_test_data = train_test_split(\n",
    "    cve_dataset['description'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split CWE dataset into train and test\n",
    "cwe_train_data, cwe_test_data = train_test_split(\n",
    "    cwe_dataset['metadescription'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine CVE and CWE training datasets into a single train dataset\n",
    "train_dataset = pd.concat([\n",
    "    pd.DataFrame({'text': cve_train_data}),\n",
    "    pd.DataFrame({'text': cwe_train_data})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Combine CVE and CWE test datasets into a single test dataset\n",
    "test_dataset = pd.concat([\n",
    "    pd.DataFrame({'text': cve_test_data}),\n",
    "    pd.DataFrame({'text': cwe_test_data})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Remove rows with missing values (NA) from train and test datasets\n",
    "train_dataset = train_dataset.dropna()\n",
    "test_dataset = test_dataset.dropna()\n",
    "\n",
    "# Shuffle both datasets to ensure randomness\n",
    "train_dataset = train_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the train and test datasets as .csv files\n",
    "train_dataset.to_csv(f\"./{dataset_base_dir}/train.csv\", index=False)\n",
    "test_dataset.to_csv(f\"./{dataset_base_dir}/test.csv\", index=False)\n",
    "\n",
    "# Save only the 'text' column as .txt files for model consumption\n",
    "train_dataset['text'].to_csv(f\"./{dataset_base_dir}/train.txt\", index=False, header=False)\n",
    "test_dataset['text'].to_csv(f\"./{dataset_base_dir}/test.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'roberta', 'roberta.embeddings', 'roberta.embeddings.word_embeddings', 'roberta.embeddings.position_embeddings', 'roberta.embeddings.token_type_embeddings', 'roberta.embeddings.LayerNorm', 'roberta.embeddings.dropout', 'roberta.encoder', 'roberta.encoder.layer', 'roberta.encoder.layer.0', 'roberta.encoder.layer.0.attention', 'roberta.encoder.layer.0.attention.self', 'roberta.encoder.layer.0.attention.self.query', 'roberta.encoder.layer.0.attention.self.key', 'roberta.encoder.layer.0.attention.self.value', 'roberta.encoder.layer.0.attention.self.dropout', 'roberta.encoder.layer.0.attention.output', 'roberta.encoder.layer.0.attention.output.dense', 'roberta.encoder.layer.0.attention.output.LayerNorm', 'roberta.encoder.layer.0.attention.output.dropout', 'roberta.encoder.layer.0.intermediate', 'roberta.encoder.layer.0.intermediate.dense', 'roberta.encoder.layer.0.intermediate.intermediate_act_fn', 'roberta.encoder.layer.0.output', 'roberta.encoder.layer.0.output.dense', 'roberta.encoder.layer.0.output.LayerNorm', 'roberta.encoder.layer.0.output.dropout', 'roberta.encoder.layer.1', 'roberta.encoder.layer.1.attention', 'roberta.encoder.layer.1.attention.self', 'roberta.encoder.layer.1.attention.self.query', 'roberta.encoder.layer.1.attention.self.key', 'roberta.encoder.layer.1.attention.self.value', 'roberta.encoder.layer.1.attention.self.dropout', 'roberta.encoder.layer.1.attention.output', 'roberta.encoder.layer.1.attention.output.dense', 'roberta.encoder.layer.1.attention.output.LayerNorm', 'roberta.encoder.layer.1.attention.output.dropout', 'roberta.encoder.layer.1.intermediate', 'roberta.encoder.layer.1.intermediate.dense', 'roberta.encoder.layer.1.intermediate.intermediate_act_fn', 'roberta.encoder.layer.1.output', 'roberta.encoder.layer.1.output.dense', 'roberta.encoder.layer.1.output.LayerNorm', 'roberta.encoder.layer.1.output.dropout', 'roberta.encoder.layer.2', 'roberta.encoder.layer.2.attention', 'roberta.encoder.layer.2.attention.self', 'roberta.encoder.layer.2.attention.self.query', 'roberta.encoder.layer.2.attention.self.key', 'roberta.encoder.layer.2.attention.self.value', 'roberta.encoder.layer.2.attention.self.dropout', 'roberta.encoder.layer.2.attention.output', 'roberta.encoder.layer.2.attention.output.dense', 'roberta.encoder.layer.2.attention.output.LayerNorm', 'roberta.encoder.layer.2.attention.output.dropout', 'roberta.encoder.layer.2.intermediate', 'roberta.encoder.layer.2.intermediate.dense', 'roberta.encoder.layer.2.intermediate.intermediate_act_fn', 'roberta.encoder.layer.2.output', 'roberta.encoder.layer.2.output.dense', 'roberta.encoder.layer.2.output.LayerNorm', 'roberta.encoder.layer.2.output.dropout', 'roberta.encoder.layer.3', 'roberta.encoder.layer.3.attention', 'roberta.encoder.layer.3.attention.self', 'roberta.encoder.layer.3.attention.self.query', 'roberta.encoder.layer.3.attention.self.key', 'roberta.encoder.layer.3.attention.self.value', 'roberta.encoder.layer.3.attention.self.dropout', 'roberta.encoder.layer.3.attention.output', 'roberta.encoder.layer.3.attention.output.dense', 'roberta.encoder.layer.3.attention.output.LayerNorm', 'roberta.encoder.layer.3.attention.output.dropout', 'roberta.encoder.layer.3.intermediate', 'roberta.encoder.layer.3.intermediate.dense', 'roberta.encoder.layer.3.intermediate.intermediate_act_fn', 'roberta.encoder.layer.3.output', 'roberta.encoder.layer.3.output.dense', 'roberta.encoder.layer.3.output.LayerNorm', 'roberta.encoder.layer.3.output.dropout', 'roberta.encoder.layer.4', 'roberta.encoder.layer.4.attention', 'roberta.encoder.layer.4.attention.self', 'roberta.encoder.layer.4.attention.self.query', 'roberta.encoder.layer.4.attention.self.key', 'roberta.encoder.layer.4.attention.self.value', 'roberta.encoder.layer.4.attention.self.dropout', 'roberta.encoder.layer.4.attention.output', 'roberta.encoder.layer.4.attention.output.dense', 'roberta.encoder.layer.4.attention.output.LayerNorm', 'roberta.encoder.layer.4.attention.output.dropout', 'roberta.encoder.layer.4.intermediate', 'roberta.encoder.layer.4.intermediate.dense', 'roberta.encoder.layer.4.intermediate.intermediate_act_fn', 'roberta.encoder.layer.4.output', 'roberta.encoder.layer.4.output.dense', 'roberta.encoder.layer.4.output.LayerNorm', 'roberta.encoder.layer.4.output.dropout', 'roberta.encoder.layer.5', 'roberta.encoder.layer.5.attention', 'roberta.encoder.layer.5.attention.self', 'roberta.encoder.layer.5.attention.self.query', 'roberta.encoder.layer.5.attention.self.key', 'roberta.encoder.layer.5.attention.self.value', 'roberta.encoder.layer.5.attention.self.dropout', 'roberta.encoder.layer.5.attention.output', 'roberta.encoder.layer.5.attention.output.dense', 'roberta.encoder.layer.5.attention.output.LayerNorm', 'roberta.encoder.layer.5.attention.output.dropout', 'roberta.encoder.layer.5.intermediate', 'roberta.encoder.layer.5.intermediate.dense', 'roberta.encoder.layer.5.intermediate.intermediate_act_fn', 'roberta.encoder.layer.5.output', 'roberta.encoder.layer.5.output.dense', 'roberta.encoder.layer.5.output.LayerNorm', 'roberta.encoder.layer.5.output.dropout', 'roberta.encoder.layer.6', 'roberta.encoder.layer.6.attention', 'roberta.encoder.layer.6.attention.self', 'roberta.encoder.layer.6.attention.self.query', 'roberta.encoder.layer.6.attention.self.key', 'roberta.encoder.layer.6.attention.self.value', 'roberta.encoder.layer.6.attention.self.dropout', 'roberta.encoder.layer.6.attention.output', 'roberta.encoder.layer.6.attention.output.dense', 'roberta.encoder.layer.6.attention.output.LayerNorm', 'roberta.encoder.layer.6.attention.output.dropout', 'roberta.encoder.layer.6.intermediate', 'roberta.encoder.layer.6.intermediate.dense', 'roberta.encoder.layer.6.intermediate.intermediate_act_fn', 'roberta.encoder.layer.6.output', 'roberta.encoder.layer.6.output.dense', 'roberta.encoder.layer.6.output.LayerNorm', 'roberta.encoder.layer.6.output.dropout', 'roberta.encoder.layer.7', 'roberta.encoder.layer.7.attention', 'roberta.encoder.layer.7.attention.self', 'roberta.encoder.layer.7.attention.self.query', 'roberta.encoder.layer.7.attention.self.key', 'roberta.encoder.layer.7.attention.self.value', 'roberta.encoder.layer.7.attention.self.dropout', 'roberta.encoder.layer.7.attention.output', 'roberta.encoder.layer.7.attention.output.dense', 'roberta.encoder.layer.7.attention.output.LayerNorm', 'roberta.encoder.layer.7.attention.output.dropout', 'roberta.encoder.layer.7.intermediate', 'roberta.encoder.layer.7.intermediate.dense', 'roberta.encoder.layer.7.intermediate.intermediate_act_fn', 'roberta.encoder.layer.7.output', 'roberta.encoder.layer.7.output.dense', 'roberta.encoder.layer.7.output.LayerNorm', 'roberta.encoder.layer.7.output.dropout', 'roberta.encoder.layer.8', 'roberta.encoder.layer.8.attention', 'roberta.encoder.layer.8.attention.self', 'roberta.encoder.layer.8.attention.self.query', 'roberta.encoder.layer.8.attention.self.key', 'roberta.encoder.layer.8.attention.self.value', 'roberta.encoder.layer.8.attention.self.dropout', 'roberta.encoder.layer.8.attention.output', 'roberta.encoder.layer.8.attention.output.dense', 'roberta.encoder.layer.8.attention.output.LayerNorm', 'roberta.encoder.layer.8.attention.output.dropout', 'roberta.encoder.layer.8.intermediate', 'roberta.encoder.layer.8.intermediate.dense', 'roberta.encoder.layer.8.intermediate.intermediate_act_fn', 'roberta.encoder.layer.8.output', 'roberta.encoder.layer.8.output.dense', 'roberta.encoder.layer.8.output.LayerNorm', 'roberta.encoder.layer.8.output.dropout', 'roberta.encoder.layer.9', 'roberta.encoder.layer.9.attention', 'roberta.encoder.layer.9.attention.self', 'roberta.encoder.layer.9.attention.self.query', 'roberta.encoder.layer.9.attention.self.key', 'roberta.encoder.layer.9.attention.self.value', 'roberta.encoder.layer.9.attention.self.dropout', 'roberta.encoder.layer.9.attention.output', 'roberta.encoder.layer.9.attention.output.dense', 'roberta.encoder.layer.9.attention.output.LayerNorm', 'roberta.encoder.layer.9.attention.output.dropout', 'roberta.encoder.layer.9.intermediate', 'roberta.encoder.layer.9.intermediate.dense', 'roberta.encoder.layer.9.intermediate.intermediate_act_fn', 'roberta.encoder.layer.9.output', 'roberta.encoder.layer.9.output.dense', 'roberta.encoder.layer.9.output.LayerNorm', 'roberta.encoder.layer.9.output.dropout', 'roberta.encoder.layer.10', 'roberta.encoder.layer.10.attention', 'roberta.encoder.layer.10.attention.self', 'roberta.encoder.layer.10.attention.self.query', 'roberta.encoder.layer.10.attention.self.key', 'roberta.encoder.layer.10.attention.self.value', 'roberta.encoder.layer.10.attention.self.dropout', 'roberta.encoder.layer.10.attention.output', 'roberta.encoder.layer.10.attention.output.dense', 'roberta.encoder.layer.10.attention.output.LayerNorm', 'roberta.encoder.layer.10.attention.output.dropout', 'roberta.encoder.layer.10.intermediate', 'roberta.encoder.layer.10.intermediate.dense', 'roberta.encoder.layer.10.intermediate.intermediate_act_fn', 'roberta.encoder.layer.10.output', 'roberta.encoder.layer.10.output.dense', 'roberta.encoder.layer.10.output.LayerNorm', 'roberta.encoder.layer.10.output.dropout', 'roberta.encoder.layer.11', 'roberta.encoder.layer.11.attention', 'roberta.encoder.layer.11.attention.self', 'roberta.encoder.layer.11.attention.self.query', 'roberta.encoder.layer.11.attention.self.key', 'roberta.encoder.layer.11.attention.self.value', 'roberta.encoder.layer.11.attention.self.dropout', 'roberta.encoder.layer.11.attention.output', 'roberta.encoder.layer.11.attention.output.dense', 'roberta.encoder.layer.11.attention.output.LayerNorm', 'roberta.encoder.layer.11.attention.output.dropout', 'roberta.encoder.layer.11.intermediate', 'roberta.encoder.layer.11.intermediate.dense', 'roberta.encoder.layer.11.intermediate.intermediate_act_fn', 'roberta.encoder.layer.11.output', 'roberta.encoder.layer.11.output.dense', 'roberta.encoder.layer.11.output.LayerNorm', 'roberta.encoder.layer.11.output.dropout', 'lm_head', 'lm_head.dense', 'lm_head.layer_norm', 'lm_head.decoder']\n"
     ]
    }
   ],
   "source": [
    "# Load the RoBERTa tokenizer and base model for Masked Language Modeling (MLM)\n",
    "model_name = \"roberta-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Tokenizer for tokenizing input text\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(model_name)  # RoBERTa model for MLM\n",
    "\n",
    "print([name for name, _ in base_model.named_modules()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LoRa Fine-Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'roberta', 'roberta.embeddings', 'roberta.embeddings.word_embeddings', 'roberta.embeddings.position_embeddings', 'roberta.embeddings.token_type_embeddings', 'roberta.embeddings.LayerNorm', 'roberta.embeddings.dropout', 'roberta.encoder', 'roberta.encoder.layer', 'roberta.encoder.layer.0', 'roberta.encoder.layer.0.attention', 'roberta.encoder.layer.0.attention.self', 'roberta.encoder.layer.0.attention.self.query', 'roberta.encoder.layer.0.attention.self.key', 'roberta.encoder.layer.0.attention.self.value', 'roberta.encoder.layer.0.attention.self.dropout', 'roberta.encoder.layer.0.attention.output', 'roberta.encoder.layer.0.attention.output.dense', 'roberta.encoder.layer.0.attention.output.LayerNorm', 'roberta.encoder.layer.0.attention.output.dropout', 'roberta.encoder.layer.0.intermediate', 'roberta.encoder.layer.0.intermediate.dense', 'roberta.encoder.layer.0.intermediate.intermediate_act_fn', 'roberta.encoder.layer.0.output', 'roberta.encoder.layer.0.output.dense', 'roberta.encoder.layer.0.output.LayerNorm', 'roberta.encoder.layer.0.output.dropout', 'roberta.encoder.layer.1', 'roberta.encoder.layer.1.attention', 'roberta.encoder.layer.1.attention.self', 'roberta.encoder.layer.1.attention.self.query', 'roberta.encoder.layer.1.attention.self.key', 'roberta.encoder.layer.1.attention.self.value', 'roberta.encoder.layer.1.attention.self.dropout', 'roberta.encoder.layer.1.attention.output', 'roberta.encoder.layer.1.attention.output.dense', 'roberta.encoder.layer.1.attention.output.LayerNorm', 'roberta.encoder.layer.1.attention.output.dropout', 'roberta.encoder.layer.1.intermediate', 'roberta.encoder.layer.1.intermediate.dense', 'roberta.encoder.layer.1.intermediate.intermediate_act_fn', 'roberta.encoder.layer.1.output', 'roberta.encoder.layer.1.output.dense', 'roberta.encoder.layer.1.output.LayerNorm', 'roberta.encoder.layer.1.output.dropout', 'roberta.encoder.layer.2', 'roberta.encoder.layer.2.attention', 'roberta.encoder.layer.2.attention.self', 'roberta.encoder.layer.2.attention.self.query', 'roberta.encoder.layer.2.attention.self.key', 'roberta.encoder.layer.2.attention.self.value', 'roberta.encoder.layer.2.attention.self.dropout', 'roberta.encoder.layer.2.attention.output', 'roberta.encoder.layer.2.attention.output.dense', 'roberta.encoder.layer.2.attention.output.LayerNorm', 'roberta.encoder.layer.2.attention.output.dropout', 'roberta.encoder.layer.2.intermediate', 'roberta.encoder.layer.2.intermediate.dense', 'roberta.encoder.layer.2.intermediate.intermediate_act_fn', 'roberta.encoder.layer.2.output', 'roberta.encoder.layer.2.output.dense', 'roberta.encoder.layer.2.output.LayerNorm', 'roberta.encoder.layer.2.output.dropout', 'roberta.encoder.layer.3', 'roberta.encoder.layer.3.attention', 'roberta.encoder.layer.3.attention.self', 'roberta.encoder.layer.3.attention.self.query', 'roberta.encoder.layer.3.attention.self.key', 'roberta.encoder.layer.3.attention.self.value', 'roberta.encoder.layer.3.attention.self.dropout', 'roberta.encoder.layer.3.attention.output', 'roberta.encoder.layer.3.attention.output.dense', 'roberta.encoder.layer.3.attention.output.LayerNorm', 'roberta.encoder.layer.3.attention.output.dropout', 'roberta.encoder.layer.3.intermediate', 'roberta.encoder.layer.3.intermediate.dense', 'roberta.encoder.layer.3.intermediate.intermediate_act_fn', 'roberta.encoder.layer.3.output', 'roberta.encoder.layer.3.output.dense', 'roberta.encoder.layer.3.output.LayerNorm', 'roberta.encoder.layer.3.output.dropout', 'roberta.encoder.layer.4', 'roberta.encoder.layer.4.attention', 'roberta.encoder.layer.4.attention.self', 'roberta.encoder.layer.4.attention.self.query', 'roberta.encoder.layer.4.attention.self.key', 'roberta.encoder.layer.4.attention.self.value', 'roberta.encoder.layer.4.attention.self.dropout', 'roberta.encoder.layer.4.attention.output', 'roberta.encoder.layer.4.attention.output.dense', 'roberta.encoder.layer.4.attention.output.LayerNorm', 'roberta.encoder.layer.4.attention.output.dropout', 'roberta.encoder.layer.4.intermediate', 'roberta.encoder.layer.4.intermediate.dense', 'roberta.encoder.layer.4.intermediate.intermediate_act_fn', 'roberta.encoder.layer.4.output', 'roberta.encoder.layer.4.output.dense', 'roberta.encoder.layer.4.output.LayerNorm', 'roberta.encoder.layer.4.output.dropout', 'roberta.encoder.layer.5', 'roberta.encoder.layer.5.attention', 'roberta.encoder.layer.5.attention.self', 'roberta.encoder.layer.5.attention.self.query', 'roberta.encoder.layer.5.attention.self.key', 'roberta.encoder.layer.5.attention.self.value', 'roberta.encoder.layer.5.attention.self.dropout', 'roberta.encoder.layer.5.attention.output', 'roberta.encoder.layer.5.attention.output.dense', 'roberta.encoder.layer.5.attention.output.LayerNorm', 'roberta.encoder.layer.5.attention.output.dropout', 'roberta.encoder.layer.5.intermediate', 'roberta.encoder.layer.5.intermediate.dense', 'roberta.encoder.layer.5.intermediate.intermediate_act_fn', 'roberta.encoder.layer.5.output', 'roberta.encoder.layer.5.output.dense', 'roberta.encoder.layer.5.output.LayerNorm', 'roberta.encoder.layer.5.output.dropout', 'roberta.encoder.layer.6', 'roberta.encoder.layer.6.attention', 'roberta.encoder.layer.6.attention.self', 'roberta.encoder.layer.6.attention.self.query', 'roberta.encoder.layer.6.attention.self.key', 'roberta.encoder.layer.6.attention.self.value', 'roberta.encoder.layer.6.attention.self.dropout', 'roberta.encoder.layer.6.attention.output', 'roberta.encoder.layer.6.attention.output.dense', 'roberta.encoder.layer.6.attention.output.LayerNorm', 'roberta.encoder.layer.6.attention.output.dropout', 'roberta.encoder.layer.6.intermediate', 'roberta.encoder.layer.6.intermediate.dense', 'roberta.encoder.layer.6.intermediate.intermediate_act_fn', 'roberta.encoder.layer.6.output', 'roberta.encoder.layer.6.output.dense', 'roberta.encoder.layer.6.output.LayerNorm', 'roberta.encoder.layer.6.output.dropout', 'roberta.encoder.layer.7', 'roberta.encoder.layer.7.attention', 'roberta.encoder.layer.7.attention.self', 'roberta.encoder.layer.7.attention.self.query', 'roberta.encoder.layer.7.attention.self.key', 'roberta.encoder.layer.7.attention.self.value', 'roberta.encoder.layer.7.attention.self.dropout', 'roberta.encoder.layer.7.attention.output', 'roberta.encoder.layer.7.attention.output.dense', 'roberta.encoder.layer.7.attention.output.LayerNorm', 'roberta.encoder.layer.7.attention.output.dropout', 'roberta.encoder.layer.7.intermediate', 'roberta.encoder.layer.7.intermediate.dense', 'roberta.encoder.layer.7.intermediate.intermediate_act_fn', 'roberta.encoder.layer.7.output', 'roberta.encoder.layer.7.output.dense', 'roberta.encoder.layer.7.output.LayerNorm', 'roberta.encoder.layer.7.output.dropout', 'roberta.encoder.layer.8', 'roberta.encoder.layer.8.attention', 'roberta.encoder.layer.8.attention.self', 'roberta.encoder.layer.8.attention.self.query', 'roberta.encoder.layer.8.attention.self.key', 'roberta.encoder.layer.8.attention.self.value', 'roberta.encoder.layer.8.attention.self.dropout', 'roberta.encoder.layer.8.attention.output', 'roberta.encoder.layer.8.attention.output.dense', 'roberta.encoder.layer.8.attention.output.LayerNorm', 'roberta.encoder.layer.8.attention.output.dropout', 'roberta.encoder.layer.8.intermediate', 'roberta.encoder.layer.8.intermediate.dense', 'roberta.encoder.layer.8.intermediate.intermediate_act_fn', 'roberta.encoder.layer.8.output', 'roberta.encoder.layer.8.output.dense', 'roberta.encoder.layer.8.output.LayerNorm', 'roberta.encoder.layer.8.output.dropout', 'roberta.encoder.layer.9', 'roberta.encoder.layer.9.attention', 'roberta.encoder.layer.9.attention.self', 'roberta.encoder.layer.9.attention.self.query', 'roberta.encoder.layer.9.attention.self.key', 'roberta.encoder.layer.9.attention.self.value', 'roberta.encoder.layer.9.attention.self.dropout', 'roberta.encoder.layer.9.attention.output', 'roberta.encoder.layer.9.attention.output.dense', 'roberta.encoder.layer.9.attention.output.LayerNorm', 'roberta.encoder.layer.9.attention.output.dropout', 'roberta.encoder.layer.9.intermediate', 'roberta.encoder.layer.9.intermediate.dense', 'roberta.encoder.layer.9.intermediate.intermediate_act_fn', 'roberta.encoder.layer.9.output', 'roberta.encoder.layer.9.output.dense', 'roberta.encoder.layer.9.output.LayerNorm', 'roberta.encoder.layer.9.output.dropout', 'roberta.encoder.layer.10', 'roberta.encoder.layer.10.attention', 'roberta.encoder.layer.10.attention.self', 'roberta.encoder.layer.10.attention.self.query', 'roberta.encoder.layer.10.attention.self.key', 'roberta.encoder.layer.10.attention.self.value', 'roberta.encoder.layer.10.attention.self.dropout', 'roberta.encoder.layer.10.attention.output', 'roberta.encoder.layer.10.attention.output.dense', 'roberta.encoder.layer.10.attention.output.LayerNorm', 'roberta.encoder.layer.10.attention.output.dropout', 'roberta.encoder.layer.10.intermediate', 'roberta.encoder.layer.10.intermediate.dense', 'roberta.encoder.layer.10.intermediate.intermediate_act_fn', 'roberta.encoder.layer.10.output', 'roberta.encoder.layer.10.output.dense', 'roberta.encoder.layer.10.output.LayerNorm', 'roberta.encoder.layer.10.output.dropout', 'roberta.encoder.layer.11', 'roberta.encoder.layer.11.attention', 'roberta.encoder.layer.11.attention.self', 'roberta.encoder.layer.11.attention.self.query', 'roberta.encoder.layer.11.attention.self.key', 'roberta.encoder.layer.11.attention.self.value', 'roberta.encoder.layer.11.attention.self.dropout', 'roberta.encoder.layer.11.attention.output', 'roberta.encoder.layer.11.attention.output.dense', 'roberta.encoder.layer.11.attention.output.LayerNorm', 'roberta.encoder.layer.11.attention.output.dropout', 'roberta.encoder.layer.11.intermediate', 'roberta.encoder.layer.11.intermediate.dense', 'roberta.encoder.layer.11.intermediate.intermediate_act_fn', 'roberta.encoder.layer.11.output', 'roberta.encoder.layer.11.output.dense', 'roberta.encoder.layer.11.output.LayerNorm', 'roberta.encoder.layer.11.output.dropout', 'lm_head', 'lm_head.dense', 'lm_head.layer_norm', 'lm_head.decoder']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd176cc150a4dee831b31cfda12abbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bee2f610d1543aeb6ca3c415a6bfb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ba0b6405e42b9b80a4e56db45dcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8283f5958940e48554386ff9f310b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo72\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\leo72\\AppData\\Local\\Temp\\ipykernel_29388\\3991497891.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d91a529fc84217b231c344ed38eeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1368, 'grad_norm': 2.732752799987793, 'learning_rate': 0.0004986680518926982, 'epoch': 0.01}\n",
      "{'loss': 3.0099, 'grad_norm': 3.0292582511901855, 'learning_rate': 0.0004973361037853965, 'epoch': 0.02}\n",
      "{'loss': 2.913, 'grad_norm': 2.477691411972046, 'learning_rate': 0.0004960041556780948, 'epoch': 0.02}\n",
      "{'loss': 2.8346, 'grad_norm': 2.91054105758667, 'learning_rate': 0.0004946722075707931, 'epoch': 0.03}\n",
      "{'loss': 2.8479, 'grad_norm': 2.6438703536987305, 'learning_rate': 0.0004933535789445644, 'epoch': 0.04}\n",
      "{'loss': 2.824, 'grad_norm': 3.6584205627441406, 'learning_rate': 0.0004920216308372626, 'epoch': 0.05}\n",
      "{'loss': 2.7525, 'grad_norm': 3.443819999694824, 'learning_rate': 0.0004906896827299609, 'epoch': 0.06}\n",
      "{'loss': 2.7015, 'grad_norm': 2.6627304553985596, 'learning_rate': 0.0004893577346226591, 'epoch': 0.06}\n",
      "{'loss': 2.7652, 'grad_norm': 3.3510396480560303, 'learning_rate': 0.0004880257865153574, 'epoch': 0.07}\n",
      "{'loss': 2.7167, 'grad_norm': 4.011176586151123, 'learning_rate': 0.00048669383840805567, 'epoch': 0.08}\n",
      "{'loss': 2.7044, 'grad_norm': 3.67460036277771, 'learning_rate': 0.0004853752097818269, 'epoch': 0.09}\n",
      "{'loss': 2.6518, 'grad_norm': 3.060028553009033, 'learning_rate': 0.0004840432616745252, 'epoch': 0.1}\n",
      "{'loss': 2.6444, 'grad_norm': 3.384659767150879, 'learning_rate': 0.00048271131356722344, 'epoch': 0.1}\n",
      "{'loss': 2.6109, 'grad_norm': 2.9321823120117188, 'learning_rate': 0.00048137936545992164, 'epoch': 0.11}\n",
      "{'loss': 2.6431, 'grad_norm': 3.334712028503418, 'learning_rate': 0.00048004741735261996, 'epoch': 0.12}\n",
      "{'loss': 2.6271, 'grad_norm': 4.30015754699707, 'learning_rate': 0.0004787154692453182, 'epoch': 0.13}\n",
      "{'loss': 2.6, 'grad_norm': 3.081502676010132, 'learning_rate': 0.0004773835211380165, 'epoch': 0.14}\n",
      "{'loss': 2.5667, 'grad_norm': 4.5047430992126465, 'learning_rate': 0.00047605157303071474, 'epoch': 0.14}\n",
      "{'loss': 2.557, 'grad_norm': 4.265442848205566, 'learning_rate': 0.00047471962492341295, 'epoch': 0.15}\n",
      "{'loss': 2.5234, 'grad_norm': 3.182345151901245, 'learning_rate': 0.00047338767681611127, 'epoch': 0.16}\n",
      "{'loss': 2.6014, 'grad_norm': 3.794981002807617, 'learning_rate': 0.00047205572870880953, 'epoch': 0.17}\n",
      "{'loss': 2.5932, 'grad_norm': 4.040923118591309, 'learning_rate': 0.0004707237806015078, 'epoch': 0.18}\n",
      "{'loss': 2.5705, 'grad_norm': 3.7622485160827637, 'learning_rate': 0.00046939183249420605, 'epoch': 0.18}\n",
      "{'loss': 2.641, 'grad_norm': 3.5529391765594482, 'learning_rate': 0.00046805988438690426, 'epoch': 0.19}\n",
      "{'loss': 2.5276, 'grad_norm': 4.0049614906311035, 'learning_rate': 0.0004667279362796026, 'epoch': 0.2}\n",
      "{'loss': 2.5595, 'grad_norm': 5.016360282897949, 'learning_rate': 0.00046539598817230083, 'epoch': 0.21}\n",
      "{'loss': 2.5077, 'grad_norm': 3.5975420475006104, 'learning_rate': 0.0004640640400649991, 'epoch': 0.22}\n",
      "{'loss': 2.5034, 'grad_norm': 4.680834770202637, 'learning_rate': 0.00046273209195769736, 'epoch': 0.22}\n",
      "{'loss': 2.517, 'grad_norm': 4.211978912353516, 'learning_rate': 0.00046140014385039556, 'epoch': 0.23}\n",
      "{'loss': 2.4335, 'grad_norm': 4.762767791748047, 'learning_rate': 0.0004600681957430938, 'epoch': 0.24}\n",
      "{'loss': 2.506, 'grad_norm': 4.55452823638916, 'learning_rate': 0.0004587495671168652, 'epoch': 0.25}\n",
      "{'loss': 2.4612, 'grad_norm': 2.8916244506835938, 'learning_rate': 0.0004574176190095634, 'epoch': 0.26}\n",
      "{'loss': 2.4795, 'grad_norm': 3.6839678287506104, 'learning_rate': 0.00045608567090226165, 'epoch': 0.26}\n",
      "{'loss': 2.4131, 'grad_norm': 5.327315330505371, 'learning_rate': 0.0004547537227949599, 'epoch': 0.27}\n",
      "{'loss': 2.3802, 'grad_norm': 4.682794094085693, 'learning_rate': 0.0004534217746876582, 'epoch': 0.28}\n",
      "{'loss': 2.4616, 'grad_norm': 3.848092794418335, 'learning_rate': 0.00045208982658035644, 'epoch': 0.29}\n",
      "{'loss': 2.3809, 'grad_norm': 3.400266408920288, 'learning_rate': 0.0004507578784730547, 'epoch': 0.3}\n",
      "{'loss': 2.4182, 'grad_norm': 3.7408628463745117, 'learning_rate': 0.00044942593036575296, 'epoch': 0.3}\n",
      "{'loss': 2.4423, 'grad_norm': 3.9559247493743896, 'learning_rate': 0.0004480939822584512, 'epoch': 0.31}\n",
      "{'loss': 2.4419, 'grad_norm': 4.129666805267334, 'learning_rate': 0.0004467620341511495, 'epoch': 0.32}\n",
      "{'loss': 2.4347, 'grad_norm': 3.9509127140045166, 'learning_rate': 0.0004454300860438477, 'epoch': 0.33}\n",
      "{'loss': 2.4499, 'grad_norm': 4.521331787109375, 'learning_rate': 0.000444098137936546, 'epoch': 0.34}\n",
      "{'loss': 2.3858, 'grad_norm': 5.256606101989746, 'learning_rate': 0.00044276618982924427, 'epoch': 0.34}\n",
      "{'loss': 2.4251, 'grad_norm': 4.335057735443115, 'learning_rate': 0.0004414342417219425, 'epoch': 0.35}\n",
      "{'loss': 2.4496, 'grad_norm': 4.796816349029541, 'learning_rate': 0.0004401022936146408, 'epoch': 0.36}\n",
      "{'loss': 2.4096, 'grad_norm': 6.099845886230469, 'learning_rate': 0.000438770345507339, 'epoch': 0.37}\n",
      "{'loss': 2.3855, 'grad_norm': 3.931281566619873, 'learning_rate': 0.0004374383974000373, 'epoch': 0.38}\n",
      "{'loss': 2.4422, 'grad_norm': 4.313470363616943, 'learning_rate': 0.00043610644929273557, 'epoch': 0.38}\n",
      "{'loss': 2.3853, 'grad_norm': 3.8221967220306396, 'learning_rate': 0.00043477450118543383, 'epoch': 0.39}\n",
      "{'loss': 2.4277, 'grad_norm': 3.2951762676239014, 'learning_rate': 0.0004334425530781321, 'epoch': 0.4}\n",
      "{'loss': 2.3528, 'grad_norm': 5.738968372344971, 'learning_rate': 0.0004321106049708303, 'epoch': 0.41}\n",
      "{'loss': 2.4265, 'grad_norm': 4.706576347351074, 'learning_rate': 0.0004307919763446016, 'epoch': 0.42}\n",
      "{'loss': 2.3637, 'grad_norm': 4.961411952972412, 'learning_rate': 0.0004294600282372999, 'epoch': 0.42}\n",
      "{'loss': 2.3317, 'grad_norm': 5.286367416381836, 'learning_rate': 0.00042812808012999813, 'epoch': 0.43}\n",
      "{'loss': 2.3795, 'grad_norm': 4.200657367706299, 'learning_rate': 0.0004267961320226964, 'epoch': 0.44}\n",
      "{'loss': 2.3933, 'grad_norm': 3.640956163406372, 'learning_rate': 0.00042546418391539465, 'epoch': 0.45}\n",
      "{'loss': 2.3452, 'grad_norm': 4.000317573547363, 'learning_rate': 0.0004241322358080929, 'epoch': 0.46}\n",
      "{'loss': 2.3669, 'grad_norm': 5.197394371032715, 'learning_rate': 0.00042280028770079123, 'epoch': 0.46}\n",
      "{'loss': 2.3845, 'grad_norm': 4.535641670227051, 'learning_rate': 0.00042146833959348943, 'epoch': 0.47}\n",
      "{'loss': 2.3195, 'grad_norm': 4.961315155029297, 'learning_rate': 0.0004201363914861877, 'epoch': 0.48}\n",
      "{'loss': 2.3505, 'grad_norm': 5.2347025871276855, 'learning_rate': 0.00041880444337888596, 'epoch': 0.49}\n",
      "{'loss': 2.4117, 'grad_norm': 5.447612285614014, 'learning_rate': 0.0004174724952715842, 'epoch': 0.5}\n",
      "{'loss': 2.411, 'grad_norm': 3.6135759353637695, 'learning_rate': 0.00041614054716428253, 'epoch': 0.5}\n",
      "{'loss': 2.4295, 'grad_norm': 6.219202518463135, 'learning_rate': 0.00041480859905698074, 'epoch': 0.51}\n",
      "{'loss': 2.4159, 'grad_norm': 4.610805988311768, 'learning_rate': 0.000413476650949679, 'epoch': 0.52}\n",
      "{'loss': 2.3433, 'grad_norm': 4.918600082397461, 'learning_rate': 0.00041214470284237726, 'epoch': 0.53}\n",
      "{'loss': 2.3272, 'grad_norm': 4.779319763183594, 'learning_rate': 0.0004108127547350755, 'epoch': 0.54}\n",
      "{'loss': 2.3283, 'grad_norm': 4.660211563110352, 'learning_rate': 0.00040948080662777384, 'epoch': 0.54}\n",
      "{'loss': 2.4031, 'grad_norm': 4.773199081420898, 'learning_rate': 0.00040814885852047205, 'epoch': 0.55}\n",
      "{'loss': 2.3578, 'grad_norm': 3.485262155532837, 'learning_rate': 0.00040683022989424335, 'epoch': 0.56}\n",
      "{'loss': 2.3414, 'grad_norm': 4.750330448150635, 'learning_rate': 0.0004054982817869416, 'epoch': 0.57}\n",
      "{'loss': 2.3475, 'grad_norm': 4.7572808265686035, 'learning_rate': 0.0004041663336796398, 'epoch': 0.58}\n",
      "{'loss': 2.3291, 'grad_norm': 4.459456920623779, 'learning_rate': 0.0004028343855723381, 'epoch': 0.58}\n",
      "{'loss': 2.3468, 'grad_norm': 4.769024848937988, 'learning_rate': 0.0004015024374650364, 'epoch': 0.59}\n",
      "{'loss': 2.3375, 'grad_norm': 3.7228522300720215, 'learning_rate': 0.00040017048935773466, 'epoch': 0.6}\n",
      "{'loss': 2.314, 'grad_norm': 5.353236198425293, 'learning_rate': 0.0003988385412504329, 'epoch': 0.61}\n",
      "{'loss': 2.3749, 'grad_norm': 5.861856937408447, 'learning_rate': 0.0003975065931431311, 'epoch': 0.62}\n",
      "{'loss': 2.319, 'grad_norm': 6.393261909484863, 'learning_rate': 0.0003961746450358294, 'epoch': 0.62}\n",
      "{'loss': 2.3288, 'grad_norm': 5.450260639190674, 'learning_rate': 0.0003948426969285277, 'epoch': 0.63}\n",
      "{'loss': 2.3388, 'grad_norm': 5.707064628601074, 'learning_rate': 0.00039351074882122597, 'epoch': 0.64}\n",
      "{'loss': 2.3088, 'grad_norm': 4.342197895050049, 'learning_rate': 0.0003921788007139242, 'epoch': 0.65}\n",
      "{'loss': 2.3102, 'grad_norm': 5.315873622894287, 'learning_rate': 0.00039084685260662243, 'epoch': 0.66}\n",
      "{'loss': 2.3965, 'grad_norm': 4.705062389373779, 'learning_rate': 0.0003895149044993207, 'epoch': 0.66}\n",
      "{'loss': 2.3335, 'grad_norm': 3.8893792629241943, 'learning_rate': 0.00038818295639201896, 'epoch': 0.67}\n",
      "{'loss': 2.3278, 'grad_norm': 3.87214732170105, 'learning_rate': 0.00038685100828471727, 'epoch': 0.68}\n",
      "{'loss': 2.3199, 'grad_norm': 4.7409772872924805, 'learning_rate': 0.0003855190601774155, 'epoch': 0.69}\n",
      "{'loss': 2.3543, 'grad_norm': 3.800950050354004, 'learning_rate': 0.00038418711207011374, 'epoch': 0.7}\n",
      "{'loss': 2.2952, 'grad_norm': 4.960910797119141, 'learning_rate': 0.000382855163962812, 'epoch': 0.7}\n",
      "{'loss': 2.2939, 'grad_norm': 4.0987372398376465, 'learning_rate': 0.00038152321585551026, 'epoch': 0.71}\n",
      "{'loss': 2.3313, 'grad_norm': 5.018887519836426, 'learning_rate': 0.0003801912677482086, 'epoch': 0.72}\n",
      "{'loss': 2.3243, 'grad_norm': 4.482113361358643, 'learning_rate': 0.0003788593196409068, 'epoch': 0.73}\n",
      "{'loss': 2.3142, 'grad_norm': 5.078419208526611, 'learning_rate': 0.00037752737153360505, 'epoch': 0.74}\n",
      "{'loss': 2.3022, 'grad_norm': 4.784209251403809, 'learning_rate': 0.0003761954234263033, 'epoch': 0.74}\n",
      "{'loss': 2.3096, 'grad_norm': 5.493999004364014, 'learning_rate': 0.00037486347531900157, 'epoch': 0.75}\n",
      "{'loss': 2.3551, 'grad_norm': 4.876006126403809, 'learning_rate': 0.0003735315272116999, 'epoch': 0.76}\n",
      "{'loss': 2.3926, 'grad_norm': 4.721347332000732, 'learning_rate': 0.0003721995791043981, 'epoch': 0.77}\n",
      "{'loss': 2.3725, 'grad_norm': 4.954073905944824, 'learning_rate': 0.00037086763099709636, 'epoch': 0.78}\n",
      "{'loss': 2.3215, 'grad_norm': 3.975302219390869, 'learning_rate': 0.0003695356828897946, 'epoch': 0.78}\n",
      "{'loss': 2.3096, 'grad_norm': 3.6432676315307617, 'learning_rate': 0.0003682037347824929, 'epoch': 0.79}\n",
      "{'loss': 2.3162, 'grad_norm': 6.298996448516846, 'learning_rate': 0.00036687178667519114, 'epoch': 0.8}\n",
      "{'loss': 2.2986, 'grad_norm': 5.0301833152771, 'learning_rate': 0.0003655398385678894, 'epoch': 0.81}\n",
      "{'loss': 2.3633, 'grad_norm': 5.237517833709717, 'learning_rate': 0.00036420789046058766, 'epoch': 0.82}\n",
      "{'loss': 2.3416, 'grad_norm': 4.81888484954834, 'learning_rate': 0.0003628759423532859, 'epoch': 0.82}\n",
      "{'loss': 2.3375, 'grad_norm': 5.261603355407715, 'learning_rate': 0.0003615439942459842, 'epoch': 0.83}\n",
      "{'loss': 2.2479, 'grad_norm': 4.711301803588867, 'learning_rate': 0.00036022536561975543, 'epoch': 0.84}\n",
      "{'loss': 2.2852, 'grad_norm': 5.076519966125488, 'learning_rate': 0.00035889341751245375, 'epoch': 0.85}\n",
      "{'loss': 2.3074, 'grad_norm': 4.715648651123047, 'learning_rate': 0.000357561469405152, 'epoch': 0.86}\n",
      "{'loss': 2.3361, 'grad_norm': 5.4985246658325195, 'learning_rate': 0.00035622952129785027, 'epoch': 0.86}\n",
      "{'loss': 2.2468, 'grad_norm': 6.3859543800354, 'learning_rate': 0.0003548975731905485, 'epoch': 0.87}\n",
      "{'loss': 2.3444, 'grad_norm': 5.418402671813965, 'learning_rate': 0.00035356562508324674, 'epoch': 0.88}\n",
      "{'loss': 2.3161, 'grad_norm': 4.093523025512695, 'learning_rate': 0.00035223367697594506, 'epoch': 0.89}\n",
      "{'loss': 2.324, 'grad_norm': 5.629938125610352, 'learning_rate': 0.0003509017288686433, 'epoch': 0.9}\n",
      "{'loss': 2.3301, 'grad_norm': 5.725546360015869, 'learning_rate': 0.0003495697807613415, 'epoch': 0.9}\n",
      "{'loss': 2.2807, 'grad_norm': 6.677602767944336, 'learning_rate': 0.0003482378326540398, 'epoch': 0.91}\n",
      "{'loss': 2.2495, 'grad_norm': 4.594287872314453, 'learning_rate': 0.00034690588454673805, 'epoch': 0.92}\n",
      "{'loss': 2.3088, 'grad_norm': 5.605828285217285, 'learning_rate': 0.00034557393643943636, 'epoch': 0.93}\n",
      "{'loss': 2.3083, 'grad_norm': 4.0877366065979, 'learning_rate': 0.0003442419883321346, 'epoch': 0.94}\n",
      "{'loss': 2.2776, 'grad_norm': 5.1784491539001465, 'learning_rate': 0.00034291004022483283, 'epoch': 0.94}\n",
      "{'loss': 2.2658, 'grad_norm': 6.817361354827881, 'learning_rate': 0.0003415780921175311, 'epoch': 0.95}\n",
      "{'loss': 2.3722, 'grad_norm': 5.998700141906738, 'learning_rate': 0.00034024614401022935, 'epoch': 0.96}\n",
      "{'loss': 2.2756, 'grad_norm': 7.395671844482422, 'learning_rate': 0.0003389141959029276, 'epoch': 0.97}\n",
      "{'loss': 2.2747, 'grad_norm': 5.043878078460693, 'learning_rate': 0.00033758224779562593, 'epoch': 0.97}\n",
      "{'loss': 2.2759, 'grad_norm': 4.592922687530518, 'learning_rate': 0.00033625029968832414, 'epoch': 0.98}\n",
      "{'loss': 2.2255, 'grad_norm': 5.046980857849121, 'learning_rate': 0.0003349183515810224, 'epoch': 0.99}\n",
      "{'loss': 2.2347, 'grad_norm': 5.68693208694458, 'learning_rate': 0.00033358640347372066, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e288975caa463dafe52e87de35126b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0636942386627197, 'eval_runtime': 186.7836, 'eval_samples_per_second': 267.962, 'eval_steps_per_second': 16.752, 'epoch': 1.0}\n",
      "{'loss': 2.2443, 'grad_norm': 4.779221534729004, 'learning_rate': 0.0003322544553664189, 'epoch': 1.01}\n",
      "{'loss': 2.3147, 'grad_norm': 5.842565536499023, 'learning_rate': 0.0003309225072591172, 'epoch': 1.01}\n",
      "{'loss': 2.2626, 'grad_norm': 5.922913074493408, 'learning_rate': 0.00032959055915181545, 'epoch': 1.02}\n",
      "{'loss': 2.237, 'grad_norm': 4.585598945617676, 'learning_rate': 0.0003282586110445137, 'epoch': 1.03}\n",
      "{'loss': 2.3169, 'grad_norm': 6.996572017669678, 'learning_rate': 0.00032692666293721197, 'epoch': 1.04}\n",
      "{'loss': 2.2411, 'grad_norm': 5.235696315765381, 'learning_rate': 0.00032559471482991023, 'epoch': 1.05}\n",
      "{'loss': 2.3266, 'grad_norm': 4.901758193969727, 'learning_rate': 0.0003242627667226085, 'epoch': 1.05}\n",
      "{'loss': 2.2313, 'grad_norm': 5.304924964904785, 'learning_rate': 0.00032293081861530675, 'epoch': 1.06}\n",
      "{'loss': 2.2944, 'grad_norm': 5.6912431716918945, 'learning_rate': 0.000321598870508005, 'epoch': 1.07}\n",
      "{'loss': 2.2196, 'grad_norm': 6.645760536193848, 'learning_rate': 0.0003202669224007033, 'epoch': 1.08}\n",
      "{'loss': 2.2669, 'grad_norm': 4.870731353759766, 'learning_rate': 0.00031893497429340154, 'epoch': 1.09}\n",
      "{'loss': 2.3075, 'grad_norm': 4.814261436462402, 'learning_rate': 0.0003176030261860998, 'epoch': 1.09}\n",
      "{'loss': 2.2819, 'grad_norm': 4.544769763946533, 'learning_rate': 0.00031627107807879806, 'epoch': 1.1}\n",
      "{'loss': 2.2703, 'grad_norm': 5.12971305847168, 'learning_rate': 0.0003149391299714963, 'epoch': 1.11}\n",
      "{'loss': 2.2457, 'grad_norm': 5.78225040435791, 'learning_rate': 0.0003136071818641946, 'epoch': 1.12}\n",
      "{'loss': 2.2951, 'grad_norm': 4.938091278076172, 'learning_rate': 0.00031227523375689284, 'epoch': 1.13}\n",
      "{'loss': 2.2772, 'grad_norm': 5.266896724700928, 'learning_rate': 0.0003109432856495911, 'epoch': 1.13}\n",
      "{'loss': 2.1996, 'grad_norm': 6.570862770080566, 'learning_rate': 0.00030961133754228937, 'epoch': 1.14}\n",
      "{'loss': 2.2729, 'grad_norm': 6.207876682281494, 'learning_rate': 0.00030827938943498763, 'epoch': 1.15}\n",
      "{'loss': 2.2951, 'grad_norm': 5.716145038604736, 'learning_rate': 0.0003069607608087589, 'epoch': 1.16}\n",
      "{'loss': 2.2283, 'grad_norm': 6.436923503875732, 'learning_rate': 0.00030562881270145714, 'epoch': 1.17}\n",
      "{'loss': 2.2348, 'grad_norm': 5.265610694885254, 'learning_rate': 0.0003042968645941554, 'epoch': 1.17}\n",
      "{'loss': 2.2802, 'grad_norm': 7.5213494300842285, 'learning_rate': 0.0003029649164868537, 'epoch': 1.18}\n",
      "{'loss': 2.2826, 'grad_norm': 5.450938701629639, 'learning_rate': 0.000301632968379552, 'epoch': 1.19}\n",
      "{'loss': 2.2384, 'grad_norm': 5.82449197769165, 'learning_rate': 0.0003003010202722502, 'epoch': 1.2}\n",
      "{'loss': 2.2815, 'grad_norm': 5.820165157318115, 'learning_rate': 0.00029896907216494845, 'epoch': 1.21}\n",
      "{'loss': 2.2069, 'grad_norm': 6.248181343078613, 'learning_rate': 0.0002976371240576467, 'epoch': 1.21}\n",
      "{'loss': 2.2347, 'grad_norm': 5.161111831665039, 'learning_rate': 0.000296305175950345, 'epoch': 1.22}\n",
      "{'loss': 2.1803, 'grad_norm': 5.8171257972717285, 'learning_rate': 0.0002949732278430433, 'epoch': 1.23}\n",
      "{'loss': 2.2664, 'grad_norm': 6.546154022216797, 'learning_rate': 0.0002936412797357415, 'epoch': 1.24}\n",
      "{'loss': 2.234, 'grad_norm': 5.926713466644287, 'learning_rate': 0.00029230933162843975, 'epoch': 1.25}\n",
      "{'loss': 2.2504, 'grad_norm': 6.684126853942871, 'learning_rate': 0.000290977383521138, 'epoch': 1.25}\n",
      "{'loss': 2.2797, 'grad_norm': 4.931268692016602, 'learning_rate': 0.0002896454354138363, 'epoch': 1.26}\n",
      "{'loss': 2.28, 'grad_norm': 5.9488325119018555, 'learning_rate': 0.00028831348730653454, 'epoch': 1.27}\n",
      "{'loss': 2.2658, 'grad_norm': 4.634033679962158, 'learning_rate': 0.0002869815391992328, 'epoch': 1.28}\n",
      "{'loss': 2.3017, 'grad_norm': 6.345796585083008, 'learning_rate': 0.00028564959109193106, 'epoch': 1.29}\n",
      "{'loss': 2.2182, 'grad_norm': 4.481930255889893, 'learning_rate': 0.00028433096246570236, 'epoch': 1.29}\n",
      "{'loss': 2.2627, 'grad_norm': 4.936617851257324, 'learning_rate': 0.00028299901435840057, 'epoch': 1.3}\n",
      "{'loss': 2.3256, 'grad_norm': 4.455478191375732, 'learning_rate': 0.00028166706625109883, 'epoch': 1.31}\n",
      "{'loss': 2.215, 'grad_norm': 4.273805141448975, 'learning_rate': 0.00028033511814379715, 'epoch': 1.32}\n",
      "{'loss': 2.2317, 'grad_norm': 5.50758171081543, 'learning_rate': 0.0002790031700364954, 'epoch': 1.33}\n",
      "{'loss': 2.2253, 'grad_norm': 6.018428802490234, 'learning_rate': 0.0002776712219291936, 'epoch': 1.33}\n",
      "{'loss': 2.2691, 'grad_norm': 3.7782230377197266, 'learning_rate': 0.0002763392738218919, 'epoch': 1.34}\n",
      "{'loss': 2.2057, 'grad_norm': 5.367452621459961, 'learning_rate': 0.00027500732571459014, 'epoch': 1.35}\n",
      "{'loss': 2.2353, 'grad_norm': 5.080740928649902, 'learning_rate': 0.00027367537760728845, 'epoch': 1.36}\n",
      "{'loss': 2.2196, 'grad_norm': 5.760361671447754, 'learning_rate': 0.0002723434294999867, 'epoch': 1.37}\n",
      "{'loss': 2.2555, 'grad_norm': 5.206796169281006, 'learning_rate': 0.0002710114813926849, 'epoch': 1.37}\n",
      "{'loss': 2.2212, 'grad_norm': 5.586594104766846, 'learning_rate': 0.0002696795332853832, 'epoch': 1.38}\n",
      "{'loss': 2.2222, 'grad_norm': 6.159029960632324, 'learning_rate': 0.00026834758517808144, 'epoch': 1.39}\n",
      "{'loss': 2.2023, 'grad_norm': 4.550049781799316, 'learning_rate': 0.00026701563707077976, 'epoch': 1.4}\n",
      "{'loss': 2.1845, 'grad_norm': 5.4454827308654785, 'learning_rate': 0.000265683688963478, 'epoch': 1.41}\n",
      "{'loss': 2.2547, 'grad_norm': 5.098746299743652, 'learning_rate': 0.00026435174085617623, 'epoch': 1.41}\n",
      "{'loss': 2.255, 'grad_norm': 6.1747050285339355, 'learning_rate': 0.0002630197927488745, 'epoch': 1.42}\n",
      "{'loss': 2.2361, 'grad_norm': 5.415824890136719, 'learning_rate': 0.00026168784464157275, 'epoch': 1.43}\n",
      "{'loss': 2.2726, 'grad_norm': 3.733720302581787, 'learning_rate': 0.00026035589653427107, 'epoch': 1.44}\n",
      "{'loss': 2.2331, 'grad_norm': 4.520310401916504, 'learning_rate': 0.00025902394842696933, 'epoch': 1.45}\n",
      "{'loss': 2.1593, 'grad_norm': 5.1253767013549805, 'learning_rate': 0.0002577053198007406, 'epoch': 1.45}\n",
      "{'loss': 2.2378, 'grad_norm': 4.774387836456299, 'learning_rate': 0.00025637337169343884, 'epoch': 1.46}\n",
      "{'loss': 2.2337, 'grad_norm': 5.041096210479736, 'learning_rate': 0.0002550414235861371, 'epoch': 1.47}\n",
      "{'loss': 2.2119, 'grad_norm': 5.8191399574279785, 'learning_rate': 0.0002537094754788353, 'epoch': 1.48}\n",
      "{'loss': 2.2514, 'grad_norm': 6.050417423248291, 'learning_rate': 0.0002523775273715336, 'epoch': 1.49}\n",
      "{'loss': 2.2081, 'grad_norm': 5.295169353485107, 'learning_rate': 0.0002510455792642319, 'epoch': 1.49}\n",
      "{'loss': 2.2283, 'grad_norm': 6.738333225250244, 'learning_rate': 0.00024971363115693015, 'epoch': 1.5}\n",
      "{'loss': 2.3151, 'grad_norm': 6.420480251312256, 'learning_rate': 0.0002483816830496284, 'epoch': 1.51}\n",
      "{'loss': 2.2496, 'grad_norm': 4.657458782196045, 'learning_rate': 0.00024704973494232667, 'epoch': 1.52}\n",
      "{'loss': 2.2575, 'grad_norm': 3.931138277053833, 'learning_rate': 0.00024571778683502493, 'epoch': 1.53}\n",
      "{'loss': 2.1985, 'grad_norm': 5.611766338348389, 'learning_rate': 0.0002443858387277232, 'epoch': 1.53}\n",
      "{'loss': 2.1829, 'grad_norm': 6.413047790527344, 'learning_rate': 0.00024305389062042143, 'epoch': 1.54}\n",
      "{'loss': 2.2615, 'grad_norm': 4.617260932922363, 'learning_rate': 0.0002417219425131197, 'epoch': 1.55}\n",
      "{'loss': 2.3556, 'grad_norm': 7.036892890930176, 'learning_rate': 0.00024038999440581798, 'epoch': 1.56}\n",
      "{'loss': 2.1612, 'grad_norm': 5.318265438079834, 'learning_rate': 0.0002390580462985162, 'epoch': 1.57}\n",
      "{'loss': 2.3129, 'grad_norm': 5.997229099273682, 'learning_rate': 0.00023772609819121447, 'epoch': 1.57}\n",
      "{'loss': 2.2178, 'grad_norm': 5.282750606536865, 'learning_rate': 0.00023639415008391273, 'epoch': 1.58}\n",
      "{'loss': 2.1993, 'grad_norm': 8.798601150512695, 'learning_rate': 0.000235062201976611, 'epoch': 1.59}\n",
      "{'loss': 2.2089, 'grad_norm': 6.331231117248535, 'learning_rate': 0.00023373025386930926, 'epoch': 1.6}\n",
      "{'loss': 2.1614, 'grad_norm': 5.399386405944824, 'learning_rate': 0.00023239830576200752, 'epoch': 1.61}\n",
      "{'loss': 2.2108, 'grad_norm': 5.904013633728027, 'learning_rate': 0.00023106635765470578, 'epoch': 1.61}\n",
      "{'loss': 2.2845, 'grad_norm': 5.253742694854736, 'learning_rate': 0.00022973440954740404, 'epoch': 1.62}\n",
      "{'loss': 2.2255, 'grad_norm': 4.773291110992432, 'learning_rate': 0.0002284024614401023, 'epoch': 1.63}\n",
      "{'loss': 2.196, 'grad_norm': 5.2915849685668945, 'learning_rate': 0.00022707051333280056, 'epoch': 1.64}\n",
      "{'loss': 2.2366, 'grad_norm': 5.677837371826172, 'learning_rate': 0.00022573856522549882, 'epoch': 1.65}\n",
      "{'loss': 2.216, 'grad_norm': 5.146585941314697, 'learning_rate': 0.00022440661711819706, 'epoch': 1.65}\n",
      "{'loss': 2.2499, 'grad_norm': 4.781584739685059, 'learning_rate': 0.00022307466901089535, 'epoch': 1.66}\n",
      "{'loss': 2.2485, 'grad_norm': 6.8456196784973145, 'learning_rate': 0.0002217427209035936, 'epoch': 1.67}\n",
      "{'loss': 2.2303, 'grad_norm': 5.522871971130371, 'learning_rate': 0.00022041077279629184, 'epoch': 1.68}\n",
      "{'loss': 2.1798, 'grad_norm': 5.538457870483398, 'learning_rate': 0.00021907882468899013, 'epoch': 1.69}\n",
      "{'loss': 2.1993, 'grad_norm': 4.685487270355225, 'learning_rate': 0.00021774687658168837, 'epoch': 1.69}\n",
      "{'loss': 2.2295, 'grad_norm': 5.569851875305176, 'learning_rate': 0.00021641492847438665, 'epoch': 1.7}\n",
      "{'loss': 2.2165, 'grad_norm': 4.746661186218262, 'learning_rate': 0.00021508298036708492, 'epoch': 1.71}\n",
      "{'loss': 2.1382, 'grad_norm': 4.769225120544434, 'learning_rate': 0.00021375103225978315, 'epoch': 1.72}\n",
      "{'loss': 2.1635, 'grad_norm': 5.539544582366943, 'learning_rate': 0.00021241908415248144, 'epoch': 1.73}\n",
      "{'loss': 2.253, 'grad_norm': 7.298468589782715, 'learning_rate': 0.00021108713604517967, 'epoch': 1.73}\n",
      "{'loss': 2.1882, 'grad_norm': 5.649975776672363, 'learning_rate': 0.00020975518793787796, 'epoch': 1.74}\n",
      "{'loss': 2.2105, 'grad_norm': 5.102168083190918, 'learning_rate': 0.0002084232398305762, 'epoch': 1.75}\n",
      "{'loss': 2.2406, 'grad_norm': 5.812136173248291, 'learning_rate': 0.00020709129172327446, 'epoch': 1.76}\n",
      "{'loss': 2.1581, 'grad_norm': 6.642938137054443, 'learning_rate': 0.00020575934361597275, 'epoch': 1.77}\n",
      "{'loss': 2.2045, 'grad_norm': 4.919799327850342, 'learning_rate': 0.00020444071498974402, 'epoch': 1.77}\n",
      "{'loss': 2.1907, 'grad_norm': 4.826469421386719, 'learning_rate': 0.00020310876688244225, 'epoch': 1.78}\n",
      "{'loss': 2.237, 'grad_norm': 5.038005352020264, 'learning_rate': 0.00020177681877514054, 'epoch': 1.79}\n",
      "{'loss': 2.2377, 'grad_norm': 5.117117404937744, 'learning_rate': 0.00020044487066783878, 'epoch': 1.8}\n",
      "{'loss': 2.2226, 'grad_norm': 5.000080108642578, 'learning_rate': 0.00019911292256053704, 'epoch': 1.81}\n",
      "{'loss': 2.1798, 'grad_norm': 5.661811351776123, 'learning_rate': 0.0001977809744532353, 'epoch': 1.81}\n",
      "{'loss': 2.1636, 'grad_norm': 4.323950290679932, 'learning_rate': 0.00019644902634593356, 'epoch': 1.82}\n",
      "{'loss': 2.2358, 'grad_norm': 4.864846229553223, 'learning_rate': 0.00019511707823863185, 'epoch': 1.83}\n",
      "{'loss': 2.1859, 'grad_norm': 4.483724117279053, 'learning_rate': 0.00019378513013133008, 'epoch': 1.84}\n",
      "{'loss': 2.1972, 'grad_norm': 6.146681308746338, 'learning_rate': 0.00019245318202402835, 'epoch': 1.85}\n",
      "{'loss': 2.2057, 'grad_norm': 5.880358695983887, 'learning_rate': 0.0001911212339167266, 'epoch': 1.85}\n",
      "{'loss': 2.1867, 'grad_norm': 5.413558006286621, 'learning_rate': 0.00018978928580942487, 'epoch': 1.86}\n",
      "{'loss': 2.1662, 'grad_norm': 5.786203384399414, 'learning_rate': 0.0001884573377021231, 'epoch': 1.87}\n",
      "{'loss': 2.1721, 'grad_norm': 6.485547065734863, 'learning_rate': 0.0001871253895948214, 'epoch': 1.88}\n",
      "{'loss': 2.159, 'grad_norm': 5.55372953414917, 'learning_rate': 0.00018579344148751965, 'epoch': 1.89}\n",
      "{'loss': 2.1977, 'grad_norm': 4.681952476501465, 'learning_rate': 0.00018446149338021791, 'epoch': 1.89}\n",
      "{'loss': 2.1806, 'grad_norm': 6.619851112365723, 'learning_rate': 0.00018312954527291618, 'epoch': 1.9}\n",
      "{'loss': 2.209, 'grad_norm': 5.820911884307861, 'learning_rate': 0.0001817975971656144, 'epoch': 1.91}\n",
      "{'loss': 2.1802, 'grad_norm': 5.077188491821289, 'learning_rate': 0.0001804656490583127, 'epoch': 1.92}\n",
      "{'loss': 2.1941, 'grad_norm': 5.344286918640137, 'learning_rate': 0.00017913370095101096, 'epoch': 1.93}\n",
      "{'loss': 2.2076, 'grad_norm': 3.7459120750427246, 'learning_rate': 0.00017780175284370922, 'epoch': 1.93}\n",
      "{'loss': 2.1782, 'grad_norm': 5.105837345123291, 'learning_rate': 0.0001764831242174805, 'epoch': 1.94}\n",
      "{'loss': 2.2192, 'grad_norm': 5.0781121253967285, 'learning_rate': 0.00017515117611017876, 'epoch': 1.95}\n",
      "{'loss': 2.2179, 'grad_norm': 4.761772155761719, 'learning_rate': 0.000173819228002877, 'epoch': 1.96}\n",
      "{'loss': 2.2164, 'grad_norm': 5.716140270233154, 'learning_rate': 0.00017248727989557528, 'epoch': 1.97}\n",
      "{'loss': 2.1267, 'grad_norm': 5.163365840911865, 'learning_rate': 0.00017115533178827352, 'epoch': 1.97}\n",
      "{'loss': 2.163, 'grad_norm': 5.197056770324707, 'learning_rate': 0.0001698233836809718, 'epoch': 1.98}\n",
      "{'loss': 2.2145, 'grad_norm': 4.567313194274902, 'learning_rate': 0.00016849143557367007, 'epoch': 1.99}\n",
      "{'loss': 2.2101, 'grad_norm': 6.164327621459961, 'learning_rate': 0.0001671594874663683, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf9e25c320b465ab2ec1c11e9376fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9804930686950684, 'eval_runtime': 174.8271, 'eval_samples_per_second': 286.289, 'eval_steps_per_second': 17.898, 'epoch': 2.0}\n",
      "{'loss': 2.1674, 'grad_norm': 4.986512660980225, 'learning_rate': 0.0001658275393590666, 'epoch': 2.01}\n",
      "{'loss': 2.1813, 'grad_norm': 4.670063018798828, 'learning_rate': 0.00016449559125176482, 'epoch': 2.01}\n",
      "{'loss': 2.1934, 'grad_norm': 6.169525146484375, 'learning_rate': 0.0001631636431444631, 'epoch': 2.02}\n",
      "{'loss': 2.1633, 'grad_norm': 5.671971321105957, 'learning_rate': 0.00016183169503716135, 'epoch': 2.03}\n",
      "{'loss': 2.154, 'grad_norm': 6.124148368835449, 'learning_rate': 0.0001604997469298596, 'epoch': 2.04}\n",
      "{'loss': 2.1516, 'grad_norm': 6.246192455291748, 'learning_rate': 0.0001591677988225579, 'epoch': 2.05}\n",
      "{'loss': 2.1772, 'grad_norm': 6.866201400756836, 'learning_rate': 0.00015783585071525613, 'epoch': 2.05}\n",
      "{'loss': 2.1887, 'grad_norm': 4.121890544891357, 'learning_rate': 0.0001565039026079544, 'epoch': 2.06}\n",
      "{'loss': 2.2179, 'grad_norm': 6.99081563949585, 'learning_rate': 0.00015517195450065265, 'epoch': 2.07}\n",
      "{'loss': 2.1992, 'grad_norm': 5.478553771972656, 'learning_rate': 0.00015384000639335091, 'epoch': 2.08}\n",
      "{'loss': 2.1063, 'grad_norm': 5.196871757507324, 'learning_rate': 0.0001525080582860492, 'epoch': 2.09}\n",
      "{'loss': 2.1664, 'grad_norm': 5.629741191864014, 'learning_rate': 0.00015117611017874744, 'epoch': 2.09}\n",
      "{'loss': 2.2165, 'grad_norm': 5.0685648918151855, 'learning_rate': 0.0001498441620714457, 'epoch': 2.1}\n",
      "{'loss': 2.2078, 'grad_norm': 5.311417579650879, 'learning_rate': 0.00014851221396414396, 'epoch': 2.11}\n",
      "{'loss': 2.1519, 'grad_norm': 4.987418174743652, 'learning_rate': 0.00014718026585684222, 'epoch': 2.12}\n",
      "{'loss': 2.1411, 'grad_norm': 5.332749366760254, 'learning_rate': 0.00014584831774954048, 'epoch': 2.13}\n",
      "{'loss': 2.2332, 'grad_norm': 4.996980667114258, 'learning_rate': 0.00014451636964223874, 'epoch': 2.13}\n",
      "{'loss': 2.1399, 'grad_norm': 5.026033878326416, 'learning_rate': 0.000143184421534937, 'epoch': 2.14}\n",
      "{'loss': 2.2077, 'grad_norm': 4.904752731323242, 'learning_rate': 0.00014185247342763527, 'epoch': 2.15}\n",
      "{'loss': 2.204, 'grad_norm': 6.006564140319824, 'learning_rate': 0.00014052052532033353, 'epoch': 2.16}\n",
      "{'loss': 2.1314, 'grad_norm': 5.830959320068359, 'learning_rate': 0.00013918857721303176, 'epoch': 2.17}\n",
      "{'loss': 2.1972, 'grad_norm': 5.049122333526611, 'learning_rate': 0.00013785662910573005, 'epoch': 2.17}\n",
      "{'loss': 2.1656, 'grad_norm': 5.636696815490723, 'learning_rate': 0.00013652468099842829, 'epoch': 2.18}\n",
      "{'loss': 2.2216, 'grad_norm': 5.668638229370117, 'learning_rate': 0.00013519273289112657, 'epoch': 2.19}\n",
      "{'loss': 2.1717, 'grad_norm': 5.292688369750977, 'learning_rate': 0.00013386078478382484, 'epoch': 2.2}\n",
      "{'loss': 2.1503, 'grad_norm': 5.813315391540527, 'learning_rate': 0.00013252883667652307, 'epoch': 2.21}\n",
      "{'loss': 2.2345, 'grad_norm': 5.186374187469482, 'learning_rate': 0.00013119688856922136, 'epoch': 2.21}\n",
      "{'loss': 2.1911, 'grad_norm': 6.750067710876465, 'learning_rate': 0.0001298649404619196, 'epoch': 2.22}\n",
      "{'loss': 2.1737, 'grad_norm': 8.279193878173828, 'learning_rate': 0.00012854631183569087, 'epoch': 2.23}\n",
      "{'loss': 2.2045, 'grad_norm': 5.100640296936035, 'learning_rate': 0.00012721436372838916, 'epoch': 2.24}\n",
      "{'loss': 2.2013, 'grad_norm': 5.058160781860352, 'learning_rate': 0.00012588241562108742, 'epoch': 2.25}\n",
      "{'loss': 2.1695, 'grad_norm': 7.993875026702881, 'learning_rate': 0.00012455046751378568, 'epoch': 2.25}\n",
      "{'loss': 2.1847, 'grad_norm': 6.925723552703857, 'learning_rate': 0.00012321851940648394, 'epoch': 2.26}\n",
      "{'loss': 2.1192, 'grad_norm': 5.050206661224365, 'learning_rate': 0.00012188657129918217, 'epoch': 2.27}\n",
      "{'loss': 2.2238, 'grad_norm': 5.1704301834106445, 'learning_rate': 0.00012055462319188045, 'epoch': 2.28}\n",
      "{'loss': 2.1893, 'grad_norm': 4.848993301391602, 'learning_rate': 0.00011922267508457871, 'epoch': 2.29}\n",
      "{'loss': 2.203, 'grad_norm': 5.802577495574951, 'learning_rate': 0.00011789072697727697, 'epoch': 2.29}\n",
      "{'loss': 2.238, 'grad_norm': 5.659438610076904, 'learning_rate': 0.00011655877886997523, 'epoch': 2.3}\n",
      "{'loss': 2.1379, 'grad_norm': 4.273326396942139, 'learning_rate': 0.00011522683076267348, 'epoch': 2.31}\n",
      "{'loss': 2.1662, 'grad_norm': 5.206600666046143, 'learning_rate': 0.00011389488265537174, 'epoch': 2.32}\n",
      "{'loss': 2.2636, 'grad_norm': 4.801243305206299, 'learning_rate': 0.00011256293454807002, 'epoch': 2.33}\n",
      "{'loss': 2.2302, 'grad_norm': 6.292725086212158, 'learning_rate': 0.00011123098644076828, 'epoch': 2.33}\n",
      "{'loss': 2.1796, 'grad_norm': 4.680057525634766, 'learning_rate': 0.00010989903833346653, 'epoch': 2.34}\n",
      "{'loss': 2.1721, 'grad_norm': 4.6389570236206055, 'learning_rate': 0.00010856709022616479, 'epoch': 2.35}\n",
      "{'loss': 2.2493, 'grad_norm': 5.721165657043457, 'learning_rate': 0.00010723514211886305, 'epoch': 2.36}\n",
      "{'loss': 2.1463, 'grad_norm': 4.564913272857666, 'learning_rate': 0.00010590319401156131, 'epoch': 2.37}\n",
      "{'loss': 2.2339, 'grad_norm': 4.646454811096191, 'learning_rate': 0.00010457124590425956, 'epoch': 2.37}\n",
      "{'loss': 2.108, 'grad_norm': 6.121894359588623, 'learning_rate': 0.00010323929779695783, 'epoch': 2.38}\n",
      "{'loss': 2.2207, 'grad_norm': 7.221587181091309, 'learning_rate': 0.0001019073496896561, 'epoch': 2.39}\n",
      "{'loss': 2.2489, 'grad_norm': 6.457777976989746, 'learning_rate': 0.00010057540158235436, 'epoch': 2.4}\n",
      "{'loss': 2.2372, 'grad_norm': 5.646657943725586, 'learning_rate': 9.924345347505262e-05, 'epoch': 2.41}\n",
      "{'loss': 2.1474, 'grad_norm': 6.0178704261779785, 'learning_rate': 9.791150536775087e-05, 'epoch': 2.41}\n",
      "{'loss': 2.1798, 'grad_norm': 7.220373630523682, 'learning_rate': 9.657955726044913e-05, 'epoch': 2.42}\n",
      "{'loss': 2.2407, 'grad_norm': 5.187565326690674, 'learning_rate': 9.52476091531474e-05, 'epoch': 2.43}\n",
      "{'loss': 2.1915, 'grad_norm': 6.457372188568115, 'learning_rate': 9.391566104584566e-05, 'epoch': 2.44}\n",
      "{'loss': 2.1718, 'grad_norm': 5.265717029571533, 'learning_rate': 9.258371293854391e-05, 'epoch': 2.45}\n",
      "{'loss': 2.1861, 'grad_norm': 5.671113014221191, 'learning_rate': 9.125176483124217e-05, 'epoch': 2.45}\n",
      "{'loss': 2.1664, 'grad_norm': 6.512973785400391, 'learning_rate': 8.991981672394044e-05, 'epoch': 2.46}\n",
      "{'loss': 2.1841, 'grad_norm': 6.343705654144287, 'learning_rate': 8.85878686166387e-05, 'epoch': 2.47}\n",
      "{'loss': 2.1433, 'grad_norm': 6.649384498596191, 'learning_rate': 8.725592050933696e-05, 'epoch': 2.48}\n",
      "{'loss': 2.2235, 'grad_norm': 6.54374885559082, 'learning_rate': 8.592397240203522e-05, 'epoch': 2.49}\n",
      "{'loss': 2.1208, 'grad_norm': 5.013640880584717, 'learning_rate': 8.459202429473348e-05, 'epoch': 2.49}\n",
      "{'loss': 2.153, 'grad_norm': 5.196292400360107, 'learning_rate': 8.327339566850476e-05, 'epoch': 2.5}\n",
      "{'loss': 2.1301, 'grad_norm': 4.047738552093506, 'learning_rate': 8.194144756120302e-05, 'epoch': 2.51}\n",
      "{'loss': 2.215, 'grad_norm': 4.518636226654053, 'learning_rate': 8.060949945390128e-05, 'epoch': 2.52}\n",
      "{'loss': 2.1572, 'grad_norm': 6.290866851806641, 'learning_rate': 7.927755134659954e-05, 'epoch': 2.53}\n",
      "{'loss': 2.1172, 'grad_norm': 4.0396857261657715, 'learning_rate': 7.794560323929779e-05, 'epoch': 2.53}\n",
      "{'loss': 2.1182, 'grad_norm': 5.697015762329102, 'learning_rate': 7.661365513199606e-05, 'epoch': 2.54}\n",
      "{'loss': 2.1965, 'grad_norm': 5.121277332305908, 'learning_rate': 7.528170702469432e-05, 'epoch': 2.55}\n",
      "{'loss': 2.1629, 'grad_norm': 4.797724723815918, 'learning_rate': 7.394975891739259e-05, 'epoch': 2.56}\n",
      "{'loss': 2.237, 'grad_norm': 6.540056228637695, 'learning_rate': 7.261781081009083e-05, 'epoch': 2.57}\n",
      "{'loss': 2.2069, 'grad_norm': 5.442212104797363, 'learning_rate': 7.12858627027891e-05, 'epoch': 2.57}\n",
      "{'loss': 2.1757, 'grad_norm': 5.604047775268555, 'learning_rate': 6.995391459548736e-05, 'epoch': 2.58}\n",
      "{'loss': 2.2121, 'grad_norm': 4.798964023590088, 'learning_rate': 6.862196648818563e-05, 'epoch': 2.59}\n",
      "{'loss': 2.1439, 'grad_norm': 4.959336757659912, 'learning_rate': 6.729001838088389e-05, 'epoch': 2.6}\n",
      "{'loss': 2.2246, 'grad_norm': 4.527223587036133, 'learning_rate': 6.595807027358214e-05, 'epoch': 2.61}\n",
      "{'loss': 2.1455, 'grad_norm': 5.89091682434082, 'learning_rate': 6.46261221662804e-05, 'epoch': 2.61}\n",
      "{'loss': 2.2086, 'grad_norm': 4.779417991638184, 'learning_rate': 6.329417405897866e-05, 'epoch': 2.62}\n",
      "{'loss': 2.1314, 'grad_norm': 4.989590644836426, 'learning_rate': 6.196222595167693e-05, 'epoch': 2.63}\n",
      "{'loss': 2.1811, 'grad_norm': 6.199790000915527, 'learning_rate': 6.0630277844375187e-05, 'epoch': 2.64}\n",
      "{'loss': 2.1929, 'grad_norm': 7.134167671203613, 'learning_rate': 5.929832973707344e-05, 'epoch': 2.65}\n",
      "{'loss': 2.1499, 'grad_norm': 7.495222091674805, 'learning_rate': 5.79663816297717e-05, 'epoch': 2.65}\n",
      "{'loss': 2.151, 'grad_norm': 7.308454990386963, 'learning_rate': 5.663443352246997e-05, 'epoch': 2.66}\n",
      "{'loss': 2.1852, 'grad_norm': 4.728734493255615, 'learning_rate': 5.5302485415168225e-05, 'epoch': 2.67}\n",
      "{'loss': 2.2571, 'grad_norm': 4.620800495147705, 'learning_rate': 5.397053730786649e-05, 'epoch': 2.68}\n",
      "{'loss': 2.1888, 'grad_norm': 4.3527302742004395, 'learning_rate': 5.263858920056475e-05, 'epoch': 2.69}\n",
      "{'loss': 2.183, 'grad_norm': 6.461121559143066, 'learning_rate': 5.130664109326301e-05, 'epoch': 2.69}\n",
      "{'loss': 2.2055, 'grad_norm': 4.830435276031494, 'learning_rate': 4.9974692985961264e-05, 'epoch': 2.7}\n",
      "{'loss': 2.1307, 'grad_norm': 4.610945701599121, 'learning_rate': 4.864274487865953e-05, 'epoch': 2.71}\n",
      "{'loss': 2.2249, 'grad_norm': 5.384176254272461, 'learning_rate': 4.731079677135779e-05, 'epoch': 2.72}\n",
      "{'loss': 2.1024, 'grad_norm': 6.558887481689453, 'learning_rate': 4.597884866405605e-05, 'epoch': 2.73}\n",
      "{'loss': 2.1202, 'grad_norm': 6.0827789306640625, 'learning_rate': 4.46469005567543e-05, 'epoch': 2.73}\n",
      "{'loss': 2.1485, 'grad_norm': 5.580050468444824, 'learning_rate': 4.331495244945257e-05, 'epoch': 2.74}\n",
      "{'loss': 2.1395, 'grad_norm': 4.741929054260254, 'learning_rate': 4.198300434215083e-05, 'epoch': 2.75}\n",
      "{'loss': 2.1366, 'grad_norm': 6.394212245941162, 'learning_rate': 4.065105623484909e-05, 'epoch': 2.76}\n",
      "{'loss': 2.209, 'grad_norm': 7.056485652923584, 'learning_rate': 3.9319108127547356e-05, 'epoch': 2.77}\n",
      "{'loss': 2.1548, 'grad_norm': 5.304403305053711, 'learning_rate': 3.798716002024561e-05, 'epoch': 2.77}\n",
      "{'loss': 2.1298, 'grad_norm': 4.774303913116455, 'learning_rate': 3.665521191294387e-05, 'epoch': 2.78}\n",
      "{'loss': 2.205, 'grad_norm': 6.928602695465088, 'learning_rate': 3.532326380564213e-05, 'epoch': 2.79}\n",
      "{'loss': 2.1768, 'grad_norm': 5.438051700592041, 'learning_rate': 3.3991315698340395e-05, 'epoch': 2.8}\n",
      "{'loss': 2.1481, 'grad_norm': 6.035121917724609, 'learning_rate': 3.265936759103865e-05, 'epoch': 2.81}\n",
      "{'loss': 2.1778, 'grad_norm': 4.503566741943359, 'learning_rate': 3.132741948373692e-05, 'epoch': 2.81}\n",
      "{'loss': 2.1201, 'grad_norm': 4.888858318328857, 'learning_rate': 2.9995471376435176e-05, 'epoch': 2.82}\n",
      "{'loss': 2.205, 'grad_norm': 4.420898914337158, 'learning_rate': 2.8676842750206454e-05, 'epoch': 2.83}\n",
      "{'loss': 2.1677, 'grad_norm': 4.685615062713623, 'learning_rate': 2.7344894642904715e-05, 'epoch': 2.84}\n",
      "{'loss': 2.1375, 'grad_norm': 4.8848443031311035, 'learning_rate': 2.6012946535602973e-05, 'epoch': 2.85}\n",
      "{'loss': 2.1653, 'grad_norm': 4.122997283935547, 'learning_rate': 2.4680998428301235e-05, 'epoch': 2.85}\n",
      "{'loss': 2.2339, 'grad_norm': 5.131232261657715, 'learning_rate': 2.3349050320999493e-05, 'epoch': 2.86}\n",
      "{'loss': 2.2193, 'grad_norm': 4.769345283508301, 'learning_rate': 2.2017102213697754e-05, 'epoch': 2.87}\n",
      "{'loss': 2.1472, 'grad_norm': 4.332467555999756, 'learning_rate': 2.0685154106396016e-05, 'epoch': 2.88}\n",
      "{'loss': 2.1542, 'grad_norm': 4.130829811096191, 'learning_rate': 1.9353205999094274e-05, 'epoch': 2.88}\n",
      "{'loss': 2.1561, 'grad_norm': 4.819817066192627, 'learning_rate': 1.8021257891792535e-05, 'epoch': 2.89}\n",
      "{'loss': 2.1756, 'grad_norm': 5.572491645812988, 'learning_rate': 1.6689309784490797e-05, 'epoch': 2.9}\n",
      "{'loss': 2.1493, 'grad_norm': 4.069593906402588, 'learning_rate': 1.5357361677189058e-05, 'epoch': 2.91}\n",
      "{'loss': 2.1447, 'grad_norm': 4.728322982788086, 'learning_rate': 1.4025413569887316e-05, 'epoch': 2.92}\n",
      "{'loss': 2.1859, 'grad_norm': 4.957803249359131, 'learning_rate': 1.269346546258558e-05, 'epoch': 2.92}\n",
      "{'loss': 2.1998, 'grad_norm': 5.285063743591309, 'learning_rate': 1.1361517355283839e-05, 'epoch': 2.93}\n",
      "{'loss': 2.2359, 'grad_norm': 6.594818115234375, 'learning_rate': 1.0029569247982099e-05, 'epoch': 2.94}\n",
      "{'loss': 2.2234, 'grad_norm': 6.442505836486816, 'learning_rate': 8.697621140680358e-06, 'epoch': 2.95}\n",
      "{'loss': 2.1178, 'grad_norm': 4.623482704162598, 'learning_rate': 7.36567303337862e-06, 'epoch': 2.96}\n",
      "{'loss': 2.1874, 'grad_norm': 7.649477958679199, 'learning_rate': 6.0337249260768805e-06, 'epoch': 2.96}\n",
      "{'loss': 2.1772, 'grad_norm': 5.079281330108643, 'learning_rate': 4.70177681877514e-06, 'epoch': 2.97}\n",
      "{'loss': 2.1588, 'grad_norm': 4.783857822418213, 'learning_rate': 3.369828711473401e-06, 'epoch': 2.98}\n",
      "{'loss': 2.0988, 'grad_norm': 5.304446697235107, 'learning_rate': 2.051200085244679e-06, 'epoch': 2.99}\n",
      "{'loss': 2.0759, 'grad_norm': 5.304930210113525, 'learning_rate': 7.192519779429394e-07, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8535a49771ee4f05b862d3e7f7a8286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9573867321014404, 'eval_runtime': 183.9104, 'eval_samples_per_second': 272.149, 'eval_steps_per_second': 17.014, 'epoch': 3.0}\n",
      "{'train_runtime': 5139.9484, 'train_samples_per_second': 116.849, 'train_steps_per_second': 7.303, 'train_loss': 2.278235405685899, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/model_2/fine_tuned_lora_mlm\\\\tokenizer_config.json',\n",
       " './models/model_2/fine_tuned_lora_mlm\\\\special_tokens_map.json',\n",
       " './models/model_2/fine_tuned_lora_mlm\\\\vocab.json',\n",
       " './models/model_2/fine_tuned_lora_mlm\\\\merges.txt',\n",
       " './models/model_2/fine_tuned_lora_mlm\\\\added_tokens.json',\n",
       " './models/model_2/fine_tuned_lora_mlm\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a version number for the model to manage multiple versions effectively\n",
    "MODEL_VERSION = 2\n",
    "\n",
    "# Create a directory for storing the model outputs specific to the current version\n",
    "os.makedirs(f\"./models/model_{MODEL_VERSION}\", exist_ok=True)\n",
    "\n",
    "# Load the RoBERTa tokenizer and base model for Masked Language Modeling (MLM)\n",
    "model_name = \"roberta-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Tokenizer for tokenizing input text\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(model_name)  # RoBERTa model for MLM\n",
    "\n",
    "print([name for name, _ in base_model.named_modules()])\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",  # Specifies that the task is Masked Language Modeling\n",
    "    r=8,                    # Rank of the low-rank matrices used for adaptation\n",
    "    lora_alpha=16,          # Scaling factor for the LoRA update\n",
    "    lora_dropout=0.1,       # Dropout probability to improve generalization\n",
    "    target_modules=[\"query\", \"value\"], # Modules in transformer layers to be adapted\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the base model, creating a parameter-efficient fine-tuned model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Load the dataset from text files containing training and testing data\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"./datasets/train.txt\", \"test\": \"./datasets/test.txt\"})\n",
    "\n",
    "# Define a preprocessing function to tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset using the preprocessing function\n",
    "# Batched processing speeds up the tokenization, and the original \"text\" column is removed\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator for Masked Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,      # Tokenizer is used to dynamically mask tokens in batches\n",
    "    mlm=True,                 # Indicates that MLM is enabled\n",
    "    mlm_probability=0.15,     # Probability of masking tokens in input sequences\n",
    ")\n",
    "\n",
    "# Define training arguments for the Trainer API\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./models/model_{MODEL_VERSION}/results_lora\",  # Directory to save results\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate the model at the end of each epoch\n",
    "    learning_rate=5e-4,                   # Initial learning rate for optimization\n",
    "    per_device_train_batch_size=16,       # Batch size for training\n",
    "    per_device_eval_batch_size=16,        # Batch size for evaluation\n",
    "    num_train_epochs=3,                   # Number of epochs to train the model\n",
    "    weight_decay=0.01,                    # Regularization parameter to prevent overfitting\n",
    "    save_total_limit=2,                   # Keep only the latest two checkpoints to save disk space\n",
    "    logging_steps=100,                    # Log metrics every 100 steps\n",
    "    save_steps=500,                       # Save the model checkpoint every 500 steps\n",
    "    report_to=\"none\",                     # Disable external reporting (e.g., TensorBoard or WandB)\n",
    "    fp16=True,                            # Use mixed precision training for faster computation on GPUs\n",
    ")\n",
    "\n",
    "# Create a Trainer instance to manage the training loop\n",
    "trainer = Trainer(\n",
    "    model=model,                          # The fine-tunable model with LoRA applied\n",
    "    args=training_args,                   # Training arguments defined above\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Tokenized training dataset\n",
    "    eval_dataset=tokenized_datasets[\"test\"],    # Tokenized evaluation dataset\n",
    "    tokenizer=tokenizer,                  # Tokenizer for handling text input\n",
    "    data_collator=data_collator,          # Data collator for dynamically masking tokens\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer to the specified directory\n",
    "model.save_pretrained(f\"./models/model_{MODEL_VERSION}/fine_tuned_lora_mlm\")\n",
    "tokenizer.save_pretrained(f\"./models/model_{MODEL_VERSION}/fine_tuned_lora_mlm\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
